{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **BirdCLEF 2025 Data Preprocessing Notebook**\n",
    "This notebook demonstrates how we can transform audio data into mel-spectrogram data. This transformation is essential for training 2D Convolutional Neural Networks (CNNs) on audio data, as it converts the one-dimensional audio signals into two-dimensional image-like representations.\n",
    "I run this public notebook in debug mode(only a few sample processing). You can find the fully preprocessed mel spectrogram training dataset here --> [BirdCLEF'25 | Mel Spectrograms](https://www.kaggle.com/datasets/kadircandrisolu/birdclef25-mel-spectrograms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import time\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "import random\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    " \n",
    "    DEBUG_MODE = True\n",
    "    \n",
    "    OUTPUT_DIR = './working/'\n",
    "    DATA_ROOT = './Data'\n",
    "    FS = 32000\n",
    "    \n",
    "    SEED = 42\n",
    "\n",
    "    # Mel spectrogram parameters\n",
    "    N_FFT = 1024\n",
    "    HOP_LENGTH = 512\n",
    "    N_MELS = 128\n",
    "    FMIN = 50\n",
    "    FMAX = 14000\n",
    "\n",
    "    EXCLUDE_HUMAN_VOICE = True\n",
    "    NOHUMAN_DURATION = 5.0\n",
    "\n",
    "    OVERSAMPLE_THRESHOLD = 200\n",
    "    \n",
    "    TARGET_DURATION = 5.0\n",
    "    TARGET_SHAPE = (256, 256)  \n",
    "    \n",
    "    N_MAX = 50 if DEBUG_MODE else None  \n",
    "\n",
    "config = Config()\n",
    "random.seed(config.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug mode: ON\n",
      "Max samples to process: 50\n",
      "Loading taxonomy data...\n",
      "Load vocal data...\n",
      "Loading training metadata...\n"
     ]
    }
   ],
   "source": [
    "print(f\"Debug mode: {'ON' if config.DEBUG_MODE else 'OFF'}\")\n",
    "print(f\"Max samples to process: {config.N_MAX if config.N_MAX is not None else 'ALL'}\")\n",
    "\n",
    "print(\"Loading taxonomy data...\")\n",
    "taxonomy_df = pd.read_csv(f'{config.DATA_ROOT}/taxonomy.csv')\n",
    "species_class_map = dict(zip(taxonomy_df['primary_label'], taxonomy_df['class_name']))\n",
    "\n",
    "print(\"Load vocal data...\")\n",
    "with open(\"train_voice_data.pkl\", \"rb\") as fr :\n",
    "    voice_dict = pickle.load(fr)\n",
    "\n",
    "print(\"Loading training metadata...\")\n",
    "train_df = pd.read_csv(f'{config.DATA_ROOT}/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 206 unique species\n",
      "Total samples to process: 50 out of 28564 available\n",
      "Samples by class:\n",
      "class\n",
      "Aves        27648\n",
      "Amphibia      583\n",
      "Mammalia      178\n",
      "Insecta       155\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "label_list = sorted(train_df['primary_label'].unique())\n",
    "label_id_list = list(range(len(label_list)))\n",
    "label2id = dict(zip(label_list, label_id_list))\n",
    "id2label = dict(zip(label_id_list, label_list))\n",
    "\n",
    "print(f'Found {len(label_list)} unique species')\n",
    "working_df = train_df[['primary_label', 'rating', 'filename']].copy()\n",
    "working_df['target'] = working_df.primary_label.map(label2id)\n",
    "working_df['filepath'] = config.DATA_ROOT + '/train_audio/' + working_df.filename\n",
    "working_df['samplename'] = working_df.filename.map(lambda x: x.split('/')[0] + '-' + x.split('/')[-1].split('.')[0])\n",
    "working_df['class'] = working_df.primary_label.map(lambda x: species_class_map.get(x, 'Unknown'))\n",
    "total_samples = min(len(working_df), config.N_MAX or len(working_df))\n",
    "print(f'Total samples to process: {total_samples} out of {len(working_df)} available')\n",
    "print(f'Samples by class:')\n",
    "print(working_df['class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3846 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function tqdm.__del__ at 0x7fa8a368ee60>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/celenort/.local/lib/python3.10/site-packages/tqdm/std.py\", line 1148, in __del__\n",
      "    self.close()\n",
      "  File \"/home/celenort/.local/lib/python3.10/site-packages/tqdm/notebook.py\", line 279, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "AttributeError: 'tqdm_notebook' object has no attribute 'disp'\n",
      "Exception ignored in: <function tqdm.__del__ at 0x7fa8a368ee60>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/celenort/.local/lib/python3.10/site-packages/tqdm/std.py\", line 1148, in __del__\n",
      "    self.close()\n",
      "  File \"/home/celenort/.local/lib/python3.10/site-packages/tqdm/notebook.py\", line 279, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "AttributeError: 'tqdm_notebook' object has no attribute 'disp'\n",
      "Exception ignored in: <function tqdm.__del__ at 0x7fa8a368ee60>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/celenort/.local/lib/python3.10/site-packages/tqdm/std.py\", line 1148, in __del__\n",
      "    self.close()\n",
      "  File \"/home/celenort/.local/lib/python3.10/site-packages/tqdm/notebook.py\", line 279, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "AttributeError: 'tqdm_notebook' object has no attribute 'disp'\n"
     ]
    }
   ],
   "source": [
    "# voice_file_dict = {key[40:]: value for key, value in voice_dict.items()} # remove /kaggle/input/birdclef-2025/train_audio/\n",
    "\n",
    "voice_file_dict = {config.DATA_ROOT+key[27:]: value for key, value in voice_dict.items()} # remove /kaggle/input/birdclef-2025/train_audio/\n",
    "\n",
    "#audio_data, _ = librosa.load(row.filepath, sr=config.FS)\n",
    "nv_file_dict = {}\n",
    "print(f'Found {len(voice_file_dict)} files')\n",
    "for (dir, vlist) in voice_file_dict.items() :\n",
    "    audio_file, _ = librosa.load(dir, sr=config.FS)\n",
    "    lenaudio = len(audio_file)\n",
    "    nvlist = []\n",
    "    nvlist.append({'start' : 0, 'end' : vlist[0]['start']-1})\n",
    "    for i in range(len(vlist)-1) :\n",
    "        nvlist.append({'start' : vlist[i]['end']+1, 'end' : vlist[i+1]['start']-1})\n",
    "    if len(vlist)==1 :\n",
    "        nvlist.append({'start' : vlist[0]['end']+1, 'end' : lenaudio})\n",
    "    else :\n",
    "        nvlist.append({'start' : vlist[i+1]['end']+1, 'end' : lenaudio})\n",
    "    # check for too short count\n",
    "    \n",
    "    for j in reversed(range(len(nvlist))) :\n",
    "        start = nvlist[j]['start']\n",
    "        end = nvlist[j]['end']\n",
    "        if (start+config.FS * config.TARGET_DURATION) >= end :\n",
    "            nvlist.pop(j)\n",
    "            # remove too short ones\n",
    "    nv_file_dict[dir[19:]] = nvlist\n",
    "\n",
    "# nv_file_dict : same format but stores no-voice range\n",
    "print(\"Finished creating no-voice list\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio2melspec(audio_data):\n",
    "    if np.isnan(audio_data).any():\n",
    "        mean_signal = np.nanmean(audio_data)\n",
    "        audio_data = np.nan_to_num(audio_data, nan=mean_signal)\n",
    "\n",
    "    mel_spec = librosa.feature.melspectrogram(\n",
    "        y=audio_data,\n",
    "        sr=config.FS,\n",
    "        n_fft=config.N_FFT,\n",
    "        hop_length=config.HOP_LENGTH,\n",
    "        n_mels=config.N_MELS,\n",
    "        fmin=config.FMIN,\n",
    "        fmax=config.FMAX,\n",
    "        power=2.0\n",
    "    )\n",
    "\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    mel_spec_norm = (mel_spec_db - mel_spec_db.min()) / (mel_spec_db.max() - mel_spec_db.min() + 1e-8)\n",
    "    mel_spec_power_norm = (mel_spec - mel_spec.min()) / (mel_spec.max() - mel_spec.min() + 1e-8)\n",
    "\n",
    "    \n",
    "    return (mel_spec_norm, mel_spec_db, mel_spec_norm, mel_spec_power_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting audio processing...\n",
      "DEBUG MODE - Processing only 50 samples\n",
      "Processing completed in 10.32 seconds\n",
      "Successfully processed 50 files out of 50 total\n",
      "Skipped processing too short 0 files\n",
      "Failed to process 0 files\n"
     ]
    }
   ],
   "source": [
    "## Changelog : Remove cyclic padding, only add zero padding (equally on both sides)\n",
    "\n",
    "print(\"Starting audio processing...\")\n",
    "print(f\"{'DEBUG MODE - Processing only 50 samples' if config.DEBUG_MODE else 'FULL MODE - Processing all samples'}\")\n",
    "start_time = time.time()\n",
    "\n",
    "all_bird_data = {}\n",
    "errors = []\n",
    "skipcount  = 0\n",
    "\n",
    "for i, row in working_df.iterrows(): #tqdm(working_df.iterrows(), total=total_samples):\n",
    "    if config.N_MAX is not None and i >= config.N_MAX:\n",
    "        break\n",
    "    \n",
    "    try:\n",
    "        audio_data, _ = librosa.load(row.filepath, sr=config.FS)\n",
    "\n",
    "        target_samples = int(config.TARGET_DURATION * config.FS)\n",
    "\n",
    "        start_frame = -1\n",
    "\n",
    "        if row.filename in voice_file_dict :\n",
    "            nvlist = nv_file_dict[row.filename]\n",
    "            if len(nvlist) == 0 :\n",
    "                # too short, also contains human voice\n",
    "                skipcount = skipcount+1\n",
    "                continue\n",
    "            for i in range(len(nvlist)):\n",
    "                nvlist[i]['end'] = nvlist[i]['end']-target_samples\n",
    "                # narrowing range for start idx\n",
    "            \n",
    "            # let's choose among nvlist!\n",
    "            lengths = []\n",
    "            total = 0\n",
    "            for r in nvlist:\n",
    "                length = r['end']-r['start']+1\n",
    "                lengths.append((r['start'], length))\n",
    "                total += length\n",
    "            start_idx = random.randint(0, total-1)\n",
    "            for start, length in lengths :\n",
    "                if start_idx < length :\n",
    "                    start_frame = start + start_idx\n",
    "                    break\n",
    "                start_idx -= length\n",
    "\n",
    "        if len(audio_data) < target_samples:\n",
    "            left_pad = int((target_samples - len(audio_data))/2)\n",
    "            right_pad = int(target_samples-len(audio_data)-left_pad)\n",
    "            audio_data = np.pad(audio_data, (left_pad, right_pad), mode='constant')\n",
    "        if start_frame == -1 : # didn't pass through nv\n",
    "            start_frame = random.randint(0, len(audio_data)-target_samples)\n",
    "        \n",
    "        cropped_audio = audio_data[start_frame:start_frame+target_samples]\n",
    "\n",
    "        mel_spec = audio2melspec(cropped_audio)[0]\n",
    "\n",
    "        if mel_spec.shape != config.TARGET_SHAPE:\n",
    "            mel_spec = cv2.resize(mel_spec, config.TARGET_SHAPE, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "        all_bird_data[row.samplename] = mel_spec.astype(np.float32)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {row.filepath}: {e}\")\n",
    "        errors.append((row.filepath, str(e)))\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Processing completed in {end_time - start_time:.2f} seconds\")\n",
    "print(f\"Successfully processed {len(all_bird_data)} files out of {total_samples} total\")\n",
    "print(f'Skipped processing too short {skipcount} files')\n",
    "print(f\"Failed to process {len(errors)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data.npy', all_bird_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m samples \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m displayed_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "samples = []\n",
    "displayed_classes = set()\n",
    "\n",
    "max_samples = min(4, len(all_bird_data))\n",
    "\n",
    "for i, row in working_df.iterrows():\n",
    "    if i >= (config.N_MAX or len(working_df)):\n",
    "        break\n",
    "        \n",
    "    if row['samplename'] in all_bird_data:\n",
    "        if config.DEBUG_MODE:\n",
    "            if row['class'] not in displayed_classes:\n",
    "                samples.append((row['samplename'], row['class'], row['primary_label']))\n",
    "                displayed_classes.add(row['class'])\n",
    "        else:\n",
    "            if row['class'] not in displayed_classes:\n",
    "                samples.append((row['samplename'], row['class'], row['primary_label']))\n",
    "                displayed_classes.add(row['class'])\n",
    "        \n",
    "        if len(samples) >= max_samples:  \n",
    "            break\n",
    "\n",
    "if samples:\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    \n",
    "    for i, (samplename, class_name, species) in enumerate(samples):\n",
    "        plt.subplot(2, 2, i+1)\n",
    "        plt.imshow(all_bird_data[samplename], aspect='auto', origin='lower', cmap='viridis')\n",
    "        plt.title(f\"{class_name}: {species}\")\n",
    "        plt.colorbar(format='%+2.0f dB')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    debug_note = \"debug_\" if config.DEBUG_MODE else \"\"\n",
    "    plt.savefig(f'{debug_note}melspec_examples.png')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
