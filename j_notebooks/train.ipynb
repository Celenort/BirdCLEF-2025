{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ab7b461",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "12206e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import random\n",
    "import gc\n",
    "import time\n",
    "import cv2\n",
    "import math\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import soundfile as sf\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import librosa\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler, BatchSampler\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import timm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0bf9e0",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fa4a7e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    \n",
    "    seed = 42\n",
    "    debug = True  \n",
    "    apex = False\n",
    "    num_workers = 2\n",
    "    \n",
    "    OUTPUT_DIR = '../working/'\n",
    "\n",
    "    train_datadir = '../input/data-1024-128-256-256/1024_128_256_256'\n",
    "    train_csv = '../input/data-1024-128-256-256/1024_128_256_256.csv'\n",
    "    train_oggdir = '../input/train_audio'\n",
    "    # test_soundscapes = '../Data/test_soundscapes'\n",
    "    # submission_csv = '../Data/sample_submission.csv'\n",
    "    taxonomy_csv = '../Data/taxonomy.csv'\n",
    "    train_ssdir = '../input/SOUNDSCAPE_1024_128_256_256'\n",
    "    train_sscsv = '../input/SOUNDSCAPE_1024_128_256_256.csv'\n",
    "    train_sssub = '../input/submission.csv'\n",
    "    train_ssoggdir = '../input/train_soundscapes'\n",
    "\n",
    "\n",
    "    model_name = 'efficientnet_b0'  \n",
    "    pretrained = True\n",
    "    in_channels = 1\n",
    "\n",
    "    LOAD_DATA = False  # then, use on-the-fly spectrogram\n",
    "    FS = 32000\n",
    "    TARGET_DURATION = 5.0\n",
    "    TARGET_SHAPE = (256, 256)       ########### CHANGE!!\n",
    "    \n",
    "    N_FFT = 1024                    ########### CHANGE!!\n",
    "    HOP_LENGTH = 128  ########## CHANGE!!\n",
    "    N_MELS = 128  ########## CHANGE!!\n",
    "    FMIN = 20\n",
    "    FMAX = 16000\n",
    "    \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    epochs = 20  \n",
    "    batch_size = 32  \n",
    "    criterion = 'BCEWithLogitsLoss'\n",
    "\n",
    "    n_fold = 5\n",
    "    selected_folds = [0, 1, 2, 3, 4]   \n",
    "\n",
    "    optimizer = 'AdamW'\n",
    "    lr = 5e-4 \n",
    "    weight_decay = 1e-5\n",
    "  \n",
    "    scheduler = 'CosineAnnealingLR'\n",
    "    min_lr = 1e-6\n",
    "    T_max = epochs // 4\n",
    "\n",
    "    ss_label_smooth_temp = 5.0 # when loading dataset, smooth pseudo-labeled data with sigmoid(data/T)\n",
    "\n",
    "    augmentation = True\n",
    "    aug_scheduler = 'Constant' # Constant, Ramp, Exp\n",
    "    aug_weight_x = 0.5\n",
    "    aug_weight_y = 0.5\n",
    "\n",
    "    mixup_alpha = [0.5, 4.0]  # float or list of float with size 2 \n",
    "    # provide : [initial alpha for ss, final alpha for ss]\n",
    "    # alternatively, just give a float for fixed alpha (treats train data, ss equally)\n",
    "    mixup_ss = True  # enable mixup with soundscape data\n",
    "    mixup_scheduler = 'Ramp'  # Constant, Ramp, Exp\n",
    "    \n",
    "    def update_debug_settings(self):\n",
    "        if self.debug:\n",
    "            self.epochs = 2\n",
    "            self.selected_folds = [2]\n",
    "\n",
    "cfg = CFG()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274f4598",
   "metadata": {},
   "source": [
    "## Augmentation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d34b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentationPipeline :\n",
    "    def __init__(self, config, current_epo_fn, rng=None) :\n",
    "        if not config.augmentation :\n",
    "            print(\"Augmentation disabled.\")\n",
    "            return\n",
    "        self.max_epoch = config.epochs\n",
    "        self.current_epo_fn = current_epo_fn\n",
    "        self.weight_dict = {\n",
    "            'xmask' : config.aug_weight_x,\n",
    "            'ymask' : config.aug_weight_y\n",
    "        }\n",
    "        self.aug_scheduler = config.aug_scheduler\n",
    "        self.aug_delay = 0   # augmentation starts at # epo.\n",
    "        self.p_max = 0.5 # maximum augmentation proba\n",
    "        self.exp_decay = 5.0 # decay constant when using 'Exp' scheduler\n",
    "        if rng == None :\n",
    "            self.rng = np.random.RandomState()\n",
    "        else :\n",
    "            self.rng = rng\n",
    "    def schedule_p(self) :\n",
    "        D = self.aug_delay\n",
    "        T = self.max_epoch\n",
    "        t = self.current_epo_fn()\n",
    "        p_max = self.p_max\n",
    "\n",
    "        if t<D :\n",
    "            return 0.0\n",
    "        elif t > T :\n",
    "            return p_max\n",
    "        else :\n",
    "            if self.aug_scheduler == 'Constant' :\n",
    "                p = p_max\n",
    "            elif self.aug_scheduler == 'Ramp' :\n",
    "                p = (t-D)/(T-D) * p_max\n",
    "            elif self.aug_scheduler == 'Exp' :\n",
    "                c = self.exp_decay\n",
    "                p = p_max / (1-np.exp(-(T-D)*c))* (1-np.exp(-(t-D)*c))\n",
    "            else :\n",
    "                print(f\"Specified {self.aug_scheduler} not defined. Use 'Constant', 'Ramp' and 'Exp'\")\n",
    "                raise Exception(\"Aug Scheduler type not implemented\")\n",
    "            return p\n",
    "    \n",
    "    def __call__(self, spec):\n",
    "        \"\"\"Apply augmentations to spectrogram\"\"\"\n",
    "\n",
    "        p = self.schedule_p()\n",
    "\n",
    "        if self.rng.uniform() < p :\n",
    "\n",
    "            # Time masking (horizontal stripes)\n",
    "            if self.rng.uniform() < self.weight_dict['xmask']:\n",
    "                num_masks = self.rng.randint(1, 3)\n",
    "                #print(\"Debug : Xmask applied\")\n",
    "                for _ in range(num_masks):\n",
    "                    width = self.rng.randint(5, 20)\n",
    "                    start = self.rng.randint(0, spec.shape[2] - width)\n",
    "                    spec[0, :, start:start+width] = 0\n",
    "            \n",
    "            # Frequency masking (vertical stripes)\n",
    "            if self.rng.uniform() < self.weight_dict['ymask']:\n",
    "                num_masks = self.rng.randint(1, 3)\n",
    "                #print(\"Debug : Ymask applied\")\n",
    "                for _ in range(num_masks):\n",
    "                    height = self.rng.randint(5, 20)\n",
    "                    start = self.rng.randint(0, spec.shape[1] - height)\n",
    "                    spec[0, start:start+height, :] = 0\n",
    "      \n",
    "        return spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840529cf",
   "metadata": {},
   "source": [
    "## Mixup Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de58e105",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixupPipeline :\n",
    "    def __init__(self, config, current_epo_fn, rng=None) :\n",
    "\n",
    "        self.mixup_delay = 0\n",
    "        self.p_max = 0\n",
    "        self.exp_decay = 5.0\n",
    "        \n",
    "        if isinstance(config.mixup_alpha, float) :\n",
    "            # use fixed mixup alpha\n",
    "            self.alpha_initial = config.mixup_alpha\n",
    "            self.alpha_final = config.mixup_alpha\n",
    "        else :\n",
    "            self.alpha_initial = config.mixup_alpha[0]\n",
    "            self.alpha_final = config.mixup_alpha[1]\n",
    "\n",
    "        # self.mixup_ss = config.mixup_ss # do we need this?\n",
    "\n",
    "        self.current_epo_fn = current_epo_fn\n",
    "        self.mixup_scheduler = config.mixup_scheduler\n",
    "        self.total_epo = config.epochs\n",
    "        \n",
    "        if rng == None :\n",
    "            self.rng = np.random.RandomState()\n",
    "        else :\n",
    "            self.rng = rng\n",
    "    \n",
    "    def mixup_data(self, x_orig, x_ss):\n",
    "        \"\"\"Applies mixup to the data batch\"\"\"\n",
    "        batch_size = x_orig.size(0)\n",
    "        epochratio = (self.current_epo_fn()-self.mixup_delay) / (self.total_epo-self.mixup_delay)\n",
    "        # form of (t-D) / (T-D)\n",
    "\n",
    "        # defining original data's alpha\n",
    "        alpha_origin = self.alpha_initial * (1.0-epochratio) + self.alpha_final * epochratio\n",
    "        alpha_ss = self.alpha_initial * epochratio + self.alpha_final * (1.0-epochratio)\n",
    "\n",
    "        lam = self.rng.beta(alpha_origin, alpha_ss)\n",
    "\n",
    "        mixed_x = lam * x_orig + (1 - lam) * x_ss\n",
    "        \n",
    "        return mixed_x, lam\n",
    "    \n",
    "    def mixup_criterion(self, pred, y_orig, y_ss, lam, criterion = None):\n",
    "        \"\"\"Applies mixup to the loss function\"\"\"\n",
    "        # criterion = F.binary_cross_entropy_with_logits\n",
    "        if criterion == None :\n",
    "            criterion = F.binary_cross_entropy_with_logits\n",
    "\n",
    "        return lam * criterion(pred, y_orig) + (1 - lam) * criterion(pred, y_ss)\n",
    "    def schedule_p(self) :\n",
    "        D = self.mixup_delay\n",
    "        T = self.total_epo\n",
    "        t = self.current_epo_fn()\n",
    "        p_max = self.p_max\n",
    "\n",
    "        if t<D :\n",
    "            return 0.0\n",
    "        elif t > T :\n",
    "            return p_max\n",
    "        else :\n",
    "            if self.mixup_scheduler == 'Constant' :\n",
    "                p = p_max\n",
    "            elif self.mixup_scheduler == 'Ramp' :\n",
    "                p = (t-D)/(T-D) * p_max\n",
    "            elif self.mixup_scheduler == 'Exp' :\n",
    "                c = self.exp_decay\n",
    "                p = p_max / (1-np.exp(-(T-D)*c))* (1-np.exp(-(t-D)*c))\n",
    "            else :\n",
    "                print(f\"Specified {self.mixup_scheduler} not defined. Use 'Constant', 'Ramp' and 'Exp'\")\n",
    "                raise Exception(\"Aug Scheduler type not implemented\")\n",
    "            return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7e94e5",
   "metadata": {},
   "source": [
    "## Pre-processing \n",
    "- used when LOAD_DATA is False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9722d274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio2melspec(audio_data, cfg):\n",
    "    \"\"\"Convert audio data to mel spectrogram\"\"\"\n",
    "    if np.isnan(audio_data).any():\n",
    "        mean_signal = np.nanmean(audio_data)\n",
    "        audio_data = np.nan_to_num(audio_data, nan=mean_signal)\n",
    "\n",
    "    mel_spec = librosa.feature.melspectrogram(\n",
    "        y=audio_data,\n",
    "        sr=cfg.FS,\n",
    "        n_fft=cfg.N_FFT,\n",
    "        hop_length=cfg.HOP_LENGTH,\n",
    "        n_mels=cfg.N_MELS,\n",
    "        fmin=cfg.FMIN,\n",
    "        fmax=cfg.FMAX,\n",
    "        power=2.0\n",
    "    )\n",
    "\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    mel_spec_norm = (mel_spec_db - mel_spec_db.min()) / (mel_spec_db.max() - mel_spec_db.min() + 1e-8)\n",
    "    \n",
    "    return mel_spec_norm\n",
    "\n",
    "def process_audio_file(audio_path, cfg):\n",
    "    \"\"\"Process a single audio file to get the mel spectrogram\"\"\"\n",
    "    try:\n",
    "        #audio_data, _ = librosa.load(audio_path, sr=cfg.FS)\n",
    "        audio_data, _ = sf.read(audio_path, dtype='float32')\n",
    "        # must plot data values\n",
    "\n",
    "        target_samples = int(cfg.TARGET_DURATION * cfg.FS)\n",
    "\n",
    "        if len(audio_data) < target_samples:\n",
    "            n_copy = math.ceil(target_samples / len(audio_data))\n",
    "            if n_copy > 1:\n",
    "                audio_data = np.concatenate([audio_data] * n_copy)\n",
    "\n",
    "        # Extract center 5 seconds\n",
    "        start_idx = max(0, int(len(audio_data) / 2 - target_samples / 2))\n",
    "        end_idx = min(len(audio_data), start_idx + target_samples)\n",
    "        center_audio = audio_data[start_idx:end_idx]\n",
    "\n",
    "        if len(center_audio) < target_samples:\n",
    "            center_audio = np.pad(center_audio, \n",
    "                                 (0, target_samples - len(center_audio)), \n",
    "                                 mode='constant')\n",
    "\n",
    "        mel_spec = audio2melspec(center_audio, cfg)\n",
    "        \n",
    "        if mel_spec.shape != cfg.TARGET_SHAPE:\n",
    "            mel_spec = cv2.resize(mel_spec, cfg.TARGET_SHAPE, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "        return mel_spec.astype(np.float32)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def generate_spectrograms(df, cfg):\n",
    "    \"\"\"Generate spectrograms from audio files\"\"\"\n",
    "    print(\"Generating mel spectrograms from audio files...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    all_bird_data = {}\n",
    "    errors = []\n",
    "\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        if cfg.debug and i >= 1000:\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            samplename = row['samplename']\n",
    "            filepath = row['filepath']\n",
    "            \n",
    "            mel_spec = process_audio_file(filepath, cfg)\n",
    "            \n",
    "            if mel_spec is not None:\n",
    "                all_bird_data[samplename] = mel_spec\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {row.filepath}: {e}\")\n",
    "            errors.append((row.filepath, str(e)))\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Processing completed in {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"Successfully processed {len(all_bird_data)} files out of {len(df)}\")\n",
    "    print(f\"Failed to process {len(errors)} files\")\n",
    "    \n",
    "    return all_bird_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8528e9cb",
   "metadata": {},
   "source": [
    "## functions\n",
    "\n",
    "- set_seed : sets random, np, torch seed \n",
    "- collate_fn : Custom collate function to handle different sized specs\n",
    "- taxonomy_process : functionized repeated phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "91ae9411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    \"\"\"\n",
    "    Set seed for reproducibility\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(cfg.seed)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function to handle different sized spectrograms\"\"\"\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if len(batch) == 0:\n",
    "        return {}\n",
    "        \n",
    "    result = {key: [] for key in batch[0].keys()}\n",
    "    \n",
    "    for item in batch:\n",
    "        for key, value in item.items():\n",
    "            result[key].append(value)\n",
    "    \n",
    "    for key in result:\n",
    "        if key == 'target' and isinstance(result[key][0], torch.Tensor):\n",
    "            result[key] = torch.stack(result[key])\n",
    "        elif key == 'melspec' and isinstance(result[key][0], torch.Tensor):\n",
    "            shapes = [t.shape for t in result[key]]\n",
    "            if len(set(str(s) for s in shapes)) == 1:\n",
    "                result[key] = torch.stack(result[key])\n",
    "    \n",
    "    return result\n",
    "\n",
    "def taxonomy_process (cfg) :\n",
    "    \"\"\"\n",
    "    returns tuple of (taxonomy_df, species_ids, num_classes, label_to_idx)\n",
    "    \"\"\"\n",
    "    taxonomy_df = pd.read_csv(cfg.taxonomy_csv)\n",
    "    species_ids = taxonomy_df['primary_label'].tolist()\n",
    "    num_classes = len(species_ids)\n",
    "    label_to_idx = {label: idx for idx, label in enumerate(species_ids)}\n",
    "    return (taxonomy_df, species_ids, num_classes, label_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e130c44",
   "metadata": {},
   "source": [
    "## BirdCLEFDatasetFromNPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a84c9732",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdCLEFDatasetFromNPY(Dataset):\n",
    "    def __init__(self, df, cfg, augmentor = None, mode=\"train\"):\n",
    "        self.df = df\n",
    "        self.cfg = cfg\n",
    "        self.mode = mode\n",
    "        self.augmentor = augmentor\n",
    "        \n",
    "        _ , self.species_ids, self.num_classes, self.label_to_idx = taxonomy_process(cfg)\n",
    "        \n",
    "        if 'samplename' not in self.df.columns:\n",
    "            self.df['samplename'] = self.df.filename.map(lambda x: x.split('/')[0] + '-' + x.split('/')[-1].split('.')[0])\n",
    "\n",
    "        if 'melpath' not in self.df.columns:\n",
    "            self.df['melpath'] = self.cfg.train_datadir + '/' + self.df['samplename']\n",
    "\n",
    "        if cfg.debug:\n",
    "            self.df = self.df.sample(min(1000, len(self.df)), random_state=cfg.seed).reset_index(drop=True)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        spec = np.load(row['melpath']).astype(np.float32)\n",
    "        \n",
    "        spec = torch.tensor(spec, dtype=torch.float32).unsqueeze(0)  # Add channel dimension\n",
    "\n",
    "        if self.mode == \"train\" and self.cfg.augmentation :\n",
    "            spec = self.augmentor(spec)\n",
    "        \n",
    "        target = self.encode_label(row['primary_label'], row['secondary_labels']) \n",
    "        \n",
    "        return {\n",
    "            'melspec': spec, \n",
    "            'target': torch.tensor(target, dtype=torch.float32),\n",
    "            'filename': row['filename'],\n",
    "            'melpath' : row['melpath']\n",
    "        }\n",
    "    \n",
    "    def encode_label(self, label1, label2=None):\n",
    "        \"\"\"Encode label to multi-hot vector\"\"\"\n",
    "        target = np.zeros(self.num_classes)\n",
    "        if label1 in self.label_to_idx:\n",
    "            target[self.label_to_idx[label1]] = 1.0\n",
    "        if label2 :\n",
    "            if isinstance(label2, str):\n",
    "                l2 = eval(label2)\n",
    "            for label in l2:\n",
    "                if label in self.label_to_idx:\n",
    "                    target[self.label_to_idx[label]] = 1.0\n",
    "\n",
    "        return target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dabff6",
   "metadata": {},
   "source": [
    "## SoundscapeFromNPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b05bb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoundscapeFromNPY(Dataset):\n",
    "    def __init__(self, df,labeldf, cfg, augmentor = None, mode=\"train\"):\n",
    "        self.df = df\n",
    "        self.cfg = cfg\n",
    "        self.mode = mode\n",
    "        self.augmentor = augmentor\n",
    "        self.ldf = labeldf\n",
    "        \n",
    "        _, self.species_ids, self.num_classes, self.label_to_idx = taxonomy_process(cfg)\n",
    "\n",
    "        self.temperature = cfg.ss_label_smooth_temp\n",
    "\n",
    "\n",
    "        ## TODO : integrate with train_sssub\n",
    "\n",
    "# cfg.train_ssdir = '../input/SOUNDSCAPE_1024_128_256_256\n",
    "# filedir = ./Data/train_soundscapes/H29_20230523_194000.ogg\n",
    "# filename = H29_20230523_194000.ogg\n",
    "\n",
    "\n",
    "        if 'melpath' not in self.df.columns:\n",
    "            self.df['melpath'] = self.cfg.train_ssdir + '/' + self.df['filename'] + '-' + self.df['index'] + '.npy'\n",
    "            # SSDIR / H*****.ogg-index.npy\n",
    "        \n",
    "        if 'samplename' not in self.df.columns:\n",
    "            self.df['samplename'] = self.df['filename'] + '-' + self.df['index'] + '.npy'\n",
    "\n",
    "        if cfg.debug:\n",
    "            self.df = df.sample(min(1000, len(df)), random_state=cfg.seed).reset_index(drop=True)\n",
    "            self.ldf = labeldf.loc[self.df.index].reset_index(drop=True)\n",
    "        else:\n",
    "            self.df = df\n",
    "            self.ldf = labeldf\n",
    "\n",
    "        self.df, self.ldf = self._filter_df_by_threshold(self.df, self.ldf)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        lrow = self.ldf.iloc[idx]\n",
    "        spec = np.load(row['melpath']).astype(np.float32)\n",
    "        \n",
    "        spec = torch.tensor(spec, dtype=torch.float32).unsqueeze(0)  # Add channel dimension\n",
    "\n",
    "        #if self.mode == \"train\" and self.cfg.augmentation :\n",
    "        #    spec = self.augmentor(spec)\n",
    "        # should we add augmentation to the Soundscape?\n",
    "        \n",
    "        target = self._parse_label(lrow, self.temperature) \n",
    "        \n",
    "        return {\n",
    "            'melspec': spec, \n",
    "            'target': torch.tensor(target, dtype=torch.float32),\n",
    "            'filename': row['filename'],\n",
    "            'melpath' : row['melpath']\n",
    "            }\n",
    "        \n",
    "    def _filter_df_by_threshold(self, df, labeldf):\n",
    "        \"\"\"\n",
    "        Filters df using self.threshold based on max value of parsed label.\n",
    "        Prints number of kept and dropped samples.\n",
    "        \"\"\"\n",
    "        filtered_indices = []\n",
    "        for idx in range(len(df)):\n",
    "            lrow = labeldf.iloc[idx]\n",
    "            target = self._parse_label(lrow, self.temperature)\n",
    "            if np.max(target) >= self.threshold:\n",
    "                filtered_indices.append(idx)\n",
    "\n",
    "        filtered_df = df.iloc[filtered_indices].reset_index(drop=True)\n",
    "        filtered_labeldf = labeldf.iloc[filtered_indices].reset_index(drop=True)\n",
    "\n",
    "        num_total = len(df)\n",
    "        num_kept = len(filtered_df)\n",
    "\n",
    "        print(f\"Filtered samples with threshold {self.threshold}:\")\n",
    "        print(f\"  Kept: {num_kept} / {num_total} samples\")\n",
    "\n",
    "        return filtered_df, filtered_labeldf\n",
    "\n",
    "\n",
    "\n",
    "    def _prob2tprob(self, target, T, eps=1e-6):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            probabilities: np.array of shape (B, C), values in (0, 1)\n",
    "            T: temperature (T > 0)\n",
    "            eps: small constant to prevent log(0)\n",
    "        Returns:\n",
    "            softened probabilities in (0, 1)\n",
    "        \"\"\"\n",
    "        # Logit Transformation: log(p / (1 - p))\n",
    "        logits = np.log((target + eps) / (1 - target + eps))\n",
    "\n",
    "        # Temperature scaling\n",
    "        softened_logits = logits / T\n",
    "\n",
    "        # apply sigmoid again\n",
    "        softened_probs = 1 / (1 + np.exp(-softened_logits))\n",
    "\n",
    "        return softened_probs\n",
    "    \n",
    "    def _parse_label(self, lrow, T):\n",
    "        \"\"\"\n",
    "        Parse and apply sigmoid with temperature T\n",
    "        \"\"\"\n",
    "        target = np.zeros(self.num_classes)\n",
    "\n",
    "        for col in lrow.columns :\n",
    "            target[self.label_to_idx[col]] = lrow[col]\n",
    "\n",
    "        target = self._prob2tprob(target, T)\n",
    "        return target\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97423d31",
   "metadata": {},
   "source": [
    "## InfiniteRandomSampler for ss_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e965d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InfiniteRandomSampler(Sampler):\n",
    "    def __init__(self, data_source, generator=None):\n",
    "        self.data_source = data_source\n",
    "        self.generator = generator or torch.Generator()\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            idx = torch.randint(\n",
    "                high=len(self.data_source),\n",
    "                size=(1,),\n",
    "                generator=self.generator\n",
    "            ).item()\n",
    "            yield idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return 2**31  # Arbitrarily large number\n",
    "\n",
    "def infinite_batch_sampler(dataset, batch_size, generator=None):\n",
    "    infinite_sampler = InfiniteRandomSampler(dataset, generator=generator)\n",
    "    return BatchSampler(infinite_sampler, batch_size=batch_size, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033dfc68",
   "metadata": {},
   "source": [
    "## BirdCLEFModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ad7ed71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdCLEFModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        cfg.num_classes = taxonomy_process(cfg)[2]\n",
    "        \n",
    "        self.backbone = timm.create_model(\n",
    "            cfg.model_name,\n",
    "            pretrained=cfg.pretrained,\n",
    "            in_chans=cfg.in_channels,\n",
    "            drop_rate=0.2,\n",
    "            drop_path_rate=0.2\n",
    "        )\n",
    "        \n",
    "        if 'efficientnet' in cfg.model_name:\n",
    "            backbone_out = self.backbone.classifier.in_features\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "        elif 'resnet' in cfg.model_name:\n",
    "            backbone_out = self.backbone.fc.in_features\n",
    "            self.backbone.fc = nn.Identity()\n",
    "        else:\n",
    "            backbone_out = self.backbone.get_classifier().in_features\n",
    "            self.backbone.reset_classifier(0, '')\n",
    "        \n",
    "        self.pooling = nn.AdaptiveAvgPool2d(1)\n",
    "            \n",
    "        self.feat_dim = backbone_out\n",
    "        \n",
    "        self.classifier = nn.Linear(backbone_out, cfg.num_classes)\n",
    "        \n",
    "        self.mixup_enabled = hasattr(cfg, 'mixup_alpha') and cfg.mixup_alpha > 0\n",
    "        if self.mixup_enabled:\n",
    "            self.mixup_alpha = cfg.mixup_alpha\n",
    "            \n",
    "    def forward(self, x, targets=None):\n",
    "    \n",
    "        if self.training and self.mixup_enabled and targets is not None:\n",
    "            mixed_x, targets_a, targets_b, lam = self.mixup_data(x, targets)\n",
    "            x = mixed_x\n",
    "        else:\n",
    "            targets_a, targets_b, lam = None, None, None\n",
    "        \n",
    "        features = self.backbone(x)\n",
    "        \n",
    "        if isinstance(features, dict):\n",
    "            features = features['features']\n",
    "            \n",
    "        if len(features.shape) == 4:\n",
    "            features = self.pooling(features)\n",
    "            features = features.view(features.size(0), -1)\n",
    "        \n",
    "        logits = self.classifier(features)\n",
    "        \n",
    "        if self.training and self.mixup_enabled and targets is not None:\n",
    "            loss = self.mixup_criterion(F.binary_cross_entropy_with_logits, \n",
    "                                       logits, targets_a, targets_b, lam)\n",
    "            return logits, loss\n",
    "            \n",
    "        return logits\n",
    "    \n",
    "    def mixup_data(self, x, targets):\n",
    "        \"\"\"Applies mixup to the data batch\"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        lam = np.random.beta(self.mixup_alpha, self.mixup_alpha)\n",
    "\n",
    "        indices = torch.randperm(batch_size).to(x.device)\n",
    "\n",
    "        mixed_x = lam * x + (1 - lam) * x[indices]\n",
    "        \n",
    "        return mixed_x, targets, targets[indices], lam\n",
    "    \n",
    "    def mixup_criterion(self, criterion, pred, y_a, y_b, lam):\n",
    "        \"\"\"Applies mixup to the loss function\"\"\"\n",
    "        return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "019b6787",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, cfg):\n",
    "  \n",
    "    if cfg.optimizer == 'Adam':\n",
    "        optimizer = optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=cfg.lr,\n",
    "            weight_decay=cfg.weight_decay\n",
    "        )\n",
    "    elif cfg.optimizer == 'AdamW':\n",
    "        optimizer = optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=cfg.lr,\n",
    "            weight_decay=cfg.weight_decay\n",
    "        )\n",
    "    elif cfg.optimizer == 'SGD':\n",
    "        optimizer = optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr=cfg.lr,\n",
    "            momentum=0.9,\n",
    "            weight_decay=cfg.weight_decay\n",
    "        )\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Optimizer {cfg.optimizer} not implemented\")\n",
    "        \n",
    "    return optimizer\n",
    "\n",
    "def get_scheduler(optimizer, cfg):\n",
    "   \n",
    "    if cfg.scheduler == 'CosineAnnealingLR':\n",
    "        scheduler = lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=cfg.T_max,\n",
    "            eta_min=cfg.min_lr\n",
    "        )\n",
    "    elif cfg.scheduler == 'ReduceLROnPlateau':\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='min',\n",
    "            factor=0.5,\n",
    "            patience=2,\n",
    "            min_lr=cfg.min_lr,\n",
    "            verbose=True\n",
    "        )\n",
    "    elif cfg.scheduler == 'StepLR':\n",
    "        scheduler = lr_scheduler.StepLR(\n",
    "            optimizer,\n",
    "            step_size=cfg.epochs // 3,\n",
    "            gamma=0.5\n",
    "        )\n",
    "    elif cfg.scheduler == 'OneCycleLR':\n",
    "        scheduler = lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=cfg.lr,\n",
    "            steps_per_epoch=len(train_loader),\n",
    "            epochs=cfg.epochs,\n",
    "            pct_start=0.1\n",
    "        )\n",
    "    elif cfg.scheduler == 'CosineAnnealingLRwithWarmup' :\n",
    "        warmup_epochs = 5\n",
    "        warmpup_scheduler = lr_scheduler.LambdaLR(\n",
    "            optimizer, \n",
    "            lr_lambda=lambda epoch: epoch / warmup_epochs)\n",
    "        cosine_scheduler = lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=cfg.T_max,\n",
    "            eta_min=cfg.min_lr\n",
    "        )\n",
    "        scheduler = lr_scheduler.SequentialLR(\n",
    "            optimizer, \n",
    "            schedulers=[warmup_scheduler, cosine_scheduler],\n",
    "            milestones=[warmup_epochs]\n",
    "        )\n",
    "    else:\n",
    "        scheduler = None\n",
    "        \n",
    "    return scheduler\n",
    "\n",
    "def get_criterion(cfg):\n",
    " \n",
    "    if cfg.criterion == 'BCEWithLogitsLoss':\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Criterion {cfg.criterion} not implemented\")\n",
    "        \n",
    "    return criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed600eb2",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "edd496de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, criterion, device, scheduler=None):\n",
    "    \n",
    "    model.train()\n",
    "    losses = []\n",
    "    all_targets = []\n",
    "    all_outputs = []\n",
    "    \n",
    "    pbar = tqdm(enumerate(loader), total=len(loader), desc=\"Training\")\n",
    "    \n",
    "    for step, batch in pbar:\n",
    "    \n",
    "        if isinstance(batch['melspec'], list):\n",
    "            batch_outputs = []\n",
    "            batch_losses = []\n",
    "            \n",
    "            for i in range(len(batch['melspec'])):\n",
    "                inputs = batch['melspec'][i].unsqueeze(0).to(device)\n",
    "                target = batch['target'][i].unsqueeze(0).to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                output = model(inputs)\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                \n",
    "                batch_outputs.append(output.detach().cpu())\n",
    "                batch_losses.append(loss.item())\n",
    "            \n",
    "            optimizer.step()\n",
    "            outputs = torch.cat(batch_outputs, dim=0).numpy()\n",
    "            loss = np.mean(batch_losses)\n",
    "            targets = batch['target'].numpy()\n",
    "            \n",
    "        else:\n",
    "            inputs = batch['melspec'].to(device)\n",
    "            targets = batch['target'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            if isinstance(outputs, tuple):\n",
    "                outputs, loss = outputs  \n",
    "            else:\n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            outputs = outputs.detach().cpu().numpy()\n",
    "            targets = targets.detach().cpu().numpy()\n",
    "        \n",
    "        if scheduler is not None and isinstance(scheduler, lr_scheduler.OneCycleLR):\n",
    "            scheduler.step()\n",
    "            \n",
    "        all_outputs.append(outputs)\n",
    "        all_targets.append(targets)\n",
    "        losses.append(loss if isinstance(loss, float) else loss.item())\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'train_loss': np.mean(losses[-10:]) if losses else 0,\n",
    "            'lr': optimizer.param_groups[0]['lr']\n",
    "        })\n",
    "    \n",
    "    all_outputs = np.concatenate(all_outputs)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    auc = calculate_auc(all_targets, all_outputs)\n",
    "    avg_loss = np.mean(losses)\n",
    "    \n",
    "    return avg_loss, auc\n",
    "\n",
    "# Treat ss data as normal data, one epo is just concatenation of [train_loader, ss_loader]\n",
    "def train_1epo_with_ss(model, loader, optimizer, criterion, device, ss_loader, scheduler=None):\n",
    "    \n",
    "    model.train()\n",
    "    losses = []\n",
    "    sslosses = []\n",
    "    all_targets = []\n",
    "    all_outputs = []\n",
    "    \n",
    "    pbar = tqdm(enumerate(loader), total=len(loader), desc=\"Training\")\n",
    "    \n",
    "    for step, batch in pbar:\n",
    "    \n",
    "        if isinstance(batch['melspec'], list):\n",
    "            batch_outputs = []\n",
    "            batch_losses = []\n",
    "            \n",
    "            for i in range(len(batch['melspec'])):\n",
    "                inputs = batch['melspec'][i].unsqueeze(0).to(device)\n",
    "                target = batch['target'][i].unsqueeze(0).to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                output = model(inputs)\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                \n",
    "                batch_outputs.append(output.detach().cpu())\n",
    "                batch_losses.append(loss.item())\n",
    "            \n",
    "            optimizer.step()\n",
    "            outputs = torch.cat(batch_outputs, dim=0).numpy()\n",
    "            loss = np.mean(batch_losses)\n",
    "            targets = batch['target'].numpy()\n",
    "            \n",
    "        else:\n",
    "            inputs = batch['melspec'].to(device)\n",
    "            targets = batch['target'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            if isinstance(outputs, tuple):\n",
    "                outputs, loss = outputs  \n",
    "            else:\n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            outputs = outputs.detach().cpu().numpy()\n",
    "            targets = targets.detach().cpu().numpy()\n",
    "        \n",
    "        # if scheduler is not None and isinstance(scheduler, lr_scheduler.OneCycleLR):\n",
    "        #     scheduler.step()\n",
    "            \n",
    "        all_outputs.append(outputs)\n",
    "        all_targets.append(targets)\n",
    "        losses.append(loss if isinstance(loss, float) else loss.item())\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'train_loss': np.mean(losses[-10:]) if losses else 0,\n",
    "            'lr': optimizer.param_groups[0]['lr']\n",
    "        })\n",
    "\n",
    "    # Enumerating with Soundscapes\n",
    "\n",
    "    pbarss = tqdm(enumerate(ss_loader), total=len(ss_loader), desc=\"PsuedoLabel Training\")\n",
    "    for _, batch in pbarss :\n",
    "        if isinstance(batch['melspec'], list):\n",
    "            batch_losses = []\n",
    "\n",
    "            for i in range(len(batch['melspec'])):\n",
    "                inputs = batch['melspec'][i].unsqueeze(0).to(device)\n",
    "                target = batch['target'][i].unsqueeze(0).to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                output = model(inputs)\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                batch_losses.append(loss.item())\n",
    "            \n",
    "            optimizer.step()\n",
    "            loss = np.mean(batch_losses)\n",
    "            \n",
    "        else:\n",
    "            inputs = batch['melspec'].to(device)\n",
    "            targets = batch['target'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            if isinstance(outputs, tuple):\n",
    "                outputs, loss = outputs  \n",
    "            else:\n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        if scheduler is not None and isinstance(scheduler, lr_scheduler.OneCycleLR):\n",
    "            scheduler.step()\n",
    "            \n",
    "        sslosses.append(loss if isinstance(loss, float) else loss.item())\n",
    "        \n",
    "        pbarss.set_postfix({\n",
    "            'train_loss': np.mean(sslosses[-10:]) if sslosses else 0,\n",
    "            'lr': optimizer.param_groups[0]['lr']\n",
    "        })\n",
    "\n",
    "# mixup ss with original train data\n",
    "def train_1epo_with_ss_mixup(model, loader, optimizer, criterion, device, ss_loader, mixuppipeline, rng=None, scheduler=None):\n",
    "    \n",
    "    model.train()\n",
    "    losses = []\n",
    "\n",
    "    if rng == None :\n",
    "        rng = np.random.RandomState()\n",
    "    \n",
    "    pbar = tqdm(enumerate(loader), total=len(loader), desc=\"Training\")\n",
    "    ss_iter = iter(ss_loader)\n",
    "    \n",
    "    for step, batch in pbar:\n",
    "        batch_ss = next(ss_iter)\n",
    "    \n",
    "        if isinstance(batch['melspec'], list):\n",
    "            batch_outputs = []\n",
    "            batch_losses = []\n",
    "            \n",
    "            for i in range(len(batch['melspec'])):\n",
    "                inputs = batch['melspec'][i].unsqueeze(0).to(device)\n",
    "                target = batch['target'][i].unsqueeze(0).to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                if rng.uniform() < mixuppipeline.schedule_p() :   \n",
    "                    inputs_ss = batch_ss['melspec'][i].unsqueeze(0).to(device)\n",
    "                    target_ss = batch_ss['target'][i].unsqueeze(0).to(device)\n",
    "\n",
    "                    mixed_inputs, lam = mixuppipeline.mixup_data(inputs, inputs_ss)\n",
    "                    # idk it works with batch\n",
    "\n",
    "                    output = model(mixed_inputs)\n",
    "\n",
    "                    loss = mixuppipeline.mixup_criterion(output, target, target_ss, lam)\n",
    "                else :\n",
    "                    output = model(inputs)\n",
    "                    loss = criterion(output, target)\n",
    "\n",
    "                loss.backward()\n",
    "                \n",
    "                batch_outputs.append(output.detach().cpu())\n",
    "                batch_losses.append(loss.item())\n",
    "            \n",
    "            optimizer.step()\n",
    "            outputs = torch.cat(batch_outputs, dim=0).numpy()\n",
    "            loss = np.mean(batch_losses)\n",
    "            targets = batch['target'].numpy()\n",
    "            \n",
    "        else:\n",
    "            inputs = batch['melspec'].to(device)\n",
    "            targets = batch['target'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if rng.uniform() < mixuppipeline.schedule_p() :\n",
    "\n",
    "                inputs_ss = batch_ss['melspec'].to(device)\n",
    "                targets_ss = batch_ss['target'].to(device)\n",
    "            \n",
    "                mixed_inputs, lam = mixuppipeline.mixup_data(inputs, inputs_ss)\n",
    "\n",
    "                outputs = model(mixed_inputs)\n",
    "            else :\n",
    "                outputs = model(inputs)\n",
    "\n",
    "            if isinstance(outputs, tuple):\n",
    "                outputs, loss = outputs  \n",
    "            else:\n",
    "                if targets_ss is not None : # performed mixup cause target_ss is not none\n",
    "\n",
    "                    loss = mixuppipeline.mixup_criterion(outputs, target, targets_ss, lam)\n",
    "                else :\n",
    "                    loss = criterion(outputs, targets)\n",
    "                \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            outputs = outputs.detach().cpu().numpy()\n",
    "            targets = targets.detach().cpu().numpy()\n",
    "        \n",
    "        if scheduler is not None and isinstance(scheduler, lr_scheduler.OneCycleLR):\n",
    "            scheduler.step()\n",
    "            \n",
    "        losses.append(loss if isinstance(loss, float) else loss.item())\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'train_loss': np.mean(losses[-10:]) if losses else 0,\n",
    "            'lr': optimizer.param_groups[0]['lr']\n",
    "        })\n",
    "    # auc = calculate_auc(all_targets, all_outputs)\n",
    "    avg_loss = np.mean(losses)\n",
    "    \n",
    "    return avg_loss, None # return ave_loss, auc\n",
    "\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "   \n",
    "    model.eval()\n",
    "    losses = []\n",
    "    all_targets = []\n",
    "    all_outputs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Validation\"):\n",
    "            if isinstance(batch['melspec'], list):\n",
    "                batch_outputs = []\n",
    "                batch_losses = []\n",
    "                \n",
    "                for i in range(len(batch['melspec'])):\n",
    "                    inputs = batch['melspec'][i].unsqueeze(0).to(device)\n",
    "                    target = batch['target'][i].unsqueeze(0).to(device)\n",
    "                    \n",
    "                    output = model(inputs)\n",
    "                    loss = criterion(output, target)\n",
    "                    \n",
    "                    batch_outputs.append(output.detach().cpu())\n",
    "                    batch_losses.append(loss.item())\n",
    "                \n",
    "                outputs = torch.cat(batch_outputs, dim=0).numpy()\n",
    "                loss = np.mean(batch_losses)\n",
    "                targets = batch['target'].numpy()\n",
    "                \n",
    "            else:\n",
    "                inputs = batch['melspec'].to(device)\n",
    "                targets = batch['target'].to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "                outputs = outputs.detach().cpu().numpy()\n",
    "                targets = targets.detach().cpu().numpy()\n",
    "            \n",
    "            all_outputs.append(outputs)\n",
    "            all_targets.append(targets)\n",
    "            losses.append(loss if isinstance(loss, float) else loss.item())\n",
    "    \n",
    "    all_outputs = np.concatenate(all_outputs)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    \n",
    "    auc = calculate_auc(all_targets, all_outputs)\n",
    "    avg_loss = np.mean(losses)\n",
    "    \n",
    "    return avg_loss, auc\n",
    "\n",
    "def calculate_auc(targets, outputs):\n",
    "  \n",
    "    num_classes = targets.shape[1]\n",
    "    aucs = []\n",
    "    \n",
    "    probs = 1 / (1 + np.exp(-outputs))\n",
    "    \n",
    "    for i in range(num_classes):\n",
    "        \n",
    "        if np.sum(targets[:, i]) > 0:\n",
    "            class_auc = roc_auc_score(targets[:, i], probs[:, i])\n",
    "            aucs.append(class_auc)\n",
    "    \n",
    "    return np.mean(aucs) if aucs else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1bb09201",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(df, cfg, ssdf = None, sslabeldf = None):\n",
    "    \"\"\"Training function that can either use pre-computed spectrograms or generate them on-the-fly\"\"\"\n",
    "\n",
    "    cfg.num_classes = taxonomy_process(cfg)[2]\n",
    "    \n",
    "    if cfg.debug:\n",
    "        cfg.update_debug_settings()\n",
    "\n",
    "    \n",
    "    if cfg.LOAD_DATA:\n",
    "        if 'filepath' not in df.columns:\n",
    "            df['filepath'] = cfg.train_datadir + '/' + df.filename\n",
    "        if 'samplename' not in df.columns:\n",
    "            df['samplename'] = df.filename.map(lambda x: x.split('/')[0] + '-' + x.split('/')[-1].split('.')[0])\n",
    "        \n",
    "    skf = StratifiedKFold(n_splits=cfg.n_fold, shuffle=True, random_state=cfg.seed)\n",
    "    \n",
    "    best_scores = []\n",
    "\n",
    "    batch_sampler = infinite_batch_sampler(\n",
    "        ss_dataset, \n",
    "        batch_size=cfg.batch_size, \n",
    "        generator=torch.Generator().manual_seed(cfg.seed))\n",
    "\n",
    "    ss_dataset = SoundscapeFromNPY(ssdf, sslabeldf, cfg, augmentor=dataaugmentor, mode='train')\n",
    "    ss_loader = DataLoader(\n",
    "        ss_dataset,\n",
    "        batch_size=cfg.batch_size,\n",
    "        batch_sampler=batch_sampler,\n",
    "        shuffle=True,\n",
    "        num_workers=cfg.num_workers,\n",
    "        pin_memony=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    val_loss_log = np.empty((0, 6))\n",
    "\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(df, df['primary_label'])):\n",
    "        if fold not in cfg.selected_folds:\n",
    "            continue\n",
    "            \n",
    "        print(f'\\n{\"=\"*30} Fold {fold} {\"=\"*30}')\n",
    "        \n",
    "        train_df = df.iloc[train_idx].reset_index(drop=True)\n",
    "        val_df = df.iloc[val_idx].reset_index(drop=True)\n",
    "        \n",
    "        print(f'Training set: {len(train_df)} samples')\n",
    "        print(f'Validation set: {len(val_df)} samples')\n",
    "        \n",
    "        current_epo = 0\n",
    "        current_epo_fn = lambda : current_epo\n",
    "\n",
    "        dataaugmentor = AugmentationPipeline(cfg, current_epo_fn, rng=None)\n",
    "        mixuppipeline = MixupPipeline(cfg, current_epo_fn, rng = None)\n",
    "\n",
    "        train_dataset = BirdCLEFDatasetFromNPY(train_df, cfg, augmentor=dataaugmentor, mode='train')\n",
    "        val_dataset = BirdCLEFDatasetFromNPY(val_df, cfg, augmentor=None, mode='valid')\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=cfg.batch_size, \n",
    "            shuffle=True, \n",
    "            num_workers=cfg.num_workers,\n",
    "            pin_memory=True,\n",
    "            collate_fn=collate_fn,\n",
    "            drop_last=True\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=cfg.batch_size, \n",
    "            shuffle=False, \n",
    "            num_workers=cfg.num_workers,\n",
    "            pin_memory=True,\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "        \n",
    "        model = BirdCLEFModel(cfg).to(cfg.device)\n",
    "        optimizer = get_optimizer(model, cfg)\n",
    "        criterion = get_criterion(cfg)\n",
    "        \n",
    "        scheduler = get_scheduler(optimizer, cfg)\n",
    "        \n",
    "        best_auc = 0\n",
    "        best_epoch = 0\n",
    "\n",
    "        for epoch in range(cfg.epochs):\n",
    "            print(f\"\\ncur_epoch = {current_epo}\")\n",
    "            print(f\"\\nEpoch {epoch+1}/{cfg.epochs}\")\n",
    "            \n",
    "            # train_loss, train_auc = train_one_epoch(\n",
    "            #     model, \n",
    "            #     train_loader, \n",
    "            #     optimizer, \n",
    "            #     criterion, \n",
    "            #     cfg.device,\n",
    "            #     scheduler if isinstance(scheduler, lr_scheduler.OneCycleLR) else None,\n",
    "            #     ss_loader=ss_loader\n",
    "            # )\n",
    "            train_auc = None\n",
    "            train_loss, _ = train_1epo_with_ss_mixup(\n",
    "                model, \n",
    "                train_loader, \n",
    "                optimizer, \n",
    "                criterion, \n",
    "                cfg.device,\n",
    "                scheduler if isinstance(scheduler, lr_scheduler.OneCycleLR) else None,\n",
    "                ss_loader=ss_loader,\n",
    "                mixuppipeline=mixuppipeline,\n",
    "                rng=None\n",
    "            )\n",
    "\n",
    "            val_loss, val_auc = validate(model, val_loader, criterion, cfg.device)\n",
    "\n",
    "            if scheduler is not None and not isinstance(scheduler, lr_scheduler.OneCycleLR):\n",
    "                if isinstance(scheduler, lr_scheduler.ReduceLROnPlateau):\n",
    "                    scheduler.step(val_loss)\n",
    "                else:\n",
    "                    scheduler.step()\n",
    "\n",
    "            train_auc = 0.0 if train_auc==None else train_auc\n",
    "\n",
    "            print(f\"Train Loss: {train_loss:.4f}, Train AUC: {train_auc:.4f}\")\n",
    "            print(f\"Val Loss: {val_loss:.4f}, Val AUC: {val_auc:.4f}\")\n",
    "            \n",
    "            val_loss_log = np.append(val_loss_log, np.array([[fold, epoch, train_loss, train_auc, val_loss, val_auc]]), axis=0)\n",
    "            file_path = os.path.join(cfg.OUTPUT_DIR, \"val_loss_log\")\n",
    "            np.save(file_path, val_loss_log)\n",
    "            print(f\"loss file saved! {file_path}\")\n",
    "\n",
    "\n",
    "            if val_auc > best_auc:\n",
    "                best_auc = val_auc\n",
    "                best_epoch = epoch + 1\n",
    "                print(f\"New best AUC: {best_auc:.4f} at epoch {best_epoch}\")\n",
    "\n",
    "                torch.save({\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "                    'epoch': epoch,\n",
    "                    'val_auc': val_auc,\n",
    "                    'train_auc': train_auc,\n",
    "                    'cfg': cfg\n",
    "                }, f\"model_fold{fold}.pth\")\n",
    "            current_epo += 1\n",
    "        \n",
    "        best_scores.append(best_auc)\n",
    "        print(f\"\\nBest AUC for fold {fold}: {best_auc:.4f} at epoch {best_epoch}\")\n",
    "        \n",
    "        # Clear memory\n",
    "        del model, optimizer, scheduler, train_loader, val_loader\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Cross-Validation Results:\")\n",
    "    for fold, score in enumerate(best_scores):\n",
    "        print(f\"Fold {cfg.selected_folds[fold]}: {score:.4f}\")\n",
    "    print(f\"Mean AUC: {np.mean(best_scores):.4f}\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0b3744e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkhparams (cfg) :\n",
    "    t_datadir = cfg.train_datadir.split(\"/\")[2]\n",
    "\n",
    "    try :\n",
    "        parsed = t_datadir.split(\"-\")[1:]\n",
    "        nfft, nmel, shapex, shapey = parsed # , hlength = parsed\n",
    "    except Exception as e :\n",
    "        print(e)\n",
    "    if int(parsed[0]) == cfg.N_FFT and int(parsed[1]) == cfg.N_MELS and int(parsed[2]) == cfg.TARGET_SHAPE[0] and int(parsed[3]) == cfg.TARGET_SHAPE[1] : #and int(parsed[4]) == cfg.HOP_LENGTH :\n",
    "        print(\"Config and dataset hyperparameter does match.\")\n",
    "        return\n",
    "\n",
    "    else :\n",
    "        print(parsed)\n",
    "        print(cfg.N_FFT, cfg.N_MELS, cfg.TARGET_SHAPE[0], cfg.TARGET_SHAPE[1], cfg.HOP_LENGTH)\n",
    "\n",
    "        raise Exception(\"Config and dataset hyperparameter does not match\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5d5eee",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdd4e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import time\n",
    "    \n",
    "    print(\"\\nLoading training data...\")\n",
    "    train_df = pd.read_csv(cfg.train_csv)\n",
    "    ss_df = pd.read_csv(cfg.train_sscsv)\n",
    "    ss_label_df = pd.read_csv(cfg.train_sssub)\n",
    "    checkhparams(cfg)\n",
    "\n",
    "    print(\"\\nStarting training...\")\n",
    "    print(f\"LOAD_DATA is set to {cfg.LOAD_DATA}\")\n",
    "    if cfg.LOAD_DATA:\n",
    "        print(\"Using pre-computed mel spectrograms from NPY file\")\n",
    "    else:\n",
    "        print(\"Will generate spectrograms on-the-fly during training\")\n",
    "    \n",
    "    run_training(train_df, cfg, ss_df, ss_label_df)\n",
    "    \n",
    "    print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adbaf3c",
   "metadata": {},
   "source": [
    "### TO DO :\n",
    "\n",
    "- Original mixup이 model 내에서 일어나는데, batch단위로 train_one_epoch에서 일어나도록 바꾸기\n",
    "- model(inputs)의 output 인수가 1개가 되니까 if절 삭제\n",
    "- ss_data loading시 thresholding 추가"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
