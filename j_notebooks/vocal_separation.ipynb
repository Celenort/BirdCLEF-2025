{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    " \n",
    "    DEBUG_MODE = False\n",
    "    \n",
    "    OUTPUT_DIR = './working/'\n",
    "    DATA_ROOT = './Data'\n",
    "    FS = 32000\n",
    "    \n",
    "    # Mel spectrogram parameters\n",
    "    N_FFT = 1024\n",
    "    HOP_LENGTH = 512\n",
    "    N_MELS = 128\n",
    "    FMIN = 50\n",
    "    FMAX = 14000\n",
    "    \n",
    "    TARGET_DURATION = 5.0\n",
    "    TARGET_SHAPE = (256, 256)  \n",
    "    \n",
    "    N_MAX = 50 if DEBUG_MODE else None  \n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 26 Fabio's recordings in total\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['CSA'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(config.DATA_ROOT + '/train.csv')\n",
    "fabio = df[df.author == 'Fabio A. Sarria-S'].copy()\n",
    "\n",
    "print(f'We have {len(fabio)} Fabio\\'s recordings in total')\n",
    "fabio['collection'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1145"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(config.DATA_ROOT + '/train.csv')\n",
    "df = df[df.collection == 'XC'].copy()\n",
    "df['author'].unique()\n",
    "#df['collection'].unique() - iNat, XC, CSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/snakers4/silero-vad/zipball/master\" to /home/cele/.cache/torch/hub/master.zip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 1 CSA recordings by Alejandro Mendoza | Mónica Izquierdo in total\n",
      "We have 7 CSA recordings by Alexandra Buitrago-Cardona in total\n",
      "We have 6 CSA recordings by Ana María Ospina-Larrea in total\n",
      "We have 19 CSA recordings by Angela M. Mendoza-Henao in total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11713/843151244.py:61: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(figsize=(24, 3))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 37 CSA recordings by Diego A. Gomez-Morales in total\n",
      "We have 10 CSA recordings by Eliana Barona-Cortés in total\n",
      "We have 26 CSA recordings by Fabio A. Sarria-S in total\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "torch.set_num_threads(1)\n",
    "\n",
    "model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad',\n",
    "                              model='silero_vad',\n",
    "                              force_reload=True)\n",
    "\n",
    "(get_speech_timestamps,\n",
    " _, read_audio,\n",
    " *_) = utils\n",
    "\n",
    "sampling_rate = 16000 # also accepts 8000\n",
    "\n",
    "\n",
    "df = pd.read_csv(config.DATA_ROOT + '/train.csv')\n",
    "df = df[df.collection == 'iNat'].copy()\n",
    "\n",
    "author_map = {\n",
    "    'Paula Caycedo-Rosales | Juan-Pablo López': 'Paula Caycedo-Rosales',\n",
    "    'Eliana Barona-Cortés | Daniela García-Cobos': 'Eliana Barona-Cortés',\n",
    "    'Ana María Ospina-Larrea | Daniela Murillo': 'Ana María Ospina-Larrea',\n",
    "    'Alexandra Butrago-Cardona': 'Alexandra Buitrago-Cardona',\n",
    "    'Eliana Barona- Cortés': 'Eliana Barona-Cortés',\n",
    "    'Diego A Gómez-Morales': 'Diego A. Gomez-Morales',\n",
    "}\n",
    "author_map_func = lambda x: author_map[x] if x in author_map.keys() else x\n",
    "\n",
    "df.author = df.author.map(author_map_func)\n",
    "authors = sorted(df.author.unique())\n",
    "\n",
    "# Here, I limit the output to 2 authors. Otherwise, the webpage becomes too heavy to load.\n",
    "# If your are interested, please check the previous version of the notebook!\n",
    "for author in authors[:]:\n",
    "    selection = df[df.author == author].copy()\n",
    "    print(f'We have {len(selection)} CSA recordings by {author} in total')\n",
    "    \n",
    "    N = len(selection)\n",
    "    chunk_len = 0.2 # Chunk len in seconds\n",
    "    \n",
    "    for n in range(N):\n",
    "        # Load the data\n",
    "        rec = selection.iloc[n]\n",
    "        fname = config.DATA_ROOT + f'/train_audio/{rec.filename}'\n",
    "        wav, sr = librosa.load(fname)\n",
    "    \n",
    "        # Calculate the sound power\n",
    "        power = wav ** 2\n",
    "        \n",
    "        # Split the data into chunks and sum the energy in every chunk\n",
    "        chunk = int(chunk_len * sr)\n",
    "        \n",
    "        pad = int(np.ceil(len(power) / chunk) * chunk - len(power))\n",
    "        power = np.pad(power, (0, pad))\n",
    "        power = power.reshape((-1, chunk)).sum(axis=1)\n",
    "\n",
    "        speech_timestamps = get_speech_timestamps(torch.Tensor(wav), model)\n",
    "        segmentation = np.zeros_like(wav)\n",
    "        for st in speech_timestamps:\n",
    "            segmentation[st['start']: st['end']] = 20\n",
    "    \n",
    "        fig = plt.figure(figsize=(24, 3))\n",
    "        fig.suptitle(f'{rec.filename} by {rec.author}')\n",
    "        \n",
    "        t = np.arange(len(power)) * chunk_len\n",
    "        plt.plot(t, 10 * np.log10(power), 'b')\n",
    "        \n",
    "        t = np.arange(len(segmentation)) / sr\n",
    "        plt.plot(t, segmentation, 'r')        \n",
    "        #plt.show()\n",
    "        \n",
    "        #display(ipd.Audio(fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Data/train_audio/1139490/CSA36385.ogg\n",
      "[{'start': 145440, 'end': 232416}, {'start': 234016, 'end': 263136}, {'start': 265760, 'end': 291296}, {'start': 299040, 'end': 343008}, {'start': 351776, 'end': 398816}, {'start': 403488, 'end': 453088}, {'start': 455712, 'end': 546784}, {'start': 549408, 'end': 703968}, {'start': 705568, 'end': 738784}, {'start': 741408, 'end': 798176}, {'start': 802336, 'end': 822240}, {'start': 824864, 'end': 868832}, {'start': 874016, 'end': 936416}, {'start': 941600, 'end': 1016288}, {'start': 1020448, 'end': 1041888}, {'start': 1047072, 'end': 1096672}, {'start': 1104928, 'end': 1131488}, {'start': 1137696, 'end': 1196000}, {'start': 1202720, 'end': 1251808}, {'start': 1256480, 'end': 1308640}, {'start': 1310752, 'end': 1353696}, {'start': 1357344, 'end': 1434080}, {'start': 1435680, 'end': 1454560}, {'start': 1461280, 'end': 1474016}, {'start': 1489952, 'end': 1504736}, {'start': 1507360, 'end': 1520096}, {'start': 1523744, 'end': 1538528}, {'start': 1540640, 'end': 1553376}, {'start': 1557024, 'end': 1567200}, {'start': 1569312, 'end': 1582112}]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "with open(\"train_voice_data.pkl\", \"rb\") as fr:\n",
    "    voice_dict = pickle.load(fr)\n",
    "\n",
    "voice_file_dict = {'/Data/train_audio/'+key[40:]: value for key, value in voice_dict.items()} # remove /kaggle/input/birdclef-2025/train_audio/\n",
    "\n",
    "#audio_data, _ = librosa.load(row.filepath, sr=config.FS)\n",
    "\n",
    "for (dir, vlist) in voice_file_dict.items() :\n",
    "    audio_file, _ = librosa.load(dir, sr=config.FS)\n",
    "    lenaudio = len(audio_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
