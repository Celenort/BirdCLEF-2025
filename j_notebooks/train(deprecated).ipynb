{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ab7b461",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12206e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import random\n",
    "import gc\n",
    "import time\n",
    "import cv2\n",
    "import math\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import soundfile as sf\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import librosa\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler, BatchSampler\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import timm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0bf9e0",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa4a7e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    \n",
    "    general = {\n",
    "        'seed' : 42, \n",
    "        'debug' : True,\n",
    "        'num_workers' : 2,\n",
    "        'model_name' : 'efficientnet_b0',\n",
    "        'pretrained' : True,\n",
    "        'in_channels' : 1,\n",
    "        \n",
    "    }\n",
    "\n",
    "    dir = {\n",
    "        'OUTPUT_DIR' : '/kaggle/working/',\n",
    "        'train_datadir' : '/kaggle/input/mel-1024-128-256-256/1024_128_256_256',\n",
    "        'train_csv' : '/kaggle/input/mel-1024-128-256-256/1024_128_256_256.csv',\n",
    "        'train_oggdir' : '/kaggle/input/birdclef-2025/train_audio',\n",
    "        'taxonomy_csv' : '/kaggle/input/birdclef-2025/taxonomy.csv',\n",
    "        'train_ssdir' : '/kaggle/input/ssmel-1024-128-256-256-128/SOUNDSCAPE_1024_128_256_256_128',\n",
    "        'train_sscsv' : '/kaggle/input/ssmel-1024-128-256-256-128/SOUNDSCAPE_1024_128_256_256_128.csv',\n",
    "        'train_sssub' : '/kaggle/input/sub-trainss-1024-128-512-512/submission.csv',\n",
    "        'train_ssoggdir' : '/kaggle/input/birdclef-2025/train_soundscapes',\n",
    "        'chackpoint' : '/kaggle/input/birdclef-2025/model_epochx'\n",
    "\n",
    "    }\n",
    "\n",
    "    melspec = {\n",
    "        'FS' : 320000,\n",
    "        'TARGET_DURATION' : 5.0,\n",
    "        'TARGET_DURATION' : 5.0,\n",
    "        'TARGET_SHAPE' : (256, 256),\n",
    "        'N_FFT' : 1024,\n",
    "        'HOP_LENGTH' : 128,\n",
    "        'N_MELS' : 128,\n",
    "        'FMIN' : 20,\n",
    "        'FMAX' : 16000,    \n",
    "    }\n",
    "\n",
    "    train = {\n",
    "        'LOAD_DATA' : False,\n",
    "        'device' : 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        'epochs' : 20,\n",
    "        'batch_size' : 32,  \n",
    "        'criterion' : 'BCEWithLogitsLoss',\n",
    "\n",
    "\n",
    "        'n_fold' : 5,\n",
    "        'selected_folds' : [0]\n",
    "    }\n",
    "    \n",
    "    lr = {\n",
    "        'optimizer' : 'AdamW',\n",
    "        'lr' : 5e-4,\n",
    "        'weight_decay' : 1e-5,\n",
    "        'scheduler' : 'CosineAnnealingLR',\n",
    "        'min_lr' : 1e-6,\n",
    "        'T_max' : 5\n",
    "    }\n",
    "\n",
    "    soundscapes = {\n",
    "        'ss_label_smooth_temp' : 5.0, # when loading dataset, smooth pseudo-labeled data with sigmoid(data/T)\n",
    "        'ss_label_threshold' : 0.3 # based on original prediction, filters dataset\n",
    "    }\n",
    "\n",
    "    augment = {\n",
    "        'epochs' : train['epochs'],\n",
    "        'enable' : True,\n",
    "        'scheduler' : 'Ramp',\n",
    "        'weight_x' : 0.5,\n",
    "        'weight_y' : 0.5,\n",
    "        'delay' : 0,\n",
    "        'p_max' : 0.5,\n",
    "        'exp_decay' : 5.0\n",
    "    }\n",
    "    \n",
    "    mixup = {\n",
    "        'epochs' : train['epochs'],\n",
    "        'enable' : True,\n",
    "        'mode' : \"Soundscape\", \n",
    "        # \"Soundscape\" : mixup train data with soundscape\n",
    "        # \"Train\" : mixup train data by batch\n",
    "        'alpha' : [0.5, 4.0],\n",
    "        'scheduler' : 'Ramp'\n",
    "    }\n",
    "    \n",
    "    def update_debug_settings(self):\n",
    "        if self.debug:\n",
    "            self.epochs = 2\n",
    "            self.selected_folds = [2]\n",
    "\n",
    "cfg = CFG()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274f4598",
   "metadata": {},
   "source": [
    "## Augmentation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0d34b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentationPipeline :\n",
    "    def __init__(self, config, current_epo_fn, rng=None) :\n",
    "        \"\"\"\n",
    "        Config : augment\n",
    "        \"\"\"\n",
    "        if not config.enable :\n",
    "            print(\"Augmentation disabled.\")\n",
    "            return\n",
    "        self.max_epoch = config.epochs\n",
    "        self.current_epo_fn = current_epo_fn\n",
    "        self.weight_dict = {\n",
    "            'xmask' : config.weight_x,\n",
    "            'ymask' : config.weight_y\n",
    "        }\n",
    "        self.aug_scheduler = config.scheduler\n",
    "        self.aug_delay = config.delay   # augmentation starts at # epo.\n",
    "        self.p_max = config.p_max # maximum augmentation proba\n",
    "        self.exp_decay = config.exp_decay # decay constant when using 'Exp' scheduler\n",
    "        if rng == None :\n",
    "            self.rng = np.random.RandomState()\n",
    "        else :\n",
    "            self.rng = rng\n",
    "    def _schedule_p(self) :\n",
    "        D = self.aug_delay\n",
    "        T = self.max_epoch\n",
    "        t = self.current_epo_fn()\n",
    "        p_max = self.p_max\n",
    "\n",
    "        if t<D :\n",
    "            return 0.0\n",
    "        elif t > T :\n",
    "            return p_max\n",
    "        else :\n",
    "            if self.aug_scheduler == 'Constant' :\n",
    "                p = p_max\n",
    "            elif self.aug_scheduler == 'Ramp' :\n",
    "                p = (t-D)/(T-D) * p_max\n",
    "            elif self.aug_scheduler == 'Exp' :\n",
    "                c = self.exp_decay\n",
    "                p = p_max / (1-np.exp(-(T-D)*c))* (1-np.exp(-(t-D)*c))\n",
    "            else :\n",
    "                print(f\"Specified {self.aug_scheduler} not defined. Use 'Constant', 'Ramp' and 'Exp'\")\n",
    "                raise Exception(\"Aug Scheduler type not implemented\")\n",
    "            return p\n",
    "    \n",
    "    def __call__(self, spec):\n",
    "        \"\"\"Apply augmentations to spectrogram\"\"\"\n",
    "\n",
    "        p = self._schedule_p()\n",
    "\n",
    "        if self.rng.uniform() < p :\n",
    "\n",
    "            # Time masking (horizontal stripes)\n",
    "            if self.rng.uniform() < self.weight_dict['xmask']:\n",
    "                num_masks = self.rng.randint(1, 3)\n",
    "                #print(\"Debug : Xmask applied\")\n",
    "                for _ in range(num_masks):\n",
    "                    width = self.rng.randint(5, 20)\n",
    "                    start = self.rng.randint(0, spec.shape[2] - width)\n",
    "                    spec[0, :, start:start+width] = 0\n",
    "            \n",
    "            # Frequency masking (vertical stripes)\n",
    "            if self.rng.uniform() < self.weight_dict['ymask']:\n",
    "                num_masks = self.rng.randint(1, 3)\n",
    "                #print(\"Debug : Ymask applied\")\n",
    "                for _ in range(num_masks):\n",
    "                    height = self.rng.randint(5, 20)\n",
    "                    start = self.rng.randint(0, spec.shape[1] - height)\n",
    "                    spec[0, start:start+height, :] = 0\n",
    "      \n",
    "        return spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840529cf",
   "metadata": {},
   "source": [
    "## Mixup Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de58e105",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixupPipeline :\n",
    "    def __init__(self, config, current_epo_fn, rng=None) :\n",
    "        \"\"\"\n",
    "        Config : mixup\n",
    "        \"\"\"\n",
    "        if not config.enable :\n",
    "            print(\"MixupPipeline disabled.\")\n",
    "            return\n",
    "        self.mixup_delay = 0\n",
    "        self.p_max = 0\n",
    "        self.exp_decay = 5.0\n",
    "        \n",
    "        if isinstance(config.alpha, float) :\n",
    "            # use fixed mixup alpha\n",
    "            self.alpha_initial = config.alpha\n",
    "            self.alpha_final = config.alpha\n",
    "        else :\n",
    "            self.alpha_initial = config.alpha[0]\n",
    "            self.alpha_final = config.alpha[1]\n",
    "\n",
    "        self.mixup_mode = config.mode # do we need this? YES!\n",
    "\n",
    "        self.current_epo_fn = current_epo_fn\n",
    "        self.mixup_scheduler = config.scheduler\n",
    "        self.total_epo = config.epochs\n",
    "        \n",
    "        if rng == None :\n",
    "            self.rng = np.random.RandomState()\n",
    "        else :\n",
    "            self.rng = rng\n",
    "    \n",
    "    def _get_lambda(self) :\n",
    "        epochratio = (self.current_epo_fn()-self.mixup_delay) / (self.total_epo-self.mixup_delay)\n",
    "        # form of (t-D) / (T-D)\n",
    "\n",
    "        # defining original data's alpha\n",
    "        alpha_1 = self.alpha_initial * (1.0-epochratio) + self.alpha_final * epochratio\n",
    "        alpha_2 = self.alpha_initial * epochratio + self.alpha_final * (1.0-epochratio)\n",
    "\n",
    "        return self.rng.beta(alpha_1, alpha_2)\n",
    "\n",
    "    def mixup_data(self, x_orig, x_ss):\n",
    "        \"\"\"Applies mixup to the data batch\"\"\"\n",
    "        lam = self._get_lambda()\n",
    "        mixed_x = lam * x_orig + (1 - lam) * x_ss\n",
    "        return mixed_x, lam\n",
    "    \n",
    "    def mixup_criterion(self, pred, y_orig, y_ss, lam, criterion = None):\n",
    "        \"\"\"Applies mixup to the loss function\"\"\"\n",
    "        # criterion = F.binary_cross_entropy_with_logits\n",
    "        if criterion == None :\n",
    "            criterion = F.binary_cross_entropy_with_logits\n",
    "\n",
    "        return lam * criterion(pred, y_orig) + (1 - lam) * criterion(pred, y_ss)\n",
    "    def schedule_p(self) :\n",
    "        D = self.mixup_delay\n",
    "        T = self.total_epo\n",
    "        t = self.current_epo_fn()\n",
    "        p_max = self.p_max\n",
    "\n",
    "        if t<D :\n",
    "            return 0.0\n",
    "        elif t > T :\n",
    "            return p_max\n",
    "        else :\n",
    "            if self.mixup_scheduler == 'Constant' :\n",
    "                p = p_max\n",
    "            elif self.mixup_scheduler == 'Ramp' :\n",
    "                p = (t-D)/(T-D) * p_max\n",
    "            elif self.mixup_scheduler == 'Exp' :\n",
    "                c = self.exp_decay\n",
    "                p = p_max / (1-np.exp(-(T-D)*c))* (1-np.exp(-(t-D)*c))\n",
    "            else :\n",
    "                print(f\"Specified {self.mixup_scheduler} not defined. Use 'Constant', 'Ramp' and 'Exp'\")\n",
    "                raise Exception(\"Aug Scheduler type not implemented\")\n",
    "            return p\n",
    "    def old_mixup_data(self, x, targets):\n",
    "        \"\"\"Applies mixup to the data batch\"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        lam = self._get_lambda()\n",
    "\n",
    "        indices = torch.randperm(batch_size).to(x.device)\n",
    "\n",
    "        mixed_x = lam * x + (1 - lam) * x[indices]\n",
    "        \n",
    "        return mixed_x, targets, targets[indices], lam\n",
    "    \n",
    "    def old_mixup_criterion(self, criterion, pred, y_a, y_b, lam):\n",
    "        \"\"\"Applies mixup to the loss function\"\"\"\n",
    "        return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7e94e5",
   "metadata": {},
   "source": [
    "## Pre-processing \n",
    "- used when LOAD_DATA is False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9722d274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio2melspec(audio_data, cfg):\n",
    "    \"\"\"Convert audio data to mel spectrogram\"\"\"\n",
    "    if np.isnan(audio_data).any():\n",
    "        mean_signal = np.nanmean(audio_data)\n",
    "        audio_data = np.nan_to_num(audio_data, nan=mean_signal)\n",
    "\n",
    "    mel_spec = librosa.feature.melspectrogram(\n",
    "        y=audio_data,\n",
    "        sr=cfg.FS,\n",
    "        n_fft=cfg.N_FFT,\n",
    "        hop_length=cfg.HOP_LENGTH,\n",
    "        n_mels=cfg.N_MELS,\n",
    "        fmin=cfg.FMIN,\n",
    "        fmax=cfg.FMAX,\n",
    "        power=2.0\n",
    "    )\n",
    "\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    mel_spec_norm = (mel_spec_db - mel_spec_db.min()) / (mel_spec_db.max() - mel_spec_db.min() + 1e-8)\n",
    "    \n",
    "    return mel_spec_norm\n",
    "\n",
    "def process_audio_file(audio_path, cfg):\n",
    "    \"\"\"Process a single audio file to get the mel spectrogram\"\"\"\n",
    "    try:\n",
    "        #audio_data, _ = librosa.load(audio_path, sr=cfg.FS)\n",
    "        audio_data, _ = sf.read(audio_path, dtype='float32')\n",
    "        # must plot data values\n",
    "\n",
    "        target_samples = int(cfg.TARGET_DURATION * cfg.FS)\n",
    "\n",
    "        if len(audio_data) < target_samples:\n",
    "            n_copy = math.ceil(target_samples / len(audio_data))\n",
    "            if n_copy > 1:\n",
    "                audio_data = np.concatenate([audio_data] * n_copy)\n",
    "\n",
    "        # Extract center 5 seconds\n",
    "        start_idx = max(0, int(len(audio_data) / 2 - target_samples / 2))\n",
    "        end_idx = min(len(audio_data), start_idx + target_samples)\n",
    "        center_audio = audio_data[start_idx:end_idx]\n",
    "\n",
    "        if len(center_audio) < target_samples:\n",
    "            center_audio = np.pad(center_audio, \n",
    "                                 (0, target_samples - len(center_audio)), \n",
    "                                 mode='constant')\n",
    "\n",
    "        mel_spec = audio2melspec(center_audio, cfg)\n",
    "        \n",
    "        if mel_spec.shape != cfg.TARGET_SHAPE:\n",
    "            mel_spec = cv2.resize(mel_spec, cfg.TARGET_SHAPE, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "        return mel_spec.astype(np.float32)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def generate_spectrograms(df, cfg):\n",
    "    \"\"\"Generate spectrograms from audio files\"\"\"\n",
    "    print(\"Generating mel spectrograms from audio files...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    all_bird_data = {}\n",
    "    errors = []\n",
    "\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        if cfg.debug and i >= 1000:\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            samplename = row['samplename']\n",
    "            filepath = row['filepath']\n",
    "            \n",
    "            mel_spec = process_audio_file(filepath, cfg)\n",
    "            \n",
    "            if mel_spec is not None:\n",
    "                all_bird_data[samplename] = mel_spec\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {row.filepath}: {e}\")\n",
    "            errors.append((row.filepath, str(e)))\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Processing completed in {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"Successfully processed {len(all_bird_data)} files out of {len(df)}\")\n",
    "    print(f\"Failed to process {len(errors)} files\")\n",
    "    \n",
    "    return all_bird_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8528e9cb",
   "metadata": {},
   "source": [
    "## functions\n",
    "\n",
    "- set_seed : sets random, np, torch seed \n",
    "- collate_fn : Custom collate function to handle different sized specs\n",
    "- taxonomy_process : functionized repeated phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91ae9411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    \"\"\"\n",
    "    Set seed for reproducibility\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def set_rng(seed):\n",
    "    return np.random.RandomState(seed)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function to handle different sized spectrograms\"\"\"\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if len(batch) == 0:\n",
    "        return {}\n",
    "        \n",
    "    result = {key: [] for key in batch[0].keys()}\n",
    "    \n",
    "    for item in batch:\n",
    "        for key, value in item.items():\n",
    "            result[key].append(value)\n",
    "    \n",
    "    for key in result:\n",
    "        if key == 'target' and isinstance(result[key][0], torch.Tensor):\n",
    "            result[key] = torch.stack(result[key])\n",
    "        elif key == 'melspec' and isinstance(result[key][0], torch.Tensor):\n",
    "            shapes = [t.shape for t in result[key]]\n",
    "            if len(set(str(s) for s in shapes)) == 1:\n",
    "                result[key] = torch.stack(result[key])\n",
    "    \n",
    "    return result\n",
    "\n",
    "def taxonomy_process (cfg_d) :\n",
    "    \"\"\"\n",
    "    returns tuple of (taxonomy_df, species_ids, num_classes, label_to_idx)\n",
    "    \"\"\"\n",
    "    taxonomy_df = pd.read_csv(cfg_d['taxonomy_csv'])\n",
    "    species_ids = taxonomy_df['primary_label'].tolist()\n",
    "    num_classes = len(species_ids)\n",
    "    label_to_idx = {label: idx for idx, label in enumerate(species_ids)}\n",
    "    return (taxonomy_df, species_ids, num_classes, label_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e130c44",
   "metadata": {},
   "source": [
    "## BirdCLEFDatasetFromNPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a84c9732",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdCLEFDatasetFromNPY(Dataset):\n",
    "    def __init__(self, df, cfg, augmentor = None, mode=\"train\"):\n",
    "        self.df = df\n",
    "        self.cfg = cfg\n",
    "        self.mode = mode\n",
    "        self.augmentor = augmentor\n",
    "        \n",
    "        _ , self.species_ids, self.num_classes, self.label_to_idx = taxonomy_process(cfg.dir)\n",
    "        \n",
    "        if 'samplename' not in self.df.columns:\n",
    "            self.df['samplename'] = self.df.filename.map(lambda x: x.split('/')[0] + '-' + x.split('/')[-1].split('.')[0])\n",
    "\n",
    "        if 'melpath' not in self.df.columns:\n",
    "            self.df['melpath'] = self.cfg.train_datadir + '/' + self.df['samplename']\n",
    "\n",
    "        if cfg.debug:\n",
    "            self.df = self.df.sample(min(1000, len(self.df)), random_state=cfg.seed).reset_index(drop=True)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        spec = np.load(row['melpath']).astype(np.float32)\n",
    "        \n",
    "        spec = torch.tensor(spec, dtype=torch.float32).unsqueeze(0)  # Add channel dimension\n",
    "\n",
    "        if self.mode == \"train\" and self.cfg.augmentation :\n",
    "            spec = self.augmentor(spec)\n",
    "        \n",
    "        target = self.encode_label(row['primary_label'], row['secondary_labels']) \n",
    "        \n",
    "        return {\n",
    "            'melspec': spec, \n",
    "            'target': torch.tensor(target, dtype=torch.float32),\n",
    "            'filename': row['filename'],\n",
    "            'melpath' : row['melpath']\n",
    "        }\n",
    "    \n",
    "    def encode_label(self, label1, label2=None):\n",
    "        \"\"\"Encode label to multi-hot vector\"\"\"\n",
    "        target = np.zeros(self.num_classes)\n",
    "        if label1 in self.label_to_idx:\n",
    "            target[self.label_to_idx[label1]] = 1.0\n",
    "        if label2 :\n",
    "            if isinstance(label2, str):\n",
    "                l2 = eval(label2)\n",
    "            for label in l2:\n",
    "                if label in self.label_to_idx:\n",
    "                    target[self.label_to_idx[label]] = 1.0\n",
    "\n",
    "        return target\n",
    "    def load_model(self, filepath) :\n",
    "        self.backbone = timm.create_model(\n",
    "            cfg.model_name,\n",
    "            pretrained=cfg.pretrained,\n",
    "            in_chans=cfg.in_channels,\n",
    "            drop_rate=0.2,\n",
    "            drop_path_rate=0.2\n",
    "        )\n",
    "        checkpoint = torch.load(filepath, map_location=\"cpu\")  # 경로에 맞게 수정\n",
    "\n",
    "        # Step 3: state_dict 로드\n",
    "        self.backbone.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dabff6",
   "metadata": {},
   "source": [
    "## SoundscapeFromNPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b05bb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoundscapeFromNPY(Dataset):\n",
    "    def __init__(self, df,labeldf, cfg, augmentor = None, mode=\"train\"):\n",
    "        self.df = df\n",
    "        self.cfg = cfg\n",
    "        self.mode = mode\n",
    "        self.augmentor = augmentor\n",
    "        self.ldf = labeldf\n",
    "        self.threshold = cfg.ss_label_threhold\n",
    "        \n",
    "        _, self.species_ids, self.num_classes, self.label_to_idx = taxonomy_process(cfg)\n",
    "\n",
    "        self.temperature = cfg.ss_label_smooth_temp\n",
    "\n",
    "# cfg.train_ssdir = '../input/SOUNDSCAPE_1024_128_256_256\n",
    "# filedir = ./Data/train_soundscapes/H29_20230523_194000.ogg\n",
    "# filename = H29_20230523_194000.ogg\n",
    "\n",
    "        if 'melpath' not in self.df.columns:\n",
    "            self.df['melpath'] = self.cfg.train_ssdir + '/' + self.df['filename'] + '-' + self.df['index'].astype(str) + '.npy'\n",
    "            # SSDIR / H*****.ogg-index.npy\n",
    "        \n",
    "        if 'samplename' not in self.df.columns:\n",
    "            self.df['samplename'] = self.df['filename'] + '-' + self.df['index'].astype(str) + '.npy'\n",
    "\n",
    "        if cfg.debug:\n",
    "            self.df = df.sample(min(1000, len(df)), random_state=cfg.seed).reset_index(drop=True)\n",
    "            self.ldf = labeldf.loc[self.df.index].reset_index(drop=True)\n",
    "        else:\n",
    "            self.df = df\n",
    "            self.ldf = labeldf\n",
    "\n",
    "        self.df, self.ldf = self._filter_df_by_threshold(self.df, self.ldf)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        lrow = self.ldf.iloc[idx][1:]\n",
    "        spec = np.load(row['melpath']).astype(np.float32)\n",
    "        \n",
    "        spec = torch.tensor(spec, dtype=torch.float32).unsqueeze(0)  # Add channel dimension\n",
    "        \n",
    "        target = self._parse_label(lrow, self.temperature) \n",
    "        \n",
    "        return {\n",
    "            'melspec': spec, \n",
    "            'target': torch.tensor(target, dtype=torch.float32),\n",
    "            'filename': row['filename'],\n",
    "            'melpath' : row['melpath']\n",
    "            }\n",
    "        \n",
    "    def _filter_df_by_threshold(self, df, labeldf):\n",
    "        \"\"\"\n",
    "        Filters df using self.threshold based on max value of parsed label.\n",
    "        Prints number of kept and dropped samples.\n",
    "        \"\"\"\n",
    "        filtered_indices = []\n",
    "        for idx in range(len(df)):\n",
    "            lrow = labeldf.iloc[idx][1:]\n",
    "            if np.max(lrow) >= self.threshold:\n",
    "                filtered_indices.append(idx)\n",
    "\n",
    "        filtered_df = df.iloc[filtered_indices].reset_index(drop=True)\n",
    "        filtered_labeldf = labeldf.iloc[filtered_indices].reset_index(drop=True)\n",
    "\n",
    "        num_total = len(df)\n",
    "        num_kept = len(filtered_df)\n",
    "\n",
    "        print(f\"Filtered samples with threshold {self.threshold}:\")\n",
    "        print(f\"  Kept: {num_kept} / {num_total} samples\")\n",
    "\n",
    "        return filtered_df, filtered_labeldf\n",
    "\n",
    "\n",
    "    def _prob2tprob(self, target, T, eps=1e-6):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            probabilities: np.array of shape (B, C), values in (0, 1)\n",
    "            T: temperature (T > 0)\n",
    "            eps: small constant to prevent log(0)\n",
    "        Returns:\n",
    "            softened probabilities in (0, 1)\n",
    "        \"\"\"\n",
    "        # Logit Transformation: log(p / (1 - p))\n",
    "        logits = np.log((target + eps) / (1 - target + eps))\n",
    "\n",
    "        # Temperature scaling\n",
    "        softened_logits = logits / T\n",
    "\n",
    "        # apply sigmoid again\n",
    "        softened_probs = 1 / (1 + np.exp(-softened_logits))\n",
    "\n",
    "        return softened_probs\n",
    "    \n",
    "    def _parse_label(self, lrow, T):\n",
    "        \"\"\"\n",
    "        Parse and apply sigmoid with temperature T\n",
    "        \"\"\"\n",
    "        target = np.zeros(self.num_classes)\n",
    "\n",
    "        for col in lrow.index :\n",
    "            target[self.label_to_idx[col]] = lrow[col]\n",
    "\n",
    "        target = self._prob2tprob(target, T)\n",
    "        return target\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97423d31",
   "metadata": {},
   "source": [
    "## InfiniteRandomSampler for ss_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e965d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InfiniteRandomSampler(Sampler):\n",
    "    def __init__(self, data_source, generator=None):\n",
    "        self.data_source = data_source\n",
    "        self.generator = generator or torch.Generator()\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            idx = torch.randint(\n",
    "                high=len(self.data_source),\n",
    "                size=(1,),\n",
    "                generator=self.generator\n",
    "            ).item()\n",
    "            yield idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return 2**31  # Arbitrarily large number\n",
    "\n",
    "def infinite_batch_sampler(dataset, batch_size, generator=None):\n",
    "    infinite_sampler = InfiniteRandomSampler(dataset, generator=generator)\n",
    "    return BatchSampler(infinite_sampler, batch_size=batch_size, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033dfc68",
   "metadata": {},
   "source": [
    "## BirdCLEFModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad7ed71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdCLEFModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        cfg.num_classes = taxonomy_process(cfg.dir)[2]\n",
    "        \n",
    "        self.backbone = timm.create_model(\n",
    "            cfg.model_name,\n",
    "            pretrained=cfg.pretrained,\n",
    "            in_chans=cfg.in_channels,\n",
    "            drop_rate=0.2,\n",
    "            drop_path_rate=0.2\n",
    "        )\n",
    "        \n",
    "        if 'efficientnet' in cfg.model_name:\n",
    "            backbone_out = self.backbone.classifier.in_features\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "        elif 'resnet' in cfg.model_name:\n",
    "            backbone_out = self.backbone.fc.in_features\n",
    "            self.backbone.fc = nn.Identity()\n",
    "        else:\n",
    "            backbone_out = self.backbone.get_classifier().in_features\n",
    "            self.backbone.reset_classifier(0, '')\n",
    "        \n",
    "        self.pooling = nn.AdaptiveAvgPool2d(1)\n",
    "            \n",
    "        self.feat_dim = backbone_out\n",
    "        \n",
    "        self.classifier = nn.Linear(backbone_out, cfg.num_classes)\n",
    "            \n",
    "    def forward(self, x, targets=None):\n",
    "        \n",
    "        features = self.backbone(x)\n",
    "        \n",
    "        if isinstance(features, dict):\n",
    "            features = features['features']\n",
    "            \n",
    "        if len(features.shape) == 4:\n",
    "            features = self.pooling(features)\n",
    "            features = features.view(features.size(0), -1)\n",
    "        \n",
    "        logits = self.classifier(features)\n",
    "            \n",
    "        return logits\n",
    "    def load_model(self, filepath, optimizer, scheduler):\n",
    "        # Step 1: Checkpoint 불러오기\n",
    "        checkpoint = torch.load(filepath, map_location=self.cfg.device)\n",
    "        state_dict = checkpoint['model_state_dict']\n",
    "\n",
    "        # Step 2: (중요!) 모델 전체 구조에 맞게 state_dict 로드\n",
    "        self.load_state_dict(state_dict)\n",
    "\n",
    "        # Step 3: Optimizer 복원\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "        # Step 4: Scheduler 복원\n",
    "        if checkpoint['scheduler_state_dict'] is not None:\n",
    "            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "\n",
    "        # Step 5: 필요한 추가 정보 리턴\n",
    "        return checkpoint['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "019b6787",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, cfg):\n",
    "  \n",
    "    if cfg.optimizer == 'Adam':\n",
    "        optimizer = optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=cfg.lr,\n",
    "            weight_decay=cfg.weight_decay\n",
    "        )\n",
    "    elif cfg.optimizer == 'AdamW':\n",
    "        optimizer = optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=cfg.lr,\n",
    "            weight_decay=cfg.weight_decay\n",
    "        )\n",
    "    elif cfg.optimizer == 'SGD':\n",
    "        optimizer = optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr=cfg.lr,\n",
    "            momentum=0.9,\n",
    "            weight_decay=cfg.weight_decay\n",
    "        )\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Optimizer {cfg.optimizer} not implemented\")\n",
    "        \n",
    "    return optimizer\n",
    "\n",
    "def get_scheduler(optimizer, cfg):\n",
    "   \n",
    "    if cfg.scheduler == 'CosineAnnealingLR':\n",
    "        scheduler = lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=cfg.T_max,\n",
    "            eta_min=cfg.min_lr\n",
    "        )\n",
    "    elif cfg.scheduler == 'ReduceLROnPlateau':\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='min',\n",
    "            factor=0.5,\n",
    "            patience=2,\n",
    "            min_lr=cfg.min_lr,\n",
    "            verbose=True\n",
    "        )\n",
    "    elif cfg.scheduler == 'StepLR':\n",
    "        scheduler = lr_scheduler.StepLR(\n",
    "            optimizer,\n",
    "            step_size=cfg.epochs // 3,\n",
    "            gamma=0.5\n",
    "        )\n",
    "    elif cfg.scheduler == 'OneCycleLR':\n",
    "        scheduler = lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=cfg.lr,\n",
    "            steps_per_epoch=len(train_loader),\n",
    "            epochs=cfg.epochs,\n",
    "            pct_start=0.1\n",
    "        )\n",
    "    elif cfg.scheduler == 'CosineAnnealingLRwithWarmup' :\n",
    "        warmup_epochs = 5\n",
    "        warmup_scheduler = lr_scheduler.LambdaLR(\n",
    "            optimizer, \n",
    "            lr_lambda=lambda epoch: epoch / warmup_epochs)\n",
    "        cosine_scheduler = lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=cfg.T_max,\n",
    "            eta_min=cfg.min_lr\n",
    "        )\n",
    "        scheduler = lr_scheduler.SequentialLR(\n",
    "            optimizer, \n",
    "            schedulers=[warmup_scheduler, cosine_scheduler],\n",
    "            milestones=[warmup_epochs]\n",
    "        )\n",
    "    else:\n",
    "        scheduler = None\n",
    "        \n",
    "    return scheduler\n",
    "\n",
    "def get_criterion(cfg):\n",
    " \n",
    "    if cfg.criterion == 'BCEWithLogitsLoss':\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Criterion {cfg.criterion} not implemented\")\n",
    "        \n",
    "    return criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed600eb2",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "edd496de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, criterion, device, mixuppipeline, scheduler=None):\n",
    "    \n",
    "    model.train()\n",
    "    losses = []\n",
    "    all_targets = []\n",
    "    all_outputs = []\n",
    "    \n",
    "    enablemixup = False\n",
    "    if mixuppipeline.mixup_mode == \"Train\" :\n",
    "        enablemixup = True\n",
    "\n",
    "\n",
    "    pbar = tqdm(enumerate(loader), total=len(loader), desc=\"Training\")\n",
    "    \n",
    "    for step, batch in pbar:\n",
    "    \n",
    "        if isinstance(batch['melspec'], list):\n",
    "            batch_outputs = []\n",
    "            batch_losses = []\n",
    "            \n",
    "            for i in range(len(batch['melspec'])):\n",
    "                inputs = batch['melspec'][i].unsqueeze(0).to(device)\n",
    "                target = batch['target'][i].unsqueeze(0).to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                if enablemixup and mixuppipeline.rng.uniform()> mixuppipeline.schedule_p() :\n",
    "                    mixed_inputs, target, target_idx, lam = mixuppipeline.old_mixup_data(inputs, target)\n",
    "                    output = model(mixed_inputs)\n",
    "                    loss = mixuppipeline.old_mixup_criterion(criterion, output, target, target_idx, lam)\n",
    "                else :\n",
    "                    output = model(inputs)\n",
    "                    loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                \n",
    "                batch_outputs.append(output.detach().cpu())\n",
    "                batch_losses.append(loss.item())\n",
    "            \n",
    "            optimizer.step()\n",
    "            outputs = torch.cat(batch_outputs, dim=0).numpy()\n",
    "            loss = np.mean(batch_losses)\n",
    "            targets = batch['target'].numpy()\n",
    "            \n",
    "        else:\n",
    "            inputs = batch['melspec'].to(device)\n",
    "            targets = batch['target'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            if enablemixup and mixuppipeline.rng.uniform()> mixuppipeline.schedule_p() :\n",
    "                mixed_inputs, target, target_idx, lam = mixuppipeline.old_mixup_data(inputs, target)\n",
    "                outputs = model(mixed_inputs)\n",
    "                loss = mixuppipeline.old_mixup_criterion(criterion, output, target, target_idx, lam)\n",
    "            else :\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            outputs = outputs.detach().cpu().numpy()\n",
    "            targets = targets.detach().cpu().numpy()\n",
    "        \n",
    "        if scheduler is not None and isinstance(scheduler, lr_scheduler.OneCycleLR):\n",
    "            scheduler.step()\n",
    "            \n",
    "        all_outputs.append(outputs)\n",
    "        all_targets.append(targets)\n",
    "        losses.append(loss if isinstance(loss, float) else loss.item())\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'train_loss': np.mean(losses[-10:]) if losses else 0,\n",
    "            'lr': optimizer.param_groups[0]['lr']\n",
    "        })\n",
    "    \n",
    "    all_outputs = np.concatenate(all_outputs)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    auc = calculate_auc(all_targets, all_outputs)\n",
    "    avg_loss = np.mean(losses)\n",
    "    \n",
    "    return avg_loss, auc\n",
    "\n",
    "# Treat ss data as normal data, one epo is just concatenation of [train_loader, ss_loader]\n",
    "def train_1epo_with_ss(model, loader, optimizer, criterion, device, ss_loader, mixuppipeline, scheduler=None):\n",
    "    \n",
    "    model.train()\n",
    "    losses = []\n",
    "    sslosses = []\n",
    "    all_targets = []\n",
    "    all_outputs = []\n",
    "\n",
    "    enablemixup = False\n",
    "    if mixuppipeline.mixup_mode == \"Train\" :\n",
    "        enablemixup = True\n",
    "    \n",
    "    pbar = tqdm(enumerate(loader), total=len(loader), desc=\"Training\")\n",
    "    \n",
    "    for step, batch in pbar:\n",
    "    \n",
    "        if isinstance(batch['melspec'], list):\n",
    "            batch_outputs = []\n",
    "            batch_losses = []\n",
    "            \n",
    "            for i in range(len(batch['melspec'])):\n",
    "                inputs = batch['melspec'][i].unsqueeze(0).to(device)\n",
    "                target = batch['target'][i].unsqueeze(0).to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                if enablemixup and mixuppipeline.rng.uniform()> mixuppipeline.schedule_p() :\n",
    "                    mixed_inputs, target, target_idx, lam = mixuppipeline.old_mixup_data(inputs, target)\n",
    "                    output = model(mixed_inputs)\n",
    "                    loss = mixuppipeline.old_mixup_criterion(criterion, output, target, target_idx, lam)\n",
    "                else :\n",
    "                    output = model(inputs)\n",
    "                    loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                batch_outputs.append(output.detach().cpu())\n",
    "                batch_losses.append(loss.item())\n",
    "            \n",
    "            optimizer.step()\n",
    "            outputs = torch.cat(batch_outputs, dim=0).numpy()\n",
    "            loss = np.mean(batch_losses)\n",
    "            targets = batch['target'].numpy()\n",
    "            \n",
    "        else:\n",
    "            mixupthisbatch = (mixuppipeline.rng.uniform()> mixuppipeline.schedule_p())\n",
    "            inputs = batch['melspec'].to(device)\n",
    "            targets = batch['target'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            if enablemixup and mixuppipeline.rng.uniform()> mixuppipeline.schedule_p() :\n",
    "                mixed_inputs, target, target_idx, lam = mixuppipeline.old_mixup_data(inputs, target)\n",
    "                output = model(mixed_inputs)\n",
    "                loss = mixuppipeline.old_mixup_criterion(criterion, output, target, target_idx, lam)\n",
    "            else :\n",
    "                output = model(inputs)\n",
    "                loss = criterion(output, target)\n",
    "                \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            outputs = outputs.detach().cpu().numpy()\n",
    "            targets = targets.detach().cpu().numpy()\n",
    "        \n",
    "        # if scheduler is not None and isinstance(scheduler, lr_scheduler.OneCycleLR):\n",
    "        #     scheduler.step()\n",
    "            \n",
    "        all_outputs.append(outputs)\n",
    "        all_targets.append(targets)\n",
    "        losses.append(loss if isinstance(loss, float) else loss.item())\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'train_loss': np.mean(losses[-10:]) if losses else 0,\n",
    "            'lr': optimizer.param_groups[0]['lr']\n",
    "        })\n",
    "\n",
    "    # Enumerating with Soundscapes\n",
    "\n",
    "    pbarss = tqdm(enumerate(ss_loader), total=len(ss_loader), desc=\"PsuedoLabel Training\")\n",
    "    for _, batch in pbarss :\n",
    "        if isinstance(batch['melspec'], list):\n",
    "            batch_losses = []\n",
    "\n",
    "            for i in range(len(batch['melspec'])):\n",
    "                inputs = batch['melspec'][i].unsqueeze(0).to(device)\n",
    "                target = batch['target'][i].unsqueeze(0).to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                if enablemixup and mixuppipeline.rng.uniform()> mixuppipeline.schedule_p() :\n",
    "                    mixed_inputs, target, target_idx, lam = mixuppipeline.old_mixup_data(inputs, target)\n",
    "                    output = model(mixed_inputs)\n",
    "                    loss = mixuppipeline.old_mixup_criterion(criterion, output, target, target_idx, lam)\n",
    "                else :\n",
    "                    output = model(inputs)\n",
    "                    loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                batch_losses.append(loss.item())\n",
    "            \n",
    "            optimizer.step()\n",
    "            loss = np.mean(batch_losses)\n",
    "            \n",
    "        else:\n",
    "            inputs = batch['melspec'].to(device)\n",
    "            targets = batch['target'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            if enablemixup and mixuppipeline.rng.uniform()> mixuppipeline.schedule_p() :\n",
    "                mixed_inputs, target, target_idx, lam = mixuppipeline.old_mixup_data(inputs, target)\n",
    "                output = model(mixed_inputs)\n",
    "                loss = mixuppipeline.old_mixup_criterion(criterion, output, target, target_idx, lam)\n",
    "            else :\n",
    "                output = model(inputs)\n",
    "                loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        if scheduler is not None and isinstance(scheduler, lr_scheduler.OneCycleLR):\n",
    "            scheduler.step()\n",
    "            \n",
    "        sslosses.append(loss if isinstance(loss, float) else loss.item())\n",
    "        \n",
    "        pbarss.set_postfix({\n",
    "            'train_loss': np.mean(sslosses[-10:]) if sslosses else 0,\n",
    "            'lr': optimizer.param_groups[0]['lr']\n",
    "        })\n",
    "\n",
    "# mixup ss with original train data\n",
    "def train_1epo_with_ss_mixup(model, loader, optimizer, criterion, device, ss_loader, mixuppipeline, scheduler=None):\n",
    "    \n",
    "    model.train()\n",
    "    losses = []\n",
    "    \n",
    "    pbar = tqdm(enumerate(loader), total=len(loader), desc=\"Training\")\n",
    "    ss_iter = iter(ss_loader)\n",
    "    \n",
    "    enablemixup = False\n",
    "    # check if mixup mode is \"Soundscape\" \n",
    "    if mixuppipeline.mixup_mode == \"Soundscape\" :\n",
    "        enablemixup = True\n",
    "\n",
    "\n",
    "    for step, batch in pbar:\n",
    "        batch_ss = next(ss_iter)\n",
    "    \n",
    "        if isinstance(batch['melspec'], list):\n",
    "            batch_outputs = []\n",
    "            batch_losses = []\n",
    "            \n",
    "            for i in range(len(batch['melspec'])):\n",
    "                inputs = batch['melspec'][i].unsqueeze(0).to(device)\n",
    "                target = batch['target'][i].unsqueeze(0).to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                if enablemixup and mixuppipeline.rng.uniform() < mixuppipeline.schedule_p() :   \n",
    "                    inputs_ss = batch_ss['melspec'][i].unsqueeze(0).to(device)\n",
    "                    target_ss = batch_ss['target'][i].unsqueeze(0).to(device)\n",
    "\n",
    "                    mixed_inputs, lam = mixuppipeline.mixup_data(inputs, inputs_ss)\n",
    "                    # idk it works with batch\n",
    "\n",
    "                    output = model(mixed_inputs)\n",
    "\n",
    "                    loss = mixuppipeline.mixup_criterion(output, target, target_ss, lam)\n",
    "                else :\n",
    "                    output = model(inputs)\n",
    "                    loss = criterion(output, target)\n",
    "\n",
    "                loss.backward()\n",
    "                \n",
    "                batch_outputs.append(output.detach().cpu())\n",
    "                batch_losses.append(loss.item())\n",
    "            \n",
    "            optimizer.step()\n",
    "            outputs = torch.cat(batch_outputs, dim=0).numpy()\n",
    "            loss = np.mean(batch_losses)\n",
    "            targets = batch['target'].numpy()\n",
    "            \n",
    "        else:\n",
    "            inputs = batch['melspec'].to(device)\n",
    "            targets = batch['target'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if enablemixup and mixuppipeline.rng.uniform() < mixuppipeline.schedule_p() :\n",
    "\n",
    "                inputs_ss = batch_ss['melspec'].to(device)\n",
    "                targets_ss = batch_ss['target'].to(device)\n",
    "            \n",
    "                mixed_inputs, lam = mixuppipeline.mixup_data(inputs, inputs_ss)\n",
    "\n",
    "                outputs = model(mixed_inputs)\n",
    "                loss = mixuppipeline.mixup_criterion(outputs, targets, targets_ss, lam)\n",
    "\n",
    "            else :\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            outputs = outputs.detach().cpu().numpy()\n",
    "            targets = targets.detach().cpu().numpy()\n",
    "        \n",
    "        if scheduler is not None and isinstance(scheduler, lr_scheduler.OneCycleLR):\n",
    "            scheduler.step()\n",
    "            \n",
    "        losses.append(loss if isinstance(loss, float) else loss.item())\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'train_loss': np.mean(losses[-10:]) if losses else 0,\n",
    "            'lr': optimizer.param_groups[0]['lr']\n",
    "        })\n",
    "    # auc = calculate_auc(all_targets, all_outputs)\n",
    "    avg_loss = np.mean(losses)\n",
    "    \n",
    "    return avg_loss, None # return ave_loss, auc\n",
    "\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    \n",
    "    model.eval()\n",
    "    losses = []\n",
    "    all_targets = []\n",
    "    all_outputs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Validation\"):\n",
    "            if isinstance(batch['melspec'], list):\n",
    "                batch_outputs = []\n",
    "                batch_losses = []\n",
    "                \n",
    "                for i in range(len(batch['melspec'])):\n",
    "                    inputs = batch['melspec'][i].unsqueeze(0).to(device)\n",
    "                    target = batch['target'][i].unsqueeze(0).to(device)\n",
    "                    \n",
    "                    output = model(inputs)\n",
    "                    loss = criterion(output, target)\n",
    "                    \n",
    "                    batch_outputs.append(output.detach().cpu())\n",
    "                    batch_losses.append(loss.item())\n",
    "                \n",
    "                outputs = torch.cat(batch_outputs, dim=0).numpy()\n",
    "                loss = np.mean(batch_losses)\n",
    "                targets = batch['target'].numpy()\n",
    "                \n",
    "            else:\n",
    "                inputs = batch['melspec'].to(device)\n",
    "                targets = batch['target'].to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "                outputs = outputs.detach().cpu().numpy()\n",
    "                targets = targets.detach().cpu().numpy()\n",
    "            \n",
    "            all_outputs.append(outputs)\n",
    "            all_targets.append(targets)\n",
    "            losses.append(loss if isinstance(loss, float) else loss.item())\n",
    "    \n",
    "    all_outputs = np.concatenate(all_outputs)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    \n",
    "    auc = calculate_auc(all_targets, all_outputs)\n",
    "    avg_loss = np.mean(losses)\n",
    "    \n",
    "    return avg_loss, auc\n",
    "\n",
    "def calculate_auc(targets, outputs):\n",
    "  \n",
    "    num_classes = targets.shape[1]\n",
    "    aucs = []\n",
    "    \n",
    "    probs = 1 / (1 + np.exp(-outputs))\n",
    "    \n",
    "    for i in range(num_classes):\n",
    "        \n",
    "        if np.sum(targets[:, i]) > 0:\n",
    "            class_auc = roc_auc_score(targets[:, i], probs[:, i])\n",
    "            aucs.append(class_auc)\n",
    "    \n",
    "    return np.mean(aucs) if aucs else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1bb09201",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_with_folds(df, cfg, ssdf = None, sslabeldf = None, rng=None):\n",
    "    \"\"\"Training function that can either use pre-computed spectrograms or generate them on-the-fly\"\"\"\n",
    "\n",
    "    cfg.num_classes = taxonomy_process(cfg.dir)[2]\n",
    "    \n",
    "    if cfg.general['debug']:\n",
    "        cfg.update_debug_settings()\n",
    "\n",
    "    \n",
    "    if cfg.train['LOAD_DATA']:\n",
    "        if 'filepath' not in df.columns:\n",
    "            df['filepath'] = cfg.dir['train_datadir'] + '/' + df.filename\n",
    "        if 'samplename' not in df.columns:\n",
    "            df['samplename'] = df.filename.map(lambda x: x.split('/')[0] + '-' + x.split('/')[-1].split('.')[0])\n",
    "        \n",
    "    skf = StratifiedKFold(n_splits=cfg.train['n_fold'], shuffle=True, random_state=cfg.seed)\n",
    "    \n",
    "    best_scores = []\n",
    "\n",
    "    ss_dataset = SoundscapeFromNPY(ssdf, sslabeldf, cfg, augmentor=None, mode='train')\n",
    "\n",
    "    batch_sampler = infinite_batch_sampler(\n",
    "        ss_dataset, \n",
    "        batch_size=cfg.batch_size, \n",
    "        generator=torch.Generator().manual_seed(cfg.seed))\n",
    "\n",
    "    ss_loader = DataLoader(\n",
    "        ss_dataset,\n",
    "        #batch_size=cfg.batch_size,\n",
    "        batch_sampler=batch_sampler,\n",
    "        #shuffle=True,\n",
    "        num_workers=cfg.num_workers,\n",
    "        pin_memory=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    val_loss_log = np.empty((0, 6))\n",
    "    val_loss_log = np.append(val_loss_log, np.array([['fold', 'epoch', 'train_loss', 'train_auc', 'val_loss', 'val_auc']]), axis=0)\n",
    "    # save columns\n",
    "\n",
    "\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(df, df['primary_label'])):\n",
    "        if fold not in cfg.selected_folds:\n",
    "            continue\n",
    "            \n",
    "        print(f'\\n{\"=\"*30} Fold {fold} {\"=\"*30}')\n",
    "        \n",
    "        train_df = df.iloc[train_idx].reset_index(drop=True)\n",
    "        val_df = df.iloc[val_idx].reset_index(drop=True)\n",
    "        \n",
    "        print(f'Training set: {len(train_df)} samples')\n",
    "        print(f'Validation set: {len(val_df)} samples')\n",
    "        \n",
    "        current_epo = 0\n",
    "        current_epo_fn = lambda : current_epo\n",
    "\n",
    "        dataaugmentor = AugmentationPipeline(cfg, current_epo_fn, rng=rng)\n",
    "        mixuppipeline = MixupPipeline(cfg, current_epo_fn, rng = rng)\n",
    "\n",
    "        train_dataset = BirdCLEFDatasetFromNPY(train_df, cfg, augmentor=dataaugmentor, mode='train')\n",
    "        val_dataset = BirdCLEFDatasetFromNPY(val_df, cfg, augmentor=None, mode='valid')\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=cfg.batch_size, \n",
    "            shuffle=True, \n",
    "            num_workers=cfg.num_workers,\n",
    "            pin_memory=True,\n",
    "            collate_fn=collate_fn,\n",
    "            drop_last=True\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=cfg.batch_size, \n",
    "            shuffle=False, \n",
    "            num_workers=cfg.num_workers,\n",
    "            pin_memory=True,\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "        \n",
    "        model = BirdCLEFModel(cfg).to(cfg.device)\n",
    "        optimizer = get_optimizer(model, cfg)\n",
    "        criterion = get_criterion(cfg)\n",
    "        \n",
    "        scheduler = get_scheduler(optimizer, cfg)\n",
    "        \n",
    "        best_auc = 0\n",
    "        best_epoch = 0\n",
    "\n",
    "        for epoch in range(cfg.epochs):\n",
    "            print(f\"\\ncur_epoch = {current_epo}\")\n",
    "            print(f\"\\nEpoch {epoch+1}/{cfg.epochs}\")\n",
    "\n",
    "            train_auc = None\n",
    "            train_loss, _ = train_1epo_with_ss_mixup(\n",
    "                model, \n",
    "                train_loader, \n",
    "                optimizer, \n",
    "                criterion, \n",
    "                cfg.device,\n",
    "                ss_loader=ss_loader,\n",
    "                mixuppipeline=mixuppipeline,\n",
    "                # rng=rng,\n",
    "                scheduler = scheduler if isinstance(scheduler, lr_scheduler.OneCycleLR) else None\n",
    "            )\n",
    "\n",
    "            val_loss, val_auc = validate(model, val_loader, criterion, cfg.device)\n",
    "\n",
    "            if scheduler is not None and not isinstance(scheduler, lr_scheduler.OneCycleLR):\n",
    "                if isinstance(scheduler, lr_scheduler.ReduceLROnPlateau):\n",
    "                    scheduler.step(val_loss)\n",
    "                else:\n",
    "                    scheduler.step()\n",
    "\n",
    "            train_auc = 0.0 if train_auc==None else train_auc\n",
    "\n",
    "            print(f\"Train Loss: {train_loss:.4f}, Train AUC: {train_auc:.4f}\")\n",
    "            print(f\"Val Loss: {val_loss:.4f}, Val AUC: {val_auc:.4f}\")\n",
    "            \n",
    "            val_loss_log = np.append(val_loss_log, np.array([[fold, epoch, train_loss, train_auc, val_loss, val_auc]]), axis=0)\n",
    "            file_path = os.path.join(cfg.OUTPUT_DIR, \"val_loss_log\")\n",
    "            np.save(file_path, val_loss_log)\n",
    "            print(f\"loss file saved! {file_path}\")\n",
    "\n",
    "\n",
    "            if val_auc > best_auc:\n",
    "                best_auc = val_auc\n",
    "                best_epoch = epoch + 1\n",
    "                print(f\"New best AUC: {best_auc:.4f} at epoch {best_epoch}\")\n",
    "\n",
    "                torch.save({\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "                    'epoch': epoch,\n",
    "                    'val_auc': val_auc,\n",
    "                    'train_auc': train_auc,\n",
    "                    'cfg': cfg\n",
    "                }, f\"model_fold{fold}.pth\")\n",
    "            current_epo += 1\n",
    "        \n",
    "        best_scores.append(best_auc)\n",
    "        print(f\"\\nBest AUC for fold {fold}: {best_auc:.4f} at epoch {best_epoch}\")\n",
    "        \n",
    "        # Clear memory\n",
    "        del model, optimizer, scheduler, train_loader, val_loader\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Cross-Validation Results:\")\n",
    "    for fold, score in enumerate(best_scores):\n",
    "        print(f\"Fold {cfg.selected_folds[fold]}: {score:.4f}\")\n",
    "    print(f\"Mean AUC: {np.mean(best_scores):.4f}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "def run_training_single_fold(train_df, cfg, ssdf = None, sslabeldf = None, rng=None):\n",
    "    \"\"\"Training function that can either use pre-computed spectrograms or generate them on-the-fly\"\"\"\n",
    "\n",
    "    cfg.num_classes = taxonomy_process(cfg.dir)[2]\n",
    "    \n",
    "    if cfg.debug:\n",
    "        cfg.update_debug_settings()\n",
    "\n",
    "    \n",
    "    if cfg.LOAD_DATA:\n",
    "        if 'filepath' not in train_df.columns:\n",
    "            train_df['filepath'] = cfg.train_datadir + '/' + train_df.filename\n",
    "        if 'samplename' not in train_df.columns:\n",
    "            train_df['samplename'] = train_df.filename.map(lambda x: x.split('/')[0] + '-' + x.split('/')[-1].split('.')[0])\n",
    "        \n",
    "    ss_dataset = SoundscapeFromNPY(ssdf, sslabeldf, cfg, augmentor=None, mode='train')\n",
    "\n",
    "    batch_sampler = infinite_batch_sampler(\n",
    "        ss_dataset, \n",
    "        batch_size=cfg.batch_size, \n",
    "        generator=torch.Generator().manual_seed(cfg.seed))\n",
    "\n",
    "    ss_loader = DataLoader(\n",
    "        ss_dataset,\n",
    "        #batch_size=cfg.batch_size,\n",
    "        batch_sampler=batch_sampler,\n",
    "        #shuffle=True,\n",
    "        num_workers=cfg.num_workers,\n",
    "        pin_memory=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    val_loss_log = np.empty((0, 3))\n",
    "    val_loss_log = np.append(val_loss_log, np.array([['epoch', 'train_loss', 'train_auc']]), axis=0)\n",
    "    # save columns\n",
    "\n",
    "    print(f'\\n{\"=\"*30} Single Fold {\"=\"*30}')\n",
    "    print(f'Training set: {len(train_df)} samples')\n",
    "\n",
    "    current_epo = 0\n",
    "    current_epo_fn = lambda : current_epo\n",
    "\n",
    "    dataaugmentor = AugmentationPipeline(cfg, current_epo_fn, rng=rng)\n",
    "    mixuppipeline = MixupPipeline(cfg, current_epo_fn, rng = rng)\n",
    "\n",
    "    train_dataset = BirdCLEFDatasetFromNPY(train_df, cfg, augmentor=dataaugmentor, mode='train')\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=cfg.batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=cfg.num_workers,\n",
    "        pin_memory=True,\n",
    "        collate_fn=collate_fn,\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "        \n",
    "    model = BirdCLEFModel(cfg).to(cfg.device)\n",
    "    optimizer = get_optimizer(model, cfg)\n",
    "    criterion = get_criterion(cfg)\n",
    "        \n",
    "    scheduler = get_scheduler(optimizer, cfg)\n",
    "        \n",
    "    for epoch in range(cfg.epochs):\n",
    "        #print(f\"\\ncur_epoch = {current_epo}\")\n",
    "        print(f\"\\nEpoch {epoch+1}/{cfg.epochs}\")\n",
    "\n",
    "\n",
    "        train_loss, _ = train_1epo_with_ss_mixup(\n",
    "            model, \n",
    "            train_loader, \n",
    "            optimizer, \n",
    "            criterion, \n",
    "            cfg.device,\n",
    "            ss_loader=ss_loader,\n",
    "            mixuppipeline=mixuppipeline,\n",
    "            # rng=rng,\n",
    "            scheduler = scheduler if isinstance(scheduler, lr_scheduler.OneCycleLR) else None\n",
    "        )\n",
    "\n",
    "        if scheduler is not None and not isinstance(scheduler, lr_scheduler.OneCycleLR):\n",
    "            if isinstance(scheduler, lr_scheduler.ReduceLROnPlateau):\n",
    "                raise Exception(\"Cannot use ReduceLROnPlateau since there's no validation\")\n",
    "            else:\n",
    "                scheduler.step()\n",
    "\n",
    "        train_auc = 0.0 if train_auc==None else train_auc\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train AUC: {train_auc:.4f}\")\n",
    "        \n",
    "        val_loss_log = np.append(val_loss_log, np.array([[epoch, train_loss, train_auc]]), axis=0)\n",
    "        file_path = os.path.join(cfg.OUTPUT_DIR, \"val_loss_log\")\n",
    "        np.save(file_path, val_loss_log)\n",
    "        print(f\"loss file saved! {file_path}\")\n",
    "\n",
    "        if epoch in cfg.selected_folds :\n",
    "\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "                'epoch': epoch,\n",
    "                'val_auc': 0.0, # val_auc not defined\n",
    "                'train_auc': train_auc,\n",
    "                'cfg': cfg\n",
    "            }, f\"model_epoch{epoch}.pth\")\n",
    "        current_epo += 1\n",
    "        \n",
    "    # Clear memory\n",
    "    del model, optimizer, scheduler, train_loader, val_loader\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "def run_1fold_0430(train_df, cfg, ssdf = None, sslabeldf = None, rng=None):\n",
    "    \"\"\"Training function that can either use pre-computed spectrograms or generate them on-the-fly\"\"\"\n",
    "\n",
    "    cfg.num_classes = taxonomy_process(cfg)[2]\n",
    "    \n",
    "    if cfg.debug:\n",
    "        cfg.update_debug_settings()\n",
    "\n",
    "    \n",
    "    if cfg.LOAD_DATA:\n",
    "        if 'filepath' not in train_df.columns:\n",
    "            train_df['filepath'] = cfg.train_datadir + '/' + train_df.filename\n",
    "        if 'samplename' not in train_df.columns:\n",
    "            train_df['samplename'] = train_df.filename.map(lambda x: x.split('/')[0] + '-' + x.split('/')[-1].split('.')[0])\n",
    "        \n",
    "    ss_dataset = SoundscapeFromNPY(ssdf, sslabeldf, cfg, augmentor=None, mode='train')\n",
    "\n",
    "    batch_sampler = infinite_batch_sampler(\n",
    "        ss_dataset, \n",
    "        batch_size=cfg.batch_size, \n",
    "        generator=torch.Generator().manual_seed(cfg.seed))\n",
    "\n",
    "    ss_loader = DataLoader(\n",
    "        ss_dataset,\n",
    "        #batch_size=cfg.batch_size,\n",
    "        batch_sampler=batch_sampler,\n",
    "        #shuffle=True,\n",
    "        num_workers=cfg.num_workers,\n",
    "        pin_memory=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    ss_loader2 = DataLoader(\n",
    "        ss_dataset,\n",
    "        batch_size=cfg.batch_size,\n",
    "        #batch_sampler=batch_sampler,\n",
    "        shuffle=True,\n",
    "        num_workers=cfg.num_workers,\n",
    "        pin_memory=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    val_loss_log = np.empty((0, 3))\n",
    "    val_loss_log = np.append(val_loss_log, np.array([['epoch', 'train_loss', 'train_auc']]), axis=0)\n",
    "    # save columns\n",
    "\n",
    "    print(f'\\n{\"=\"*30} Single Fold {\"=\"*30}')\n",
    "    print(f'Training set: {len(train_df)} samples')\n",
    "\n",
    "    current_epo = 0\n",
    "    current_epo_fn = lambda : current_epo\n",
    "\n",
    "    dataaugmentor = AugmentationPipeline(cfg, current_epo_fn, rng=rng)\n",
    "    mixuppipeline = MixupPipeline(cfg, current_epo_fn, rng = rng)\n",
    "\n",
    "    train_dataset = BirdCLEFDatasetFromNPY(train_df, cfg, augmentor=dataaugmentor, mode='train')\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=cfg.batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=cfg.num_workers,\n",
    "        pin_memory=True,\n",
    "        collate_fn=collate_fn,\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "        \n",
    "    model = BirdCLEFModel(cfg).to(cfg.device)\n",
    "    optimizer = get_optimizer(model, cfg)\n",
    "\n",
    "    criterion = get_criterion(cfg)\n",
    "        \n",
    "    scheduler = get_scheduler(optimizer, cfg)\n",
    "    model.load_model(cfg.checkpoint, optimizer, scheduler)\n",
    "\n",
    "    for epoch in range(cfg.epochs):\n",
    "        #print(f\"\\ncur_epoch = {current_epo}\")\n",
    "        print(f\"\\nEpoch {epoch+1}/{cfg.epochs}\")\n",
    "\n",
    "        if epoch < 20 :\n",
    "                \n",
    "\n",
    "            train_loss, _ = train_1epo_with_ss_mixup(\n",
    "                model, \n",
    "                train_loader, \n",
    "                optimizer, \n",
    "                criterion, \n",
    "                cfg.device,\n",
    "                ss_loader=ss_loader,\n",
    "                mixuppipeline=mixuppipeline,\n",
    "                # rng=rng,\n",
    "                scheduler = scheduler if isinstance(scheduler, lr_scheduler.OneCycleLR) else None\n",
    "            )\n",
    "\n",
    "            if scheduler is not None and not isinstance(scheduler, lr_scheduler.OneCycleLR):\n",
    "                if isinstance(scheduler, lr_scheduler.ReduceLROnPlateau):\n",
    "                    raise Exception(\"Cannot use ReduceLROnPlateau since there's no validation\")\n",
    "                else:\n",
    "                    scheduler.step()\n",
    "\n",
    "            train_auc = 0.0 if train_auc==None else train_auc\n",
    "\n",
    "            print(f\"Train Loss: {train_loss:.4f}, Train AUC: {train_auc:.4f}\")\n",
    "            \n",
    "            val_loss_log = np.append(val_loss_log, np.array([[epoch, train_loss, train_auc]]), axis=0)\n",
    "            file_path = os.path.join(cfg.OUTPUT_DIR, \"val_loss_log\")\n",
    "            np.save(file_path, val_loss_log)\n",
    "            print(f\"loss file saved! {file_path}\")\n",
    "\n",
    "            if epoch in cfg.selected_folds :\n",
    "\n",
    "                torch.save({\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "                    'epoch': epoch,\n",
    "                    'val_auc': 0.0, # val_auc not defined\n",
    "                    'train_auc': train_auc,\n",
    "                    'cfg': cfg\n",
    "                }, f\"model_epoch{epoch}.pth\")\n",
    "            current_epo += 1\n",
    "        else :\n",
    "\n",
    "            train_loss, _ = train_one_epoch(\n",
    "                model, \n",
    "                ss_loader2, \n",
    "                optimizer, \n",
    "                criterion, \n",
    "                cfg.device,\n",
    "                mixuppipeline=mixuppipeline,\n",
    "                # rng=rng,\n",
    "                scheduler = scheduler if isinstance(scheduler, lr_scheduler.OneCycleLR) else None\n",
    "            )\n",
    "\n",
    "            if scheduler is not None and not isinstance(scheduler, lr_scheduler.OneCycleLR):\n",
    "                if isinstance(scheduler, lr_scheduler.ReduceLROnPlateau):\n",
    "                    raise Exception(\"Cannot use ReduceLROnPlateau since there's no validation\")\n",
    "                else:\n",
    "                    scheduler.step()\n",
    "\n",
    "            train_auc = 0.0 if train_auc==None else train_auc\n",
    "\n",
    "            print(f\"Train Loss: {train_loss:.4f}, Train AUC: {train_auc:.4f}\")\n",
    "            \n",
    "            val_loss_log = np.append(val_loss_log, np.array([[epoch, train_loss, train_auc]]), axis=0)\n",
    "            file_path = os.path.join(cfg.OUTPUT_DIR, \"val_loss_log\")\n",
    "            np.save(file_path, val_loss_log)\n",
    "            print(f\"loss file saved! {file_path}\")\n",
    "\n",
    "            if epoch in cfg.selected_folds :\n",
    "\n",
    "                torch.save({\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "                    'epoch': epoch,\n",
    "                    'val_auc': 0.0, # val_auc not defined\n",
    "                    'train_auc': train_auc,\n",
    "                    'cfg': cfg\n",
    "                }, f\"model_epoch{epoch}.pth\")\n",
    "            current_epo += 1\n",
    "        \n",
    "    # Clear memory\n",
    "    del model, optimizer, scheduler, train_loader, val_loader\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0b3744e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkhparams (cfg) :\n",
    "    t_datadir = cfg.train_datadir.split(\"/\")[2]\n",
    "\n",
    "    try :\n",
    "        parsed = t_datadir.split(\"-\")[1:]\n",
    "        nfft, nmel, shapex, shapey = parsed # , hlength = parsed\n",
    "    except Exception as e :\n",
    "        print(e)\n",
    "    if int(parsed[0]) == cfg.N_FFT and int(parsed[1]) == cfg.N_MELS and int(parsed[2]) == cfg.TARGET_SHAPE[0] and int(parsed[3]) == cfg.TARGET_SHAPE[1] : #and int(parsed[4]) == cfg.HOP_LENGTH :\n",
    "        print(\"Config and dataset hyperparameter does match.\")\n",
    "        return\n",
    "\n",
    "    else :\n",
    "        print(parsed)\n",
    "        print(cfg.N_FFT, cfg.N_MELS, cfg.TARGET_SHAPE[0], cfg.TARGET_SHAPE[1], cfg.HOP_LENGTH)\n",
    "\n",
    "        raise Exception(\"Config and dataset hyperparameter does not match\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5d5eee",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdd4e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading training data...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/input/mel-1024-128-256-256/1024_128_256_256.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m rng \u001b[38;5;241m=\u001b[39m set_rng(cfg\u001b[38;5;241m.\u001b[39mgeneral[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mLoading training data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m train_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdir\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m ss_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(cfg\u001b[38;5;241m.\u001b[39mdir[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_sscsv\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     10\u001b[0m ss_label_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(cfg\u001b[38;5;241m.\u001b[39mdir[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_sssub\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/mel-1024-128-256-256/1024_128_256_256.csv'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import time\n",
    "\n",
    "    set_seed(cfg.seed)\n",
    "    rng = set_rng(cfg.seed)\n",
    "\n",
    "    print(\"\\nLoading training data...\")\n",
    "    train_df = pd.read_csv(cfg.train_csv)\n",
    "    ss_df = pd.read_csv(cfg.train_sscsv)\n",
    "    ss_label_df = pd.read_csv(cfg.train_sssub)\n",
    "    #checkhparams(cfg)\n",
    "\n",
    "    print(\"\\nStarting training...\")\n",
    "    \n",
    "    run_training_with_folds(train_df, cfg, ss_df, ss_label_df, rng)\n",
    "    \n",
    "    print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adbaf3c",
   "metadata": {},
   "source": [
    "### TO DO :\n",
    "\n",
    "- None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
