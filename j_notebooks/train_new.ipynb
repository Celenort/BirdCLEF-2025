{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T01:37:17.444810Z",
     "iopub.status.busy": "2025-05-07T01:37:17.444462Z",
     "iopub.status.idle": "2025-05-07T01:37:17.450682Z",
     "shell.execute_reply": "2025-05-07T01:37:17.449800Z",
     "shell.execute_reply.started": "2025-05-07T01:37:17.444780Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import random\n",
    "import gc\n",
    "import time\n",
    "import cv2\n",
    "import math\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import soundfile as sf\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import librosa\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler, BatchSampler\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import timm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T01:37:17.452170Z",
     "iopub.status.busy": "2025-05-07T01:37:17.451965Z",
     "iopub.status.idle": "2025-05-07T01:37:17.472192Z",
     "shell.execute_reply": "2025-05-07T01:37:17.471548Z",
     "shell.execute_reply.started": "2025-05-07T01:37:17.452151Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    \n",
    "    # Default settings\n",
    "    seed = 42\n",
    "    debug = False\n",
    "    apex = False\n",
    "    num_workers = 2\n",
    "    \n",
    "    OUTPUT_DIR = '/kaggle/working/'\n",
    "\n",
    "    train_datadir = '/kaggle/input/mel-1024-128-256-256/1024_128_256_256'\n",
    "    train_csv = '/kaggle/input/mel-1024-128-256-256/1024_128_256_256.csv'\n",
    "    train_oggdir = '/kaggle/input/birdclef-2025/train_audio'\n",
    "    # test_soundscapes = '../Data/test_soundscapes'\n",
    "    # submission_csv = '../Data/sample_submission.csv'\n",
    "    taxonomy_csv = '/kaggle/input/birdclef-2025/taxonomy.csv'\n",
    "    train_ssdir = '/kaggle/input/ssmel-1024-128-256-256-128/SOUNDSCAPE_1024_128_256_256_128'\n",
    "    train_sscsv = '/kaggle/input/ssmel-1024-128-256-256-128/SOUNDSCAPE_1024_128_256_256_128.csv'\n",
    "    train_sssub = '/kaggle/input/sub-trainss-1024-128-512-512/submission.csv'\n",
    "    train_ssoggdir = '/kaggle/input/birdclef-2025/train_soundscapes'\n",
    "    checkpoint = '/kaggle/input/tic0430e20/model_epoch20.pth'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    model_name = 'efficientnet_b0'  \n",
    "    pretrained = True\n",
    "    in_channels = 1\n",
    "\n",
    "    LOAD_DATA = False  # then, use on-the-fly spectrogram\n",
    "    FS = 32000\n",
    "    TARGET_DURATION = 5.0\n",
    "    TARGET_SHAPE = (256, 256)       ########### CHANGE!!\n",
    "    \n",
    "    N_FFT = 1024                    ########### CHANGE!!\n",
    "    HOP_LENGTH = 128  ########## CHANGE!!\n",
    "    N_MELS = 128  ########## CHANGE!!\n",
    "    FMIN = 20\n",
    "    FMAX = 16000\n",
    "    \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    epochs = 30  \n",
    "    batch_size = 32  \n",
    "    criterion = 'BCEWithLogitsLoss'\n",
    "\n",
    "    n_fold = 1\n",
    "    selected_folds = [17, 20, 23, 26, 29]   \n",
    "\n",
    "    optimizer = 'AdamW'\n",
    "    lr = 5e-4 \n",
    "    weight_decay = 1e-5\n",
    "  \n",
    "    scheduler = 'CosineAnnealingLR'\n",
    "    min_lr = 1e-6\n",
    "    T_max = epochs // 4\n",
    "\n",
    "    ss_label_smooth_temp = 5.0 # when loading dataset, smooth pseudo-labeled data with sigmoid(data/T)\n",
    "    ss_label_threhold = 0.3 # based on original prediction, filters dataset\n",
    "\n",
    "    augmentation = True\n",
    "    aug_scheduler = 'Ramp' # Constant, Ramp, Exp\n",
    "    aug_weight_x = 0.5\n",
    "    aug_weight_y = 0.5\n",
    "\n",
    "    mixup_mode = \"Soundscape\" \n",
    "    # \"Soundscape\" : mixup train data with soundscape\n",
    "    # \"Train\" : mixup train data by batch\n",
    "    # \"Disabled\" mixup mode disabled\n",
    "    mixup_alpha = [0.5, 4.0]  # float or list of float with size 2 \n",
    "    # provide : [initial alpha for ss, final alpha for ss]\n",
    "    # alternatively, just give a float for fixed alpha (treats train data, ss equally)\n",
    "    mixup_scheduler = 'Ramp'  # Constant, Ramp, Exp\n",
    "    \n",
    "    def update_debug_settings(self):\n",
    "        if self.debug:\n",
    "            self.epochs = 23\n",
    "            #self.selected_folds = [2]\n",
    "\n",
    "cfg = CFG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T01:37:17.473710Z",
     "iopub.status.busy": "2025-05-07T01:37:17.473453Z",
     "iopub.status.idle": "2025-05-07T01:37:17.491371Z",
     "shell.execute_reply": "2025-05-07T01:37:17.490693Z",
     "shell.execute_reply.started": "2025-05-07T01:37:17.473689Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AugmentationPipeline :\n",
    "    def __init__(self, config, current_epo_fn, rng=None) :\n",
    "        if not config.augmentation :\n",
    "            print(\"Augmentation disabled.\")\n",
    "            return\n",
    "        self.max_epoch = config.epochs\n",
    "        self.current_epo_fn = current_epo_fn\n",
    "        self.weight_dict = {\n",
    "            'xmask' : config.aug_weight_x,\n",
    "            'ymask' : config.aug_weight_y\n",
    "        }\n",
    "        self.aug_scheduler = config.aug_scheduler\n",
    "        self.aug_delay = 0   # augmentation starts at # epo.\n",
    "        self.p_max = 0.5 # maximum augmentation proba\n",
    "        self.exp_decay = 5.0 # decay constant when using 'Exp' scheduler\n",
    "        if rng == None :\n",
    "            self.rng = np.random.RandomState()\n",
    "        else :\n",
    "            self.rng = rng\n",
    "    def _schedule_p(self) :\n",
    "        D = self.aug_delay\n",
    "        T = self.max_epoch\n",
    "        t = self.current_epo_fn()\n",
    "        p_max = self.p_max\n",
    "\n",
    "        if t<D :\n",
    "            return 0.0\n",
    "        elif t > T :\n",
    "            return p_max\n",
    "        else :\n",
    "            if self.aug_scheduler == 'Constant' :\n",
    "                p = p_max\n",
    "            elif self.aug_scheduler == 'Ramp' :\n",
    "                p = (t-D)/(T-D) * p_max\n",
    "            elif self.aug_scheduler == 'Exp' :\n",
    "                c = self.exp_decay\n",
    "                p = p_max / (1-np.exp(-(T-D)*c))* (1-np.exp(-(t-D)*c))\n",
    "            else :\n",
    "                print(f\"Specified {self.aug_scheduler} not defined. Use 'Constant', 'Ramp' and 'Exp'\")\n",
    "                raise Exception(\"Aug Scheduler type not implemented\")\n",
    "            return p\n",
    "    \n",
    "    def __call__(self, spec):\n",
    "        \"\"\"Apply augmentations to spectrogram\"\"\"\n",
    "\n",
    "        p = self._schedule_p()\n",
    "\n",
    "        if self.rng.uniform() < p :\n",
    "\n",
    "            # Time masking (horizontal stripes)\n",
    "            if self.rng.uniform() < self.weight_dict['xmask']:\n",
    "                num_masks = self.rng.randint(1, 3)\n",
    "                #print(\"Debug : Xmask applied\")\n",
    "                for _ in range(num_masks):\n",
    "                    width = self.rng.randint(5, 20)\n",
    "                    start = self.rng.randint(0, spec.shape[2] - width)\n",
    "                    spec[0, :, start:start+width] = 0\n",
    "            \n",
    "            # Frequency masking (vertical stripes)\n",
    "            if self.rng.uniform() < self.weight_dict['ymask']:\n",
    "                num_masks = self.rng.randint(1, 3)\n",
    "                #print(\"Debug : Ymask applied\")\n",
    "                for _ in range(num_masks):\n",
    "                    height = self.rng.randint(5, 20)\n",
    "                    start = self.rng.randint(0, spec.shape[1] - height)\n",
    "                    spec[0, start:start+height, :] = 0\n",
    "      \n",
    "        return spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T01:37:17.562013Z",
     "iopub.status.busy": "2025-05-07T01:37:17.561779Z",
     "iopub.status.idle": "2025-05-07T01:37:17.571745Z",
     "shell.execute_reply": "2025-05-07T01:37:17.570956Z",
     "shell.execute_reply.started": "2025-05-07T01:37:17.561993Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MixupPipeline :\n",
    "    def __init__(self, config, current_epo_fn, rng=None) :\n",
    "\n",
    "        self.mixup_delay = 10\n",
    "        self.p_max = 1.0\n",
    "        self.exp_decay = 5.0\n",
    "        \n",
    "        if isinstance(config.mixup_alpha, float) :\n",
    "            # use fixed mixup alpha\n",
    "            self.alpha_initial = config.mixup_alpha\n",
    "            self.alpha_final = config.mixup_alpha\n",
    "        else :\n",
    "            self.alpha_initial = config.mixup_alpha[0]\n",
    "            self.alpha_final = config.mixup_alpha[1]\n",
    "\n",
    "        self.mixup_mode = config.mixup_mode # do we need this? YES!\n",
    "\n",
    "        self.current_epo_fn = current_epo_fn\n",
    "        self.mixup_scheduler = config.mixup_scheduler\n",
    "        self.total_epo = 20 # config.epochs\n",
    "        \n",
    "        if rng == None :\n",
    "            self.rng = np.random.RandomState()\n",
    "        else :\n",
    "            self.rng = rng\n",
    "    \n",
    "    def _get_lambda(self) :\n",
    "        epochratio = (self.current_epo_fn()-self.mixup_delay) / (self.total_epo-self.mixup_delay)\n",
    "        # form of (t-D) / (T-D)\n",
    "\n",
    "        # defining original data's alpha\n",
    "        alpha_1 = self.alpha_initial * (1.0-epochratio) + self.alpha_final * epochratio\n",
    "        alpha_2 = self.alpha_initial * epochratio + self.alpha_final * (1.0-epochratio)\n",
    "\n",
    "        return self.rng.beta(alpha_1, alpha_2)\n",
    "\n",
    "    def mixup_data(self, x_orig, x_ss):\n",
    "        \"\"\"Applies mixup to the data batch\"\"\"\n",
    "        lam = self._get_lambda()\n",
    "        mixed_x = lam * x_orig + (1 - lam) * x_ss\n",
    "        return mixed_x, lam\n",
    "    \n",
    "    def mixup_criterion(self, pred, y_orig, y_ss, lam, criterion = None):\n",
    "        \"\"\"Applies mixup to the loss function\"\"\"\n",
    "        # criterion = F.binary_cross_entropy_with_logits\n",
    "        if criterion == None :\n",
    "            criterion = F.binary_cross_entropy_with_logits\n",
    "\n",
    "        return lam * criterion(pred, y_orig) + (1 - lam) * criterion(pred, y_ss)\n",
    "    def schedule_p(self) :\n",
    "        D = self.mixup_delay\n",
    "        T = self.total_epo\n",
    "        t = self.current_epo_fn()\n",
    "        p_max = self.p_max\n",
    "\n",
    "        if t<D :\n",
    "            return 0.0\n",
    "        elif t > T :\n",
    "            return p_max\n",
    "        else :\n",
    "            if self.mixup_scheduler == 'Constant' :\n",
    "                p = p_max\n",
    "            elif self.mixup_scheduler == 'Ramp' :\n",
    "                p = (t-D)/(T-D) * p_max\n",
    "            elif self.mixup_scheduler == 'Exp' :\n",
    "                c = self.exp_decay\n",
    "                p = p_max / (1-np.exp(-(T-D)*c))* (1-np.exp(-(t-D)*c))\n",
    "            else :\n",
    "                print(f\"Specified {self.mixup_scheduler} not defined. Use 'Constant', 'Ramp' and 'Exp'\")\n",
    "                raise Exception(\"Aug Scheduler type not implemented\")\n",
    "            return p\n",
    "    def old_mixup_data(self, x, targets):\n",
    "        \"\"\"Applies mixup to the data batch\"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        lam = self._get_lambda()\n",
    "\n",
    "        indices = torch.randperm(batch_size).to(x.device)\n",
    "\n",
    "        mixed_x = lam * x + (1 - lam) * x[indices]\n",
    "        \n",
    "        return mixed_x, targets, targets[indices], lam\n",
    "    \n",
    "    def old_mixup_criterion(self, criterion, pred, y_a, y_b, lam):\n",
    "        \"\"\"Applies mixup to the loss function\"\"\"\n",
    "        return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T01:37:17.574714Z",
     "iopub.status.busy": "2025-05-07T01:37:17.574498Z",
     "iopub.status.idle": "2025-05-07T01:37:17.591957Z",
     "shell.execute_reply": "2025-05-07T01:37:17.591166Z",
     "shell.execute_reply.started": "2025-05-07T01:37:17.574682Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    \"\"\"\n",
    "    Set seed for reproducibility\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def set_rng(seed):\n",
    "    return np.random.RandomState(seed)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function to handle different sized spectrograms\"\"\"\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if len(batch) == 0:\n",
    "        return {}\n",
    "        \n",
    "    result = {key: [] for key in batch[0].keys()}\n",
    "    \n",
    "    for item in batch:\n",
    "        for key, value in item.items():\n",
    "            result[key].append(value)\n",
    "    \n",
    "    for key in result:\n",
    "        if key == 'target' and isinstance(result[key][0], torch.Tensor):\n",
    "            result[key] = torch.stack(result[key])\n",
    "        elif key == 'melspec' and isinstance(result[key][0], torch.Tensor):\n",
    "            shapes = [t.shape for t in result[key]]\n",
    "            if len(set(str(s) for s in shapes)) == 1:\n",
    "                result[key] = torch.stack(result[key])\n",
    "    \n",
    "    return result\n",
    "\n",
    "def taxonomy_process (cfg) :\n",
    "    \"\"\"\n",
    "    returns tuple of (taxonomy_df, species_ids, num_classes, label_to_idx)\n",
    "    \"\"\"\n",
    "    taxonomy_df = pd.read_csv(cfg.taxonomy_csv)\n",
    "    species_ids = taxonomy_df['primary_label'].tolist()\n",
    "    num_classes = len(species_ids)\n",
    "    label_to_idx = {label: idx for idx, label in enumerate(species_ids)}\n",
    "    return (taxonomy_df, species_ids, num_classes, label_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "These functions handle the transformation of audio files to mel spectrograms for model input, with flexibility controlled by the `LOAD_DATA` parameter. The process involves either loading pre-computed spectrograms from this [dataset](https://www.kaggle.com/datasets/kadircandrisolu/birdclef25-mel-spectrograms) (when `LOAD_DATA=True`) or dynamically generating them (when `LOAD_DATA=False`), transforming audio data into spectrogram representations, and preparing it for the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T01:37:17.593202Z",
     "iopub.status.busy": "2025-05-07T01:37:17.592966Z",
     "iopub.status.idle": "2025-05-07T01:37:17.614282Z",
     "shell.execute_reply": "2025-05-07T01:37:17.613591Z",
     "shell.execute_reply.started": "2025-05-07T01:37:17.593183Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def audio2melspec(audio_data, cfg):\n",
    "    \"\"\"Convert audio data to mel spectrogram\"\"\"\n",
    "    if np.isnan(audio_data).any():\n",
    "        mean_signal = np.nanmean(audio_data)\n",
    "        audio_data = np.nan_to_num(audio_data, nan=mean_signal)\n",
    "\n",
    "    mel_spec = librosa.feature.melspectrogram(\n",
    "        y=audio_data,\n",
    "        sr=cfg.FS,\n",
    "        n_fft=cfg.N_FFT,\n",
    "        hop_length=cfg.HOP_LENGTH,\n",
    "        n_mels=cfg.N_MELS,\n",
    "        fmin=cfg.FMIN,\n",
    "        fmax=cfg.FMAX,\n",
    "        power=2.0\n",
    "    )\n",
    "\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    mel_spec_norm = (mel_spec_db - mel_spec_db.min()) / (mel_spec_db.max() - mel_spec_db.min() + 1e-8)\n",
    "    \n",
    "    return mel_spec_norm\n",
    "\n",
    "def process_audio_file(audio_path, cfg):\n",
    "    \"\"\"Process a single audio file to get the mel spectrogram\"\"\"\n",
    "    try:\n",
    "        #audio_data, _ = librosa.load(audio_path, sr=cfg.FS)\n",
    "        audio_data, _ = sf.read(audio_path, dtype='float32')\n",
    "        # must plot data values\n",
    "\n",
    "        target_samples = int(cfg.TARGET_DURATION * cfg.FS)\n",
    "\n",
    "        if len(audio_data) < target_samples:\n",
    "            n_copy = math.ceil(target_samples / len(audio_data))\n",
    "            if n_copy > 1:\n",
    "                audio_data = np.concatenate([audio_data] * n_copy)\n",
    "\n",
    "        # Extract center 5 seconds\n",
    "        start_idx = max(0, int(len(audio_data) / 2 - target_samples / 2))\n",
    "        end_idx = min(len(audio_data), start_idx + target_samples)\n",
    "        center_audio = audio_data[start_idx:end_idx]\n",
    "\n",
    "        if len(center_audio) < target_samples:\n",
    "            center_audio = np.pad(center_audio, \n",
    "                                 (0, target_samples - len(center_audio)), \n",
    "                                 mode='constant')\n",
    "\n",
    "        mel_spec = audio2melspec(center_audio, cfg)\n",
    "        \n",
    "        if mel_spec.shape != cfg.TARGET_SHAPE:\n",
    "            mel_spec = cv2.resize(mel_spec, cfg.TARGET_SHAPE, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "        return mel_spec.astype(np.float32)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def generate_spectrograms(df, cfg):\n",
    "    \"\"\"Generate spectrograms from audio files\"\"\"\n",
    "    print(\"Generating mel spectrograms from audio files...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    all_bird_data = {}\n",
    "    errors = []\n",
    "\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        if cfg.debug and i >= 1000:\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            samplename = row['samplename']\n",
    "            filepath = row['filepath']\n",
    "            \n",
    "            mel_spec = process_audio_file(filepath, cfg)\n",
    "            \n",
    "            if mel_spec is not None:\n",
    "                all_bird_data[samplename] = mel_spec\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {row.filepath}: {e}\")\n",
    "            errors.append((row.filepath, str(e)))\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Processing completed in {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"Successfully processed {len(all_bird_data)} files out of {len(df)}\")\n",
    "    print(f\"Failed to process {len(errors)} files\")\n",
    "    \n",
    "    return all_bird_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation and Data Augmentations\n",
    "We'll convert audio to mel spectrograms and apply random augmentations with 50% probability each - including time stretching, pitch shifting, and volume adjustments. This randomized approach creates diverse training samples from the same audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T01:37:17.615766Z",
     "iopub.status.busy": "2025-05-07T01:37:17.615537Z",
     "iopub.status.idle": "2025-05-07T01:37:17.632106Z",
     "shell.execute_reply": "2025-05-07T01:37:17.631406Z",
     "shell.execute_reply.started": "2025-05-07T01:37:17.615737Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BirdCLEFDatasetFromNPY(Dataset):\n",
    "    def __init__(self, df, cfg, augmentor = None, mode=\"train\"):\n",
    "        self.df = df\n",
    "        self.cfg = cfg\n",
    "        self.mode = mode\n",
    "        self.augmentor = augmentor\n",
    "        \n",
    "        _ , self.species_ids, self.num_classes, self.label_to_idx = taxonomy_process(cfg)\n",
    "        \n",
    "        if 'samplename' not in self.df.columns:\n",
    "            self.df['samplename'] = self.df.filename.map(lambda x: x.split('/')[0] + '-' + x.split('/')[-1].split('.')[0])\n",
    "\n",
    "        if 'melpath' not in self.df.columns:\n",
    "            self.df['melpath'] = self.cfg.train_datadir + '/' + self.df['samplename']\n",
    "\n",
    "        if cfg.debug:\n",
    "            self.df = self.df.sample(min(1000, len(self.df)), random_state=cfg.seed).reset_index(drop=True)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        spec = np.load(row['melpath']).astype(np.float32)\n",
    "        \n",
    "        spec = torch.tensor(spec, dtype=torch.float32).unsqueeze(0)  # Add channel dimension\n",
    "\n",
    "        if self.mode == \"train\" and self.cfg.augmentation :\n",
    "            spec = self.augmentor(spec)\n",
    "        \n",
    "        target = self.encode_label(row['primary_label'], row['secondary_labels']) \n",
    "        \n",
    "        return {\n",
    "            'melspec': spec, \n",
    "            'target': torch.tensor(target, dtype=torch.float32),\n",
    "            'filename': row['filename'],\n",
    "            'melpath' : row['melpath']\n",
    "        }\n",
    "    \n",
    "    def encode_label(self, label1, label2=None):\n",
    "        \"\"\"Encode label to multi-hot vector\"\"\"\n",
    "        target = np.zeros(self.num_classes)\n",
    "        if label1 in self.label_to_idx:\n",
    "            target[self.label_to_idx[label1]] = 1.0\n",
    "        if label2 :\n",
    "            if isinstance(label2, str):\n",
    "                l2 = eval(label2)\n",
    "            for label in l2:\n",
    "                if label in self.label_to_idx:\n",
    "                    target[self.label_to_idx[label]] = 1.0\n",
    "\n",
    "        return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T01:37:17.633088Z",
     "iopub.status.busy": "2025-05-07T01:37:17.632872Z",
     "iopub.status.idle": "2025-05-07T01:37:17.646436Z",
     "shell.execute_reply": "2025-05-07T01:37:17.645601Z",
     "shell.execute_reply.started": "2025-05-07T01:37:17.633070Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SoundscapeFromNPY(Dataset):\n",
    "    def __init__(self, df,labeldf, cfg, augmentor = None, mode=\"train\"):\n",
    "        self.df = df\n",
    "        self.cfg = cfg\n",
    "        self.mode = mode\n",
    "        self.augmentor = augmentor\n",
    "        self.ldf = labeldf\n",
    "        self.threshold = cfg.ss_label_threhold\n",
    "        \n",
    "        _, self.species_ids, self.num_classes, self.label_to_idx = taxonomy_process(cfg)\n",
    "\n",
    "        self.temperature = cfg.ss_label_smooth_temp\n",
    "\n",
    "# cfg.train_ssdir = '../input/SOUNDSCAPE_1024_128_256_256\n",
    "# filedir = ./Data/train_soundscapes/H29_20230523_194000.ogg\n",
    "# filename = H29_20230523_194000.ogg\n",
    "\n",
    "        if 'melpath' not in self.df.columns:\n",
    "            self.df['melpath'] = self.cfg.train_ssdir + '/' + self.df['filename'] + '-' + self.df['index'].astype(str) + '.npy'\n",
    "            # SSDIR / H*****.ogg-index.npy\n",
    "        \n",
    "        if 'samplename' not in self.df.columns:\n",
    "            self.df['samplename'] = self.df['filename'] + '-' + self.df['index'].astype(str) + '.npy'\n",
    "\n",
    "        if cfg.debug:\n",
    "            self.df = df.sample(min(1000, len(df)), random_state=cfg.seed).reset_index(drop=True)\n",
    "            self.ldf = labeldf.loc[self.df.index].reset_index(drop=True)\n",
    "        else:\n",
    "            self.df = df\n",
    "            self.ldf = labeldf\n",
    "\n",
    "        self.df, self.ldf = self._filter_df_by_threshold(self.df, self.ldf)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        lrow = self.ldf.iloc[idx][1:]\n",
    "        spec = np.load(row['melpath']).astype(np.float32)\n",
    "        \n",
    "        spec = torch.tensor(spec, dtype=torch.float32).unsqueeze(0)  # Add channel dimension\n",
    "        \n",
    "        target = self._parse_label(lrow, self.temperature) \n",
    "        \n",
    "        return {\n",
    "            'melspec': spec, \n",
    "            'target': torch.tensor(target, dtype=torch.float32),\n",
    "            'filename': row['filename'],\n",
    "            'melpath' : row['melpath']\n",
    "            }\n",
    "        \n",
    "    def _filter_df_by_threshold(self, df, labeldf):\n",
    "        \"\"\"\n",
    "        Filters df using self.threshold based on max value of parsed label.\n",
    "        Prints number of kept and dropped samples.\n",
    "        \"\"\"\n",
    "        filtered_indices = []\n",
    "        for idx in range(len(df)):\n",
    "            lrow = labeldf.iloc[idx][1:]\n",
    "            if np.max(lrow) >= self.threshold:\n",
    "                filtered_indices.append(idx)\n",
    "\n",
    "        filtered_df = df.iloc[filtered_indices].reset_index(drop=True)\n",
    "        filtered_labeldf = labeldf.iloc[filtered_indices].reset_index(drop=True)\n",
    "\n",
    "        num_total = len(df)\n",
    "        num_kept = len(filtered_df)\n",
    "\n",
    "        print(f\"Filtered samples with threshold {self.threshold}:\")\n",
    "        print(f\"  Kept: {num_kept} / {num_total} samples\")\n",
    "\n",
    "        return filtered_df, filtered_labeldf\n",
    "\n",
    "\n",
    "    def _prob2tprob(self, target, T, eps=1e-6):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            probabilities: np.array of shape (B, C), values in (0, 1)\n",
    "            T: temperature (T > 0)\n",
    "            eps: small constant to prevent log(0)\n",
    "        Returns:\n",
    "            softened probabilities in (0, 1)\n",
    "        \"\"\"\n",
    "        # Logit Transformation: log(p / (1 - p))\n",
    "        logits = np.log((target + eps) / (1 - target + eps))\n",
    "\n",
    "        # Temperature scaling\n",
    "        softened_logits = logits / T\n",
    "\n",
    "        # apply sigmoid again\n",
    "        softened_probs = 1 / (1 + np.exp(-softened_logits))\n",
    "\n",
    "        return softened_probs\n",
    "    \n",
    "    def _parse_label(self, lrow, T):\n",
    "        \"\"\"\n",
    "        Parse and apply sigmoid with temperature T\n",
    "        \"\"\"\n",
    "        target = np.zeros(self.num_classes)\n",
    "\n",
    "        for col in lrow.index :\n",
    "            target[self.label_to_idx[col]] = lrow[col]\n",
    "\n",
    "        target = self._prob2tprob(target, T)\n",
    "        return target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T01:37:17.647639Z",
     "iopub.status.busy": "2025-05-07T01:37:17.647343Z",
     "iopub.status.idle": "2025-05-07T01:37:17.663114Z",
     "shell.execute_reply": "2025-05-07T01:37:17.662420Z",
     "shell.execute_reply.started": "2025-05-07T01:37:17.647611Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class InfiniteRandomSampler(Sampler):\n",
    "    def __init__(self, data_source, generator=None):\n",
    "        self.data_source = data_source\n",
    "        self.generator = generator or torch.Generator()\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            idx = torch.randint(\n",
    "                high=len(self.data_source),\n",
    "                size=(1,),\n",
    "                generator=self.generator\n",
    "            ).item()\n",
    "            yield idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return 2**31  # Arbitrarily large number\n",
    "\n",
    "def infinite_batch_sampler(dataset, batch_size, generator=None):\n",
    "    infinite_sampler = InfiniteRandomSampler(dataset, generator=generator)\n",
    "    return BatchSampler(infinite_sampler, batch_size=batch_size, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T01:37:17.664045Z",
     "iopub.status.busy": "2025-05-07T01:37:17.663804Z",
     "iopub.status.idle": "2025-05-07T01:37:17.677628Z",
     "shell.execute_reply": "2025-05-07T01:37:17.676854Z",
     "shell.execute_reply.started": "2025-05-07T01:37:17.664022Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BirdCLEFModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        cfg.num_classes = taxonomy_process(cfg)[2]\n",
    "\n",
    "\n",
    "    \n",
    "        self.backbone = timm.create_model(\n",
    "            cfg.model_name,\n",
    "            pretrained=cfg.pretrained,\n",
    "            in_chans=cfg.in_channels,\n",
    "            drop_rate=0.2,\n",
    "            drop_path_rate=0.2\n",
    "        )\n",
    "        \n",
    "        if 'efficientnet' in cfg.model_name:\n",
    "            backbone_out = self.backbone.classifier.in_features\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "        elif 'resnet' in cfg.model_name:\n",
    "            backbone_out = self.backbone.fc.in_features\n",
    "            self.backbone.fc = nn.Identity()\n",
    "        else:\n",
    "            backbone_out = self.backbone.get_classifier().in_features\n",
    "            self.backbone.reset_classifier(0, '')\n",
    "        \n",
    "        self.pooling = nn.AdaptiveAvgPool2d(1)\n",
    "            \n",
    "        self.feat_dim = backbone_out\n",
    "        \n",
    "        self.classifier = nn.Linear(backbone_out, cfg.num_classes)\n",
    "            \n",
    "    def forward(self, x, targets=None):\n",
    "        \n",
    "        features = self.backbone(x)\n",
    "        \n",
    "        if isinstance(features, dict):\n",
    "            features = features['features']\n",
    "            \n",
    "        if len(features.shape) == 4:\n",
    "            features = self.pooling(features)\n",
    "            features = features.view(features.size(0), -1)\n",
    "        \n",
    "        logits = self.classifier(features)\n",
    "            \n",
    "        return logits\n",
    "\n",
    "    def load_model(self, filepath, optimizer, scheduler):\n",
    "        # Step 1: Checkpoint 불러오기\n",
    "        checkpoint = torch.load(filepath, map_location=self.cfg.device)\n",
    "        state_dict = checkpoint['model_state_dict']\n",
    "\n",
    "        # Step 2: (중요!) 모델 전체 구조에 맞게 state_dict 로드\n",
    "        self.load_state_dict(state_dict)\n",
    "        print(\"State dict Loaded\")\n",
    "        \n",
    "\n",
    "        # Step 3: Optimizer 복원\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        print(\"Optimizer State dict Loaded\")\n",
    "\n",
    "        # Step 4: Scheduler 복원\n",
    "        if checkpoint['scheduler_state_dict'] is not None:\n",
    "            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        print(\"Checkpoint State dict Loaded\")\n",
    "\n",
    "        # Step 5: 필요한 추가 정보 리턴\n",
    "        return checkpoint['epoch']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Utilities\n",
    "We are configuring our optimization strategy with the AdamW optimizer, cosine scheduling, and the BCEWithLogitsLoss criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T01:37:17.678620Z",
     "iopub.status.busy": "2025-05-07T01:37:17.678376Z",
     "iopub.status.idle": "2025-05-07T01:37:17.695036Z",
     "shell.execute_reply": "2025-05-07T01:37:17.694203Z",
     "shell.execute_reply.started": "2025-05-07T01:37:17.678601Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_optimizer(model, cfg):\n",
    "  \n",
    "    if cfg.optimizer == 'Adam':\n",
    "        optimizer = optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=cfg.lr,\n",
    "            weight_decay=cfg.weight_decay\n",
    "        )\n",
    "    elif cfg.optimizer == 'AdamW':\n",
    "        optimizer = optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=cfg.lr,\n",
    "            weight_decay=cfg.weight_decay\n",
    "        )\n",
    "    elif cfg.optimizer == 'SGD':\n",
    "        optimizer = optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr=cfg.lr,\n",
    "            momentum=0.9,\n",
    "            weight_decay=cfg.weight_decay\n",
    "        )\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Optimizer {cfg.optimizer} not implemented\")\n",
    "        \n",
    "    return optimizer\n",
    "\n",
    "def get_scheduler(optimizer, cfg):\n",
    "   \n",
    "    if cfg.scheduler == 'CosineAnnealingLR':\n",
    "        scheduler = lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=cfg.T_max,\n",
    "            eta_min=cfg.min_lr\n",
    "        )\n",
    "    elif cfg.scheduler == 'ReduceLROnPlateau':\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='min',\n",
    "            factor=0.5,\n",
    "            patience=2,\n",
    "            min_lr=cfg.min_lr,\n",
    "            verbose=True\n",
    "        )\n",
    "    elif cfg.scheduler == 'StepLR':\n",
    "        scheduler = lr_scheduler.StepLR(\n",
    "            optimizer,\n",
    "            step_size=cfg.epochs // 3,\n",
    "            gamma=0.5\n",
    "        )\n",
    "    elif cfg.scheduler == 'OneCycleLR':\n",
    "        scheduler = lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=cfg.lr,\n",
    "            steps_per_epoch=len(train_loader),\n",
    "            epochs=cfg.epochs,\n",
    "            pct_start=0.1\n",
    "        )\n",
    "    elif cfg.scheduler == 'CosineAnnealingLRwithWarmup' :\n",
    "        warmup_epochs = 5\n",
    "        warmpup_scheduler = lr_scheduler.LambdaLR(\n",
    "            optimizer, \n",
    "            lr_lambda=lambda epoch: epoch / warmup_epochs)\n",
    "        cosine_scheduler = lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=cfg.T_max,\n",
    "            eta_min=cfg.min_lr\n",
    "        )\n",
    "        scheduler = lr_scheduler.SequentialLR(\n",
    "            optimizer, \n",
    "            schedulers=[warmup_scheduler, cosine_scheduler],\n",
    "            milestones=[warmup_epochs]\n",
    "        )\n",
    "    else:\n",
    "        scheduler = None\n",
    "        \n",
    "    return scheduler\n",
    "\n",
    "def get_criterion(cfg):\n",
    " \n",
    "    if cfg.criterion == 'BCEWithLogitsLoss':\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Criterion {cfg.criterion} not implemented\")\n",
    "        \n",
    "    return criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T01:37:17.696397Z",
     "iopub.status.busy": "2025-05-07T01:37:17.696140Z",
     "iopub.status.idle": "2025-05-07T01:37:17.729530Z",
     "shell.execute_reply": "2025-05-07T01:37:17.728743Z",
     "shell.execute_reply.started": "2025-05-07T01:37:17.696377Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, criterion, device, mixuppipeline, scheduler=None):\n",
    "    \n",
    "    model.train()\n",
    "    losses = []\n",
    "    all_targets = []\n",
    "    all_outputs = []\n",
    "    \n",
    "    enablemixup = False\n",
    "    if mixuppipeline.mixup_mode == \"Train\" :\n",
    "        enablemixup = True\n",
    "\n",
    "\n",
    "    pbar = tqdm(enumerate(loader), total=len(loader), desc=\"Training\")\n",
    "    \n",
    "    for step, batch in pbar:\n",
    "    \n",
    "        if isinstance(batch['melspec'], list):\n",
    "            batch_outputs = []\n",
    "            batch_losses = []\n",
    "            \n",
    "            for i in range(len(batch['melspec'])):\n",
    "                inputs = batch['melspec'][i].unsqueeze(0).to(device)\n",
    "                target = batch['target'][i].unsqueeze(0).to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                if enablemixup and mixuppipeline.rng.uniform()> mixuppipeline.schedule_p() :\n",
    "                    mixed_inputs, target, target_idx, lam = mixuppipeline.old_mixup_data(inputs, target)\n",
    "                    output = model(mixed_inputs)\n",
    "                    loss = mixuppipeline.old_mixup_criterion(criterion, output, target, target_idx, lam)\n",
    "                else :\n",
    "                    output = model(inputs)\n",
    "                    loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                \n",
    "                batch_outputs.append(output.detach().cpu())\n",
    "                batch_losses.append(loss.item())\n",
    "            \n",
    "            optimizer.step()\n",
    "            outputs = torch.cat(batch_outputs, dim=0).numpy()\n",
    "            loss = np.mean(batch_losses)\n",
    "            targets = batch['target'].numpy()\n",
    "            \n",
    "        else:\n",
    "            inputs = batch['melspec'].to(device)\n",
    "            targets = batch['target'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            if enablemixup and mixuppipeline.rng.uniform()> mixuppipeline.schedule_p() :\n",
    "                mixed_inputs, target, target_idx, lam = mixuppipeline.old_mixup_data(inputs, target)\n",
    "                output = model(mixed_inputs)\n",
    "                loss = mixuppipeline.old_mixup_criterion(criterion, output, target, target_idx, lam)\n",
    "            else :\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            outputs = outputs.detach().cpu().numpy()\n",
    "            targets = targets.detach().cpu().numpy()\n",
    "        \n",
    "        if scheduler is not None and isinstance(scheduler, lr_scheduler.OneCycleLR):\n",
    "            scheduler.step()\n",
    "            \n",
    "        all_outputs.append(outputs)\n",
    "        all_targets.append(targets)\n",
    "        losses.append(loss if isinstance(loss, float) else loss.item())\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'train_loss': np.mean(losses[-10:]) if losses else 0,\n",
    "            'lr': optimizer.param_groups[0]['lr']\n",
    "        })\n",
    "    \n",
    "    all_outputs = np.concatenate(all_outputs)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    auc = 0.0 #  calculate_auc(all_targets, all_outputs)\n",
    "    avg_loss = np.mean(losses)\n",
    "    \n",
    "    return avg_loss, auc\n",
    "\n",
    "# Treat ss data as normal data, one epo is just concatenation of [train_loader, ss_loader]\n",
    "def train_1epo_with_ss(model, loader, optimizer, criterion, device, ss_loader, mixuppipeline, scheduler=None):\n",
    "    \n",
    "    model.train()\n",
    "    losses = []\n",
    "    sslosses = []\n",
    "    all_targets = []\n",
    "    all_outputs = []\n",
    "\n",
    "    enablemixup = False\n",
    "    if mixuppipeline.mixup_mode == \"Train\" :\n",
    "        enablemixup = True\n",
    "    \n",
    "    pbar = tqdm(enumerate(loader), total=len(loader), desc=\"Training\")\n",
    "    \n",
    "    for step, batch in pbar:\n",
    "    \n",
    "        if isinstance(batch['melspec'], list):\n",
    "            batch_outputs = []\n",
    "            batch_losses = []\n",
    "            \n",
    "            for i in range(len(batch['melspec'])):\n",
    "                inputs = batch['melspec'][i].unsqueeze(0).to(device)\n",
    "                target = batch['target'][i].unsqueeze(0).to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                if enablemixup and mixuppipeline.rng.uniform() < mixuppipeline.schedule_p() :\n",
    "                    mixed_inputs, target, target_idx, lam = mixuppipeline.old_mixup_data(inputs, target)\n",
    "                    output = model(mixed_inputs)\n",
    "                    loss = mixuppipeline.old_mixup_criterion(criterion, output, target, target_idx, lam)\n",
    "                else :\n",
    "                    output = model(inputs)\n",
    "                    loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                batch_outputs.append(output.detach().cpu())\n",
    "                batch_losses.append(loss.item())\n",
    "            \n",
    "            optimizer.step()\n",
    "            outputs = torch.cat(batch_outputs, dim=0).numpy()\n",
    "            loss = np.mean(batch_losses)\n",
    "            targets = batch['target'].numpy()\n",
    "            \n",
    "        else:\n",
    "            mixupthisbatch = (mixuppipeline.rng.uniform() < mixuppipeline.schedule_p())\n",
    "            inputs = batch['melspec'].to(device)\n",
    "            targets = batch['target'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            if enablemixup and mixuppipeline.rng.uniform() < mixuppipeline.schedule_p() :\n",
    "                mixed_inputs, target, target_idx, lam = mixuppipeline.old_mixup_data(inputs, target)\n",
    "                output = model(mixed_inputs)\n",
    "                loss = mixuppipeline.old_mixup_criterion(criterion, output, targets, target_idx, lam)\n",
    "            else :\n",
    "                output = model(inputs)\n",
    "                loss = criterion(output, target)\n",
    "                \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            outputs = outputs.detach().cpu().numpy()\n",
    "            targets = targets.detach().cpu().numpy()\n",
    "        \n",
    "        # if scheduler is not None and isinstance(scheduler, lr_scheduler.OneCycleLR):\n",
    "        #     scheduler.step()\n",
    "            \n",
    "        all_outputs.append(outputs)\n",
    "        all_targets.append(targets)\n",
    "        losses.append(loss if isinstance(loss, float) else loss.item())\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'train_loss': np.mean(losses[-10:]) if losses else 0,\n",
    "            'lr': optimizer.param_groups[0]['lr']\n",
    "        })\n",
    "\n",
    "    # Enumerating with Soundscapes\n",
    "\n",
    "    pbarss = tqdm(enumerate(ss_loader), total=len(ss_loader), desc=\"PsuedoLabel Training\")\n",
    "    for _, batch in pbarss :\n",
    "        if isinstance(batch['melspec'], list):\n",
    "            batch_losses = []\n",
    "\n",
    "            for i in range(len(batch['melspec'])):\n",
    "                inputs = batch['melspec'][i].unsqueeze(0).to(device)\n",
    "                target = batch['target'][i].unsqueeze(0).to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                if enablemixup and mixuppipeline.rng.uniform() < mixuppipeline.schedule_p() :\n",
    "                    mixed_inputs, target, target_idx, lam = mixuppipeline.old_mixup_data(inputs, target)\n",
    "                    output = model(mixed_inputs)\n",
    "                    loss = mixuppipeline.old_mixup_criterion(criterion, output, target, target_idx, lam)\n",
    "                else :\n",
    "                    output = model(inputs)\n",
    "                    loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                batch_losses.append(loss.item())\n",
    "            \n",
    "            optimizer.step()\n",
    "            loss = np.mean(batch_losses)\n",
    "            \n",
    "        else:\n",
    "            inputs = batch['melspec'].to(device)\n",
    "            targets = batch['target'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            if enablemixup and mixuppipeline.rng.uniform() < mixuppipeline.schedule_p() :\n",
    "                mixed_inputs, target, target_idx, lam = mixuppipeline.old_mixup_data(inputs, target)\n",
    "                output = model(mixed_inputs)\n",
    "                loss = mixuppipeline.old_mixup_criterion(criterion, output, target, target_idx, lam)\n",
    "            else :\n",
    "                output = model(inputs)\n",
    "                loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        if scheduler is not None and isinstance(scheduler, lr_scheduler.OneCycleLR):\n",
    "            scheduler.step()\n",
    "            \n",
    "        sslosses.append(loss if isinstance(loss, float) else loss.item())\n",
    "        \n",
    "        pbarss.set_postfix({\n",
    "            'train_loss': np.mean(sslosses[-10:]) if sslosses else 0,\n",
    "            'lr': optimizer.param_groups[0]['lr']\n",
    "        })\n",
    "\n",
    "# mixup ss with original train data\n",
    "def train_1epo_with_ss_mixup(model, loader, optimizer, criterion, device, ss_loader, mixuppipeline, scheduler=None):\n",
    "    \n",
    "    model.train()\n",
    "    losses = []\n",
    "    \n",
    "    pbar = tqdm(enumerate(loader), total=len(loader), desc=\"Training\")\n",
    "    ss_iter = iter(ss_loader)\n",
    "    \n",
    "    enablemixup = False\n",
    "    # check if mixup mode is \"Soundscape\" \n",
    "    if mixuppipeline.mixup_mode == \"Soundscape\" :\n",
    "        enablemixup = True\n",
    "\n",
    "\n",
    "    for step, batch in pbar:\n",
    "        batch_ss = next(ss_iter)\n",
    "    \n",
    "        if isinstance(batch['melspec'], list):\n",
    "            batch_outputs = []\n",
    "            batch_losses = []\n",
    "            \n",
    "            for i in range(len(batch['melspec'])):\n",
    "                inputs = batch['melspec'][i].unsqueeze(0).to(device)\n",
    "                target = batch['target'][i].unsqueeze(0).to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                if enablemixup and mixuppipeline.rng.uniform() < mixuppipeline.schedule_p() :   \n",
    "                    inputs_ss = batch_ss['melspec'][i].unsqueeze(0).to(device)\n",
    "                    target_ss = batch_ss['target'][i].unsqueeze(0).to(device)\n",
    "\n",
    "                    mixed_inputs, lam = mixuppipeline.mixup_data(inputs, inputs_ss)\n",
    "                    # idk it works with batch\n",
    "\n",
    "                    output = model(mixed_inputs)\n",
    "\n",
    "                    loss = mixuppipeline.mixup_criterion(output, target, target_ss, lam)\n",
    "                else :\n",
    "                    output = model(inputs)\n",
    "                    loss = criterion(output, target)\n",
    "\n",
    "                loss.backward()\n",
    "                \n",
    "                batch_outputs.append(output.detach().cpu())\n",
    "                batch_losses.append(loss.item())\n",
    "            \n",
    "            optimizer.step()\n",
    "            outputs = torch.cat(batch_outputs, dim=0).numpy()\n",
    "            loss = np.mean(batch_losses)\n",
    "            targets = batch['target'].numpy()\n",
    "            \n",
    "        else:\n",
    "            inputs = batch['melspec'].to(device)\n",
    "            targets = batch['target'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if enablemixup and mixuppipeline.rng.uniform() < mixuppipeline.schedule_p() :\n",
    "\n",
    "                inputs_ss = batch_ss['melspec'].to(device)\n",
    "                targets_ss = batch_ss['target'].to(device)\n",
    "            \n",
    "                mixed_inputs, lam = mixuppipeline.mixup_data(inputs, inputs_ss)\n",
    "\n",
    "                outputs = model(mixed_inputs)\n",
    "                loss = mixuppipeline.mixup_criterion(outputs, targets, targets_ss, lam)\n",
    "\n",
    "            else :\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            outputs = outputs.detach().cpu().numpy()\n",
    "            targets = targets.detach().cpu().numpy()\n",
    "        \n",
    "        if scheduler is not None and isinstance(scheduler, lr_scheduler.OneCycleLR):\n",
    "            scheduler.step()\n",
    "            \n",
    "        losses.append(loss if isinstance(loss, float) else loss.item())\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'train_loss': np.mean(losses[-10:]) if losses else 0,\n",
    "            'lr': optimizer.param_groups[0]['lr']\n",
    "        })\n",
    "    # auc = calculate_auc(all_targets, all_outputs)\n",
    "    avg_loss = np.mean(losses)\n",
    "    \n",
    "    return avg_loss, None # return ave_loss, auc\n",
    "\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "   \n",
    "    model.eval()\n",
    "    losses = []\n",
    "    all_targets = []\n",
    "    all_outputs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Validation\"):\n",
    "            if isinstance(batch['melspec'], list):\n",
    "                batch_outputs = []\n",
    "                batch_losses = []\n",
    "                \n",
    "                for i in range(len(batch['melspec'])):\n",
    "                    inputs = batch['melspec'][i].unsqueeze(0).to(device)\n",
    "                    target = batch['target'][i].unsqueeze(0).to(device)\n",
    "                    \n",
    "                    output = model(inputs)\n",
    "                    loss = criterion(output, target)\n",
    "                    \n",
    "                    batch_outputs.append(output.detach().cpu())\n",
    "                    batch_losses.append(loss.item())\n",
    "                \n",
    "                outputs = torch.cat(batch_outputs, dim=0).numpy()\n",
    "                loss = np.mean(batch_losses)\n",
    "                targets = batch['target'].numpy()\n",
    "                \n",
    "            else:\n",
    "                inputs = batch['melspec'].to(device)\n",
    "                targets = batch['target'].to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "                outputs = outputs.detach().cpu().numpy()\n",
    "                targets = targets.detach().cpu().numpy()\n",
    "            \n",
    "            all_outputs.append(outputs)\n",
    "            all_targets.append(targets)\n",
    "            losses.append(loss if isinstance(loss, float) else loss.item())\n",
    "    \n",
    "    all_outputs = np.concatenate(all_outputs)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    \n",
    "    auc = calculate_auc(all_targets, all_outputs)\n",
    "    avg_loss = np.mean(losses)\n",
    "    \n",
    "    return avg_loss, auc\n",
    "\n",
    "def calculate_auc(targets, outputs):\n",
    "  \n",
    "    num_classes = targets.shape[1]\n",
    "    aucs = []\n",
    "    \n",
    "    probs = 1 / (1 + np.exp(-outputs))\n",
    "    \n",
    "    for i in range(num_classes):\n",
    "        \n",
    "        if np.sum(targets[:, i]) > 0:\n",
    "            class_auc = roc_auc_score(targets[:, i], probs[:, i])\n",
    "            aucs.append(class_auc)\n",
    "    \n",
    "    return np.mean(aucs) if aucs else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T01:37:17.731166Z",
     "iopub.status.busy": "2025-05-07T01:37:17.730866Z",
     "iopub.status.idle": "2025-05-07T01:37:17.763273Z",
     "shell.execute_reply": "2025-05-07T01:37:17.762553Z",
     "shell.execute_reply.started": "2025-05-07T01:37:17.731140Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def run_training_with_folds(df, cfg, ssdf = None, sslabeldf = None, rng=None):\n",
    "    \"\"\"Training function that can either use pre-computed spectrograms or generate them on-the-fly\"\"\"\n",
    "\n",
    "    cfg.num_classes = taxonomy_process(cfg)[2]\n",
    "    \n",
    "    if cfg.debug:\n",
    "        cfg.update_debug_settings()\n",
    "\n",
    "    \n",
    "    if cfg.LOAD_DATA:\n",
    "        if 'filepath' not in df.columns:\n",
    "            df['filepath'] = cfg.train_datadir + '/' + df.filename\n",
    "        if 'samplename' not in df.columns:\n",
    "            df['samplename'] = df.filename.map(lambda x: x.split('/')[0] + '-' + x.split('/')[-1].split('.')[0])\n",
    "        \n",
    "    skf = StratifiedKFold(n_splits=cfg.n_fold, shuffle=True, random_state=cfg.seed)\n",
    "    \n",
    "    best_scores = []\n",
    "\n",
    "    ss_dataset = SoundscapeFromNPY(ssdf, sslabeldf, cfg, augmentor=None, mode='train')\n",
    "\n",
    "    batch_sampler = infinite_batch_sampler(\n",
    "        ss_dataset, \n",
    "        batch_size=cfg.batch_size, \n",
    "        generator=torch.Generator().manual_seed(cfg.seed))\n",
    "\n",
    "    ss_loader = DataLoader(\n",
    "        ss_dataset,\n",
    "        #batch_size=cfg.batch_size,\n",
    "        batch_sampler=batch_sampler,\n",
    "        #shuffle=True,\n",
    "        num_workers=cfg.num_workers,\n",
    "        pin_memory=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    val_loss_log = np.empty((0, 6))\n",
    "    val_loss_log = np.append(val_loss_log, np.array([['fold', 'epoch', 'train_loss', 'train_auc', 'val_loss', 'val_auc']]), axis=0)\n",
    "    # save columns\n",
    "\n",
    "\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(df, df['primary_label'])):\n",
    "        if fold not in cfg.selected_folds:\n",
    "            continue\n",
    "            \n",
    "        print(f'\\n{\"=\"*30} Fold {fold} {\"=\"*30}')\n",
    "        \n",
    "        train_df = df.iloc[train_idx].reset_index(drop=True)\n",
    "        val_df = df.iloc[val_idx].reset_index(drop=True)\n",
    "        \n",
    "        print(f'Training set: {len(train_df)} samples')\n",
    "        print(f'Validation set: {len(val_df)} samples')\n",
    "        \n",
    "        current_epo = 0\n",
    "        current_epo_fn = lambda : current_epo\n",
    "\n",
    "        dataaugmentor = AugmentationPipeline(cfg, current_epo_fn, rng=rng)\n",
    "        mixuppipeline = MixupPipeline(cfg, current_epo_fn, rng = rng)\n",
    "\n",
    "        train_dataset = BirdCLEFDatasetFromNPY(train_df, cfg, augmentor=dataaugmentor, mode='train')\n",
    "        val_dataset = BirdCLEFDatasetFromNPY(val_df, cfg, augmentor=None, mode='valid')\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=cfg.batch_size, \n",
    "            shuffle=True, \n",
    "            num_workers=cfg.num_workers,\n",
    "            pin_memory=True,\n",
    "            collate_fn=collate_fn,\n",
    "            drop_last=True\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=cfg.batch_size, \n",
    "            shuffle=False, \n",
    "            num_workers=cfg.num_workers,\n",
    "            pin_memory=True,\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "        \n",
    "        model = BirdCLEFModel(cfg).to(cfg.device)\n",
    "        optimizer = get_optimizer(model, cfg)\n",
    "        criterion = get_criterion(cfg)\n",
    "        \n",
    "        scheduler = get_scheduler(optimizer, cfg)\n",
    "        \n",
    "        best_auc = 0\n",
    "        best_epoch = 0\n",
    "\n",
    "        for epoch in range(cfg.epochs):\n",
    "            print(f\"\\ncur_epoch = {current_epo}\")\n",
    "            print(f\"\\nEpoch {epoch+1}/{cfg.epochs}\")\n",
    "\n",
    "            train_auc = None\n",
    "            train_loss, _ = train_1epo_with_ss_mixup(\n",
    "                model, \n",
    "                train_loader, \n",
    "                optimizer, \n",
    "                criterion, \n",
    "                cfg.device,\n",
    "                ss_loader=ss_loader,\n",
    "                mixuppipeline=mixuppipeline,\n",
    "                # rng=rng,\n",
    "                scheduler = scheduler if isinstance(scheduler, lr_scheduler.OneCycleLR) else None\n",
    "            )\n",
    "\n",
    "            val_loss, val_auc = validate(model, val_loader, criterion, cfg.device)\n",
    "\n",
    "            if scheduler is not None and not isinstance(scheduler, lr_scheduler.OneCycleLR):\n",
    "                if isinstance(scheduler, lr_scheduler.ReduceLROnPlateau):\n",
    "                    scheduler.step(val_loss)\n",
    "                else:\n",
    "                    scheduler.step()\n",
    "\n",
    "            train_auc = 0.0 if train_auc==None else train_auc\n",
    "\n",
    "            print(f\"Train Loss: {train_loss:.4f}, Train AUC: {train_auc:.4f}\")\n",
    "            print(f\"Val Loss: {val_loss:.4f}, Val AUC: {val_auc:.4f}\")\n",
    "            \n",
    "            val_loss_log = np.append(val_loss_log, np.array([[fold, epoch, train_loss, train_auc, val_loss, val_auc]]), axis=0)\n",
    "            file_path = os.path.join(cfg.OUTPUT_DIR, \"val_loss_log\")\n",
    "            np.save(file_path, val_loss_log)\n",
    "            print(f\"loss file saved! {file_path}\")\n",
    "\n",
    "\n",
    "            if val_auc > best_auc:\n",
    "                best_auc = val_auc\n",
    "                best_epoch = epoch + 1\n",
    "                print(f\"New best AUC: {best_auc:.4f} at epoch {best_epoch}\")\n",
    "\n",
    "                torch.save({\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "                    'epoch': epoch,\n",
    "                    'val_auc': val_auc,\n",
    "                    'train_auc': train_auc,\n",
    "                    'cfg': cfg\n",
    "                }, f\"model_fold{fold}.pth\")\n",
    "            current_epo += 1\n",
    "        \n",
    "        best_scores.append(best_auc)\n",
    "        print(f\"\\nBest AUC for fold {fold}: {best_auc:.4f} at epoch {best_epoch}\")\n",
    "        \n",
    "        # Clear memory\n",
    "        del model, optimizer, scheduler, train_loader, val_loader\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Cross-Validation Results:\")\n",
    "    for fold, score in enumerate(best_scores):\n",
    "        print(f\"Fold {cfg.selected_folds[fold]}: {score:.4f}\")\n",
    "    print(f\"Mean AUC: {np.mean(best_scores):.4f}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "def run_training_single_fold(train_df, cfg, ssdf = None, sslabeldf = None, rng=None):\n",
    "    \"\"\"Training function that can either use pre-computed spectrograms or generate them on-the-fly\"\"\"\n",
    "\n",
    "    cfg.num_classes = taxonomy_process(cfg)[2]\n",
    "    \n",
    "    if cfg.debug:\n",
    "        cfg.update_debug_settings()\n",
    "\n",
    "    \n",
    "    if cfg.LOAD_DATA:\n",
    "        if 'filepath' not in train_df.columns:\n",
    "            train_df['filepath'] = cfg.train_datadir + '/' + train_df.filename\n",
    "        if 'samplename' not in train_df.columns:\n",
    "            train_df['samplename'] = train_df.filename.map(lambda x: x.split('/')[0] + '-' + x.split('/')[-1].split('.')[0])\n",
    "        \n",
    "    ss_dataset = SoundscapeFromNPY(ssdf, sslabeldf, cfg, augmentor=None, mode='train')\n",
    "\n",
    "    batch_sampler = infinite_batch_sampler(\n",
    "        ss_dataset, \n",
    "        batch_size=cfg.batch_size, \n",
    "        generator=torch.Generator().manual_seed(cfg.seed))\n",
    "\n",
    "    ss_loader = DataLoader(\n",
    "        ss_dataset,\n",
    "        #batch_size=cfg.batch_size,\n",
    "        batch_sampler=batch_sampler,\n",
    "        #shuffle=True,\n",
    "        num_workers=cfg.num_workers,\n",
    "        pin_memory=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    val_loss_log = np.empty((0, 3))\n",
    "    val_loss_log = np.append(val_loss_log, np.array([['epoch', 'train_loss', 'train_auc']]), axis=0)\n",
    "    # save columns\n",
    "\n",
    "    print(f'\\n{\"=\"*30} Single Fold {\"=\"*30}')\n",
    "    print(f'Training set: {len(train_df)} samples')\n",
    "\n",
    "    current_epo = 0\n",
    "    current_epo_fn = lambda : current_epo\n",
    "\n",
    "    dataaugmentor = AugmentationPipeline(cfg, current_epo_fn, rng=rng)\n",
    "    mixuppipeline = MixupPipeline(cfg, current_epo_fn, rng = rng)\n",
    "\n",
    "    train_dataset = BirdCLEFDatasetFromNPY(train_df, cfg, augmentor=dataaugmentor, mode='train')\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=cfg.batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=cfg.num_workers,\n",
    "        pin_memory=True,\n",
    "        collate_fn=collate_fn,\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "        \n",
    "    model = BirdCLEFModel(cfg).to(cfg.device)\n",
    "    optimizer = get_optimizer(model, cfg)\n",
    "    criterion = get_criterion(cfg)\n",
    "        \n",
    "    scheduler = get_scheduler(optimizer, cfg)\n",
    "    model.load_model(cfg.checkpoint, optimizer, scheduler)\n",
    "\n",
    "    for epoch in range(cfg.epochs):\n",
    "        #print(f\"\\ncur_epoch = {current_epo}\")\n",
    "        print(f\"\\nEpoch {epoch+1}/{cfg.epochs}\")\n",
    "        train_auc = None\n",
    "\n",
    "        train_loss, _ = train_1epo_with_ss_mixup(\n",
    "            model, \n",
    "            train_loader, \n",
    "            optimizer, \n",
    "            criterion, \n",
    "            cfg.device,\n",
    "            ss_loader=ss_loader,\n",
    "            mixuppipeline=mixuppipeline,\n",
    "            # rng=rng,\n",
    "            scheduler = scheduler if isinstance(scheduler, lr_scheduler.OneCycleLR) else None\n",
    "        )\n",
    "\n",
    "        if scheduler is not None and not isinstance(scheduler, lr_scheduler.OneCycleLR):\n",
    "            if isinstance(scheduler, lr_scheduler.ReduceLROnPlateau):\n",
    "                raise Exception(\"Cannot use ReduceLROnPlateau since there's no validation\")\n",
    "            else:\n",
    "                scheduler.step()\n",
    "\n",
    "        train_auc = 0.0 if train_auc==None else train_auc\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train AUC: {train_auc:.4f}\")\n",
    "        \n",
    "        val_loss_log = np.append(val_loss_log, np.array([[epoch, train_loss, train_auc]]), axis=0)\n",
    "        file_path = os.path.join(cfg.OUTPUT_DIR, \"val_loss_log\")\n",
    "        np.save(file_path, val_loss_log)\n",
    "        print(f\"loss file saved! {file_path}\")\n",
    "\n",
    "        if epoch in cfg.selected_folds :\n",
    "\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "                'epoch': epoch,\n",
    "                'val_auc': 0.0, # val_auc not defined\n",
    "                'train_auc': train_auc,\n",
    "                'cfg': cfg\n",
    "            }, f\"model_epoch{epoch}.pth\")\n",
    "        current_epo += 1\n",
    "        \n",
    "    # Clear memory\n",
    "    del model, optimizer, scheduler, train_loader, val_loader\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "def run_1fold_0430(train_df, cfg, ssdf = None, sslabeldf = None, rng=None):\n",
    "    \"\"\"Training function that can either use pre-computed spectrograms or generate them on-the-fly\"\"\"\n",
    "\n",
    "    cfg.num_classes = taxonomy_process(cfg)[2]\n",
    "    \n",
    "    if cfg.debug:\n",
    "        cfg.update_debug_settings()\n",
    "\n",
    "    \n",
    "    if cfg.LOAD_DATA:\n",
    "        if 'filepath' not in train_df.columns:\n",
    "            train_df['filepath'] = cfg.train_datadir + '/' + train_df.filename\n",
    "        if 'samplename' not in train_df.columns:\n",
    "            train_df['samplename'] = train_df.filename.map(lambda x: x.split('/')[0] + '-' + x.split('/')[-1].split('.')[0])\n",
    "        \n",
    "    ss_dataset = SoundscapeFromNPY(ssdf, sslabeldf, cfg, augmentor=None, mode='train')\n",
    "\n",
    "    batch_sampler = infinite_batch_sampler(\n",
    "        ss_dataset, \n",
    "        batch_size=cfg.batch_size, \n",
    "        generator=torch.Generator().manual_seed(cfg.seed))\n",
    "\n",
    "    ss_loader = DataLoader(\n",
    "        ss_dataset,\n",
    "        #batch_size=cfg.batch_size,\n",
    "        batch_sampler=batch_sampler,\n",
    "        #shuffle=True,\n",
    "        num_workers=cfg.num_workers,\n",
    "        pin_memory=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    ss_loader2 = DataLoader(\n",
    "        ss_dataset,\n",
    "        batch_size=cfg.batch_size,\n",
    "        #batch_sampler=batch_sampler,\n",
    "        shuffle=True,\n",
    "        num_workers=cfg.num_workers,\n",
    "        pin_memory=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    val_loss_log = np.empty((0, 3))\n",
    "    val_loss_log = np.append(val_loss_log, np.array([['epoch', 'train_loss', 'train_auc']]), axis=0)\n",
    "    # save columns\n",
    "\n",
    "    print(f'\\n{\"=\"*30} Single Fold {\"=\"*30}')\n",
    "    print(f'Training set: {len(train_df)} samples')\n",
    "\n",
    "    current_epo = 0\n",
    "    current_epo_fn = lambda : current_epo\n",
    "\n",
    "    dataaugmentor = AugmentationPipeline(cfg, current_epo_fn, rng=rng)\n",
    "    mixuppipeline = MixupPipeline(cfg, current_epo_fn, rng = rng)\n",
    "\n",
    "    train_dataset = BirdCLEFDatasetFromNPY(train_df, cfg, augmentor=dataaugmentor, mode='train')\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=cfg.batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=cfg.num_workers,\n",
    "        pin_memory=True,\n",
    "        collate_fn=collate_fn,\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "        \n",
    "    model = BirdCLEFModel(cfg).to(cfg.device)\n",
    "    optimizer = get_optimizer(model, cfg)\n",
    "    criterion = get_criterion(cfg)\n",
    "\n",
    "    scheduler = get_scheduler(optimizer, cfg)\n",
    "    start_epoch = model.load_model(cfg.checkpoint, optimizer, scheduler)\n",
    "    for epoch in range(start_epoch+1, cfg.epochs):\n",
    "        #print(f\"\\ncur_epoch = {current_epo}\")\n",
    "        print(f\"\\nEpoch {epoch+1}/{cfg.epochs}\")\n",
    "        train_auc = None\n",
    "        \n",
    "        if epoch < 20 :\n",
    "\n",
    "            train_loss, _ = train_1epo_with_ss_mixup(\n",
    "                model, \n",
    "                train_loader, \n",
    "                optimizer, \n",
    "                criterion, \n",
    "                cfg.device,\n",
    "                ss_loader=ss_loader,\n",
    "                mixuppipeline=mixuppipeline,\n",
    "                # rng=rng,\n",
    "                scheduler = scheduler if isinstance(scheduler, lr_scheduler.OneCycleLR) else None\n",
    "            )\n",
    "\n",
    "            if scheduler is not None and not isinstance(scheduler, lr_scheduler.OneCycleLR):\n",
    "                if isinstance(scheduler, lr_scheduler.ReduceLROnPlateau):\n",
    "                    raise Exception(\"Cannot use ReduceLROnPlateau since there's no validation\")\n",
    "                else:\n",
    "                    scheduler.step()\n",
    "\n",
    "            train_auc = 0.0 if train_auc==None else train_auc\n",
    "\n",
    "            print(f\"Train Loss: {train_loss:.4f}, Train AUC: {train_auc:.4f}\")\n",
    "            \n",
    "            val_loss_log = np.append(val_loss_log, np.array([[epoch, train_loss, train_auc]]), axis=0)\n",
    "            file_path = os.path.join(cfg.OUTPUT_DIR, \"val_loss_log\")\n",
    "            np.save(file_path, val_loss_log)\n",
    "            print(f\"loss file saved! {file_path}\")\n",
    "            if epoch in cfg.selected_folds :\n",
    "\n",
    "                torch.save({\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "                    'epoch': epoch,\n",
    "                    'val_auc': 0.0, # val_auc not defined\n",
    "                    'train_auc': train_auc,\n",
    "                    'cfg': cfg\n",
    "                }, f\"model_epoch{epoch}.pth\")\n",
    "            current_epo += 1\n",
    "        else :\n",
    "\n",
    "            train_loss, _ = train_one_epoch(\n",
    "                model, \n",
    "                ss_loader2, \n",
    "                optimizer, \n",
    "                criterion, \n",
    "                cfg.device,\n",
    "                mixuppipeline=mixuppipeline,\n",
    "                # rng=rng,\n",
    "                scheduler = scheduler if isinstance(scheduler, lr_scheduler.OneCycleLR) else None\n",
    "            )\n",
    "\n",
    "            if scheduler is not None and not isinstance(scheduler, lr_scheduler.OneCycleLR):\n",
    "                if isinstance(scheduler, lr_scheduler.ReduceLROnPlateau):\n",
    "                    raise Exception(\"Cannot use ReduceLROnPlateau since there's no validation\")\n",
    "                else:\n",
    "                    scheduler.step()\n",
    "\n",
    "            train_auc = 0.0 if train_auc==None else train_auc\n",
    "\n",
    "            print(f\"Train Loss: {train_loss:.4f}, Train AUC: {train_auc:.4f}\")\n",
    "            \n",
    "            val_loss_log = np.append(val_loss_log, np.array([[epoch, train_loss, train_auc]]), axis=0)\n",
    "            file_path = os.path.join(cfg.OUTPUT_DIR, \"val_loss_log\")\n",
    "            np.save(file_path, val_loss_log)\n",
    "            print(f\"loss file saved! {file_path}\")\n",
    "\n",
    "            if epoch in cfg.selected_folds :\n",
    "\n",
    "                torch.save({\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "                    'epoch': epoch,\n",
    "                    'val_auc': 0.0, # val_auc not defined\n",
    "                    'train_auc': train_auc,\n",
    "                    'cfg': cfg\n",
    "                }, f\"model_epoch{epoch}.pth\")\n",
    "            current_epo += 1\n",
    "        \n",
    "    # Clear memory\n",
    "    del model, optimizer, scheduler, train_loader, ss_loader, ss_loader2\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-07T01:37:53.653Z",
     "iopub.execute_input": "2025-05-07T01:37:17.764854Z",
     "iopub.status.busy": "2025-05-07T01:37:17.764552Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading training data...\n",
      "\n",
      "Starting training...\n",
      "Filtered samples with threshold 0.3:\n",
      "  Kept: 40377 / 116712 samples\n",
      "\n",
      "============================== Single Fold ==============================\n",
      "Training set: 28188 samples\n",
      "\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42ac020020c540bdb08254ca6c2ccea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import time\n",
    "\n",
    "    set_seed(cfg.seed)\n",
    "    rng = set_rng(cfg.seed)\n",
    "\n",
    "    print(\"\\nLoading training data...\")\n",
    "    train_df = pd.read_csv(cfg.train_csv)\n",
    "    ss_df = pd.read_csv(cfg.train_sscsv)\n",
    "    ss_label_df = pd.read_csv(cfg.train_sssub)\n",
    "    #checkhparams(cfg)\n",
    "\n",
    "    print(\"\\nStarting training...\")\n",
    "    \n",
    "    run_1fold_0430(train_df, cfg, ss_df, ss_label_df, rng)\n",
    "    \n",
    "    print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 11361821,
     "isSourceIdPinned": false,
     "sourceId": 91844,
     "sourceType": "competition"
    },
    {
     "datasetId": 7149898,
     "sourceId": 11416185,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7176795,
     "sourceId": 11454113,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7217450,
     "sourceId": 11510276,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7349985,
     "sourceId": 11709703,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
